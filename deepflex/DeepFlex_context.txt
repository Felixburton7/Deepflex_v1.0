==========================================================
                Deep-flex Context Document
==========================================================

Project Working Directory: /home/s_felix/FINAL_PROJECT/packages/DeepFlex

---------------------------------------------------------
List of .yaml and .py files:
---------------------------------------------------------
./concatenate_fastas.py
./config.yaml
./data_processor.py
./data/raw/fix_data_.py
./dataset.py
./data/utils/filter_by_temperature.py
./evaluate_predictions.py
./main.py
./merge_predictions.py
./model.py
./predict.py
./train.py

==========================================================
File Contents:
==========================================================
===== FILE: ./concatenate_fastas.py =====
#!/usr/bin/env python3
import os
import sys
import logging

# Set up basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Configuration ---
# Adjust this path if your processed data is elsewhere
PROCESSED_DATA_DIR = os.path.join(os.path.dirname(__file__), 'data', 'processed')

INPUT_FASTA_FILES = [
    "train_sequences.fasta",
    "val_sequences.fasta",
    "test_sequences.fasta",
]

OUTPUT_FASTA_FILE = "predict_sequences.fasta"
# --- End Configuration ---

def concatenate_files():
    """Concatenates specified input FASTA files into one output FASTA file."""

    output_path = os.path.join(PROCESSED_DATA_DIR, OUTPUT_FASTA_FILE)
    input_paths = [os.path.join(PROCESSED_DATA_DIR, fname) for fname in INPUT_FASTA_FILES]

    logger.info(f"Attempting to concatenate FASTA files into: {output_path}")
    total_sequences = 0
    missing_files = []

    # Check if all input files exist first
    for path in input_paths:
        if not os.path.exists(path):
            logger.error(f"Input FASTA file not found: {path}")
            missing_files.append(path)

    if missing_files:
        logger.error("Aborting concatenation due to missing input files.")
        return False

    try:
        with open(output_path, 'w') as outfile:
            for input_path in input_paths:
                logger.info(f"Processing: {os.path.basename(input_path)}")
                sequence_count_in_file = 0
                last_line_was_newline = True # Assume start of file is like after a newline
                with open(input_path, 'r') as infile:
                    for line in infile:
                        # Count sequences for logging
                        if line.startswith('>'):
                            sequence_count_in_file += 1
                            total_sequences += 1
                        # Write line to output
                        outfile.write(line)
                        # Track if last line ended with newline (important for FASTA format)
                        last_line_was_newline = line.endswith('\n')

                logger.info(f" -> Appended {sequence_count_in_file} sequences from {os.path.basename(input_path)}.")

                # Ensure there's a newline between concatenated files if the previous didn't end with one
                if not last_line_was_newline:
                    logger.warning(f"File {os.path.basename(input_path)} did not end with a newline. Adding one.")
                    outfile.write('\n')


        logger.info("-" * 30)
        logger.info(f"Successfully concatenated {len(input_paths)} files into {output_path}")
        logger.info(f"Total sequences written: {total_sequences}")
        logger.info("-" * 30)
        return True

    except IOError as e:
        logger.error(f"An error occurred during file operations: {e}")
        return False
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    if concatenate_files():
        sys.exit(0) # Exit with success code
    else:
        sys.exit(1) # Exit with error code
---------------------------------------------------------
===== FILE: ./config.yaml =====
# Configuration for ESM-Flex Temperature-Aware Project (Enhanced)

data:
  # Path to the directory containing processed data splits
  data_dir: data/processed
  # Path to the enriched CSV file with additional features
  # raw_csv_path: /home/s_felix/drDataScience/data/analysis_complete_holdout_dataset.csv
  # raw_csv_path: /home/s_felix/FINAL_PROJECT/latest_drDS/drDataScience/data/ATLAS_final_analysis_dataset_filtered.csv
  raw_csv_path: /home/s_felix/FINAL_PROJECT/packages/DeepFlex/data/raw/aggregated_train_379k_dataset.csv
  # Path where temperature scaling parameters (min/max) will be saved/loaded from data_dir
  temp_scaling_filename: temp_scaling_params.json
  # Feature selection configuration
  features:
    # Core structural features
    use_position_info: true        # normalized_resid
    use_structure_info: true       # secondary_structure_encoded, core_exterior_encoded
    use_accessibility: true        # relative_accessibility
    use_backbone_angles: true      # phi_norm, psi_norm
    use_protein_size: true         # protein_size
    use_voxel_rmsf: false           # voxel_rmsf
    use_bfactor: true              # bfactor_norm
    # Feature normalization parameters will be saved here
    normalization_params_file: feature_normalization.json

model:
  # Identifier for the ESM-C model from the 'esm' library.
  esm_version: "esmc_600m"

  # Enhanced architecture options
  architecture:
    use_enhanced_features: true    # Use additional features beyond sequence and temperature
    use_attention: true            # Use self-attention for sequence processing
    attention_heads: 8             # Number of attention heads
    attention_dropout: 0.1         # Dropout rate for attention layers
    improved_temp_integration: true # Use advanced temperature integration

  # Regression head configuration
  regression:
    # Hidden dimension for the MLP head. Set to 0 for a direct Linear layer.
    hidden_dim: 128                # Increased for enhanced model
    # Dropout rate for the regression head
    dropout: 0.1

training:
  # Number of training epochs
  num_epochs: 50
  # Batch size (adjust based on GPU memory and model size)
  batch_size: 8
  # Learning rate for the optimizer
  learning_rate: 1.0e-4
  # Weight decay for the optimizer (applied only to non-bias/norm params)
  weight_decay: 0.01
  # AdamW epsilon parameter
  adam_epsilon: 1.0e-8
  # Gradient accumulation steps (effective_batch_size = batch_size * accumulation_steps)
  accumulation_steps: 4
  # Max gradient norm for clipping (0 to disable)
  max_gradient_norm: 1.0
  # Learning rate scheduler patience (epochs) based on validation correlation
  scheduler_patience: 5
  # Early stopping patience (epochs) based on validation correlation
  early_stopping_patience: 10
  # Random seed for reproducibility
  seed: 42
  # Optional: Maximum sequence length to process (helps manage memory)
  # max_length: 1024
  # Size of length buckets for grouping similar-length sequences in dataloader
  length_bucket_size: 50
  # Frequency (in epochs) to save intermediate checkpoints
  checkpoint_interval: 5

output:
  # Directory to save trained models, logs, plots, and scaling parameters
  model_dir: models

# Prediction settings (used by predict.py if called via main.py)
prediction:
  batch_size: 8 # Can often be larger than training batch size
  plot_predictions: true
  smoothing_window: 1 # Window size for smoothing plots (1 = no smoothing)
  # Optional: max_length for prediction if different from training
  # max_length: 1024
---------------------------------------------------------
===== FILE: ./data_processor.py =====
import pandas as pd
import numpy as np
from collections import defaultdict
import os
import random
from typing import Dict, List, Tuple, Set, Optional, Any
import logging
import json
import re # Import regex for potentially complex ID parsing

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Standard 1-letter amino acid codes
AA_MAP = {
    'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F',
    'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L',
    'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN': 'Q', 'ARG': 'R',
    'SER': 'S', 'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y',
    # Include common Histidine variants if they weren't fixed by fix_data_.py
    'HSD': 'H', 'HSE': 'H', 'HSP': 'H'
}

VALID_AA_1LETTER = set(AA_MAP.values())


# Map 3-letter code to 3-letter code (for consistency checks)
AA_3LETTER_MAP = {
    'ALA': 'ALA', 'CYS': 'CYS', 'ASP': 'ASP', 'GLU': 'GLU', 'PHE': 'PHE',
    'GLY': 'GLY', 'HIS': 'HIS', 'ILE': 'ILE', 'LYS': 'LYS', 'LEU': 'LEU',
    'MET': 'MET', 'ASN': 'ASN', 'PRO': 'PRO', 'GLN': 'GLN', 'ARG': 'ARG',
    'SER': 'SER', 'THR': 'THR', 'VAL': 'VAL', 'TRP': 'TRP', 'TYR': 'TYR',
    # Histidine variants
    'HSD': 'HIS', 'HSE': 'HIS', 'HSP': 'HIS'
}

# List of structural features to extract from the enriched dataset
STRUCTURAL_FEATURES = [
    'normalized_resid', 'core_exterior_encoded', 'secondary_structure_encoded',
    'relative_accessibility', 'phi_norm', 'psi_norm', 'protein_size',
    'voxel_rmsf', 'bfactor_norm'
]

# Define the separator for domain_id and temperature
INSTANCE_KEY_SEPARATOR = "@"

def create_instance_key(domain_id: str, temperature: float) -> str:
    """Creates a unique key combining domain ID and temperature."""
    # Format temperature to avoid floating point inconsistencies in keys
    return f"{str(domain_id)}{INSTANCE_KEY_SEPARATOR}{temperature:.1f}"

def get_domain_id_from_instance_key(instance_key: str) -> Optional[str]:
    """Extracts the original domain_id from the combined instance key."""
    if INSTANCE_KEY_SEPARATOR in instance_key:
        return instance_key.split(INSTANCE_KEY_SEPARATOR, 1)[0]
    else:
        logger.warning(f"Instance key '{instance_key}' did not contain the expected separator '{INSTANCE_KEY_SEPARATOR}'. Returning the full key as domain_id.")
        return instance_key


def load_data(csv_path: str, config: Dict = None) -> Optional[pd.DataFrame]:
    """Load data from the enriched CSV file."""
    if not os.path.exists(csv_path):
        logger.error(f"Data file not found: {csv_path}")
        raise FileNotFoundError(f"Data file not found: {csv_path}")

    try:
        logger.info(f"Loading data from {csv_path}")
        df = pd.read_csv(csv_path)
        logger.info(f"Loaded {len(df)} rows from {csv_path}")
        logger.info(f"Columns: {df.columns.tolist()}")

        # Required columns
        required_cols = ['domain_id', 'resid', 'resname', 'temperature', 'rmsf']

        # Determine which optional features are requested and available
        required_features = []
        if config and config.get('data', {}).get('features', {}):
            feature_config = config['data']['features']
            enabled_features = []
            if feature_config.get('use_position_info', True): enabled_features.append('normalized_resid')
            if feature_config.get('use_structure_info', True): enabled_features.extend(['core_exterior_encoded', 'secondary_structure_encoded'])
            if feature_config.get('use_accessibility', True): enabled_features.append('relative_accessibility')
            if feature_config.get('use_backbone_angles', True): enabled_features.extend(['phi_norm', 'psi_norm'])
            if feature_config.get('use_protein_size', True): enabled_features.append('protein_size')
            if feature_config.get('use_voxel_rmsf', True): enabled_features.append('voxel_rmsf')
            if feature_config.get('use_bfactor', True): enabled_features.append('bfactor_norm')

            for feature in enabled_features:
                if feature in df.columns:
                    required_features.append(feature)
                else:
                    logger.warning(f"Feature '{feature}' specified in config but not found in dataset")
        else:
            logger.info("No feature configuration provided or features section missing. Checking for all standard features.")
            for feature in STRUCTURAL_FEATURES:
                 if feature in df.columns:
                      required_features.append(feature)
                      logger.info(f"Found structural feature: {feature}")

        required_cols.extend(required_features)

        # Check for missing required columns
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            core_missing = [c for c in ['domain_id', 'resid', 'resname', 'temperature', 'rmsf'] if c not in df.columns]
            if core_missing:
                logger.error(f"CSV missing ESSENTIAL columns: {core_missing}. Cannot proceed.")
                return None
            else:
                logger.warning(f"CSV missing requested FEATURE columns: {[c for c in missing_cols if c in required_features]}. Proceeding without them.")
                required_cols = [c for c in required_cols if c in df.columns]


        # Rename temperature column for consistency
        if 'temperature' in df.columns:
            df.rename(columns={'temperature': 'temperature_feature'}, inplace=True)
        elif 'temperature_feature' not in df.columns:
            logger.error("Neither 'temperature' nor 'temperature_feature' columns found in the dataset")
            return None

        # Rename RMSF column for consistency
        if 'rmsf' in df.columns:
            df.rename(columns={'rmsf': 'target_rmsf'}, inplace=True)
        elif 'target_rmsf' not in df.columns:
            logger.error("Neither 'rmsf' nor 'target_rmsf' columns found in the dataset")
            return None

        # Check for NaN in essential columns (now including target_rmsf)
        nan_check_cols = ['domain_id', 'resid', 'resname', 'temperature_feature', 'target_rmsf']
        nan_counts = df[nan_check_cols].isnull().sum()
        if nan_counts.sum() > 0:
            logger.warning(f"Found NaN values in essential columns:\n{nan_counts[nan_counts > 0]}")
            logger.warning("Attempting to drop rows with NaNs in these essential columns...")
            df.dropna(subset=nan_check_cols, inplace=True)
            logger.info(f"{len(df)} rows remaining after dropping NaNs.")
            if len(df) == 0:
                logger.error("No valid rows remaining after dropping NaNs. Cannot proceed.")
                return None

        # Convert numerical columns to numeric, coercing errors
        numeric_cols_present = [col for col in ['temperature_feature', 'target_rmsf', 'resid'] if col in df.columns]
        numeric_cols_present.extend([col for col in required_features if col in df.columns])

        for col in numeric_cols_present:
            if col in df.columns:
                original_type = df[col].dtype
                df[col] = pd.to_numeric(df[col], errors='coerce')
                if df[col].dtype != original_type and not pd.api.types.is_numeric_dtype(original_type):
                     logger.debug(f"Column '{col}' coerced from {original_type} to {df[col].dtype}.")

        # Drop rows where essential numeric conversions failed (resulted in NaN)
        essential_numeric_cols = ['temperature_feature', 'target_rmsf', 'resid']
        nan_after_coerce = df[essential_numeric_cols].isnull().sum()
        if nan_after_coerce.sum() > 0:
             logger.warning(f"Found NaNs after numeric conversion in essential columns:\n{nan_after_coerce[nan_after_coerce > 0]}")
             logger.warning("Dropping rows with NaNs in these essential numeric columns...")
             df.dropna(subset=essential_numeric_cols, inplace=True)
             logger.info(f"{len(df)} rows remaining after dropping conversion NaNs.")

        # Convert resid to integer type after ensuring no NaNs
        df['resid'] = df['resid'].astype(int)

        # Map resnames to standard 3-letter codes for consistency
        if 'resname' in df.columns:
            df['resname'] = df['resname'].apply(
                lambda x: AA_3LETTER_MAP.get(str(x).upper().strip(), str(x).upper().strip()) if pd.notna(x) else x
            )
            unknown_res = df[~df['resname'].isin(AA_MAP.keys())]['resname'].unique()
            if len(unknown_res) > 0:
                 logger.warning(f"Found potentially unknown residue names after mapping: {unknown_res}")


        if len(df) == 0:
            logger.error("No valid data rows remaining after initial processing and cleaning. Cannot proceed.")
            return None

        logger.info(f"Initial data loading and cleaning complete. {len(df)} rows remaining.")
        return df
    except Exception as e:
        logger.error(f"Error loading or performing initial validation on CSV file {csv_path}: {e}", exc_info=True)
        raise


def group_by_domain_and_temp(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """
    Group data by the unique combination of domain_id and temperature_feature.
    Generates instance keys like 'domain_id@temperature'.
    """
    instance_groups = {}
    required_cols = ['domain_id', 'resid', 'temperature_feature']
    if not all(col in df.columns for col in required_cols):
        logger.error(f"DataFrame missing one or more required columns for grouping: {required_cols}.")
        return instance_groups

    grouped = df.groupby(['domain_id', 'temperature_feature'], observed=True)
    logger.info(f"Grouping by ('domain_id', 'temperature_feature')... Found {len(grouped)} potential groups.")

    processed_groups = 0
    for (domain_id, temp), group_df in grouped:
        domain_id_str = str(domain_id)
        try:
            temp_float = float(temp)
        except (ValueError, TypeError):
            logger.warning(f"Skipping group due to invalid temperature value: domain='{domain_id_str}', temp='{temp}'.")
            continue

        instance_key = create_instance_key(domain_id_str, temp_float)
        instance_groups[instance_key] = group_df.sort_values('resid')
        processed_groups += 1

    logger.info(f"Grouped data into {len(instance_groups)} unique (domain_id, temperature) instances.")
    if len(grouped) != processed_groups:
         logger.warning(f"Processed {processed_groups} groups, but initially found {len(grouped)}. Some might have been skipped due to invalid temperatures.")

    return instance_groups

def extract_sequence_rmsf_temp_features(instance_groups: Dict[str, pd.DataFrame], config: Optional[Dict] = None) -> Dict[str, Dict]:
    """
    Extract sequence, RMSF, temp, and features for each unique (domain, temp) instance.
    Imputes NaN feature values using instance median. Skips instances with residue gaps.
    """
    processed_data = {}
    processed_count = 0
    skipped_residues = defaultdict(int)
    skipped_instances_missing_cols = set()
    skipped_instances_length_mismatch = set()
    skipped_instances_no_sequence = set()
    skipped_instances_non_sequential = set()
    nan_imputation_counts = defaultdict(int)

    features_to_extract = []
    if config and config.get('data', {}).get('features', {}):
        feature_config = config['data']['features']
        if feature_config.get('use_position_info', True): features_to_extract.append('normalized_resid')
        if feature_config.get('use_structure_info', True): features_to_extract.extend(['core_exterior_encoded', 'secondary_structure_encoded'])
        if feature_config.get('use_accessibility', True): features_to_extract.append('relative_accessibility')
        if feature_config.get('use_backbone_angles', True): features_to_extract.extend(['phi_norm', 'psi_norm'])
        if feature_config.get('use_protein_size', True): features_to_extract.append('protein_size')
        if feature_config.get('use_voxel_rmsf', True): features_to_extract.append('voxel_rmsf')
        if feature_config.get('use_bfactor', True): features_to_extract.append('bfactor_norm')
    else:
        logger.info("No feature config section found, attempting to extract all standard features.")
        features_to_extract = STRUCTURAL_FEATURES

    logger.info(f"Attempting to extract structural features: {features_to_extract}")

    # Process each instance (domain_id@temp)
    for instance_key, instance_df in instance_groups.items():
        required_core_cols = ['resname', 'target_rmsf', 'temperature_feature', 'resid']
        available_features_in_df = []

        for feature in features_to_extract:
            if feature in instance_df.columns:
                available_features_in_df.append(feature)

        required_cols_for_instance = required_core_cols + available_features_in_df

        if not all(col in instance_df.columns for col in required_cols_for_instance):
            missing = [c for c in required_cols_for_instance if c not in instance_df.columns]
            logger.warning(f"Skipping instance {instance_key} due to missing columns: {missing}. Available: {instance_df.columns.tolist()}")
            skipped_instances_missing_cols.add(instance_key)
            continue

        sequence = ''
        rmsf_values = []
        temperature = float(instance_df['temperature_feature'].iloc[0])
        protein_size = None
        if 'protein_size' in available_features_in_df:
            protein_size = float(instance_df['protein_size'].iloc[0])

        feature_arrays = {feature: [] for feature in available_features_in_df if feature != 'protein_size'}
        residue_numbers = []
        valid_instance = True
        last_resid = -1

        # --- Pre-calculate medians for imputation ---
        feature_medians = {}
        for feature in feature_arrays.keys():
            median_val = instance_df[feature].median()
            if pd.isna(median_val):
                 logger.warning(f"Instance {instance_key}: Could not calculate median for feature '{feature}' (all values might be NaN?). Imputation will use 0.")
                 feature_medians[feature] = 0.0
            else:
                 feature_medians[feature] = median_val

        # Iterate through sorted residues for this instance
        for _, row in instance_df.iterrows():
            current_resid = row['resid']

            if last_resid != -1 and current_resid != last_resid + 1:
                logger.warning(f"Instance {instance_key}: Non-sequential residue number detected (expected {last_resid+1}, got {current_resid}). Skipping instance.")
                valid_instance = False
                break
            last_resid = current_resid

            # --- *** START: Corrected Residue Check Logic *** ---
            residue_name = str(row['resname']).upper().strip() # Get name processed by load_data
            one_letter_code = None

            if residue_name in AA_MAP: # Check if it's a standard 3-letter code
                one_letter_code = AA_MAP[residue_name]
            elif residue_name in VALID_AA_1LETTER: # Check if it's already a standard 1-letter code
                one_letter_code = residue_name
            # Add elif for non-standard but mappable residues like MSE if needed
            # elif residue_name == 'MSE': one_letter_code = 'M'

            if one_letter_code is not None: # Successfully identified a standard AA code
                sequence += one_letter_code
                rmsf_values.append(row['target_rmsf'])
                residue_numbers.append(current_resid)

                # Extract available structural features for this residue
                for feature in feature_arrays.keys(): # Use keys from the dict which excludes protein_size
                    feature_val = row[feature]
                    # Impute NaN using pre-calculated median
                    if pd.isna(feature_val):
                         impute_value = feature_medians[feature]
                         if nan_imputation_counts[(instance_key, feature)] == 0:
                              logger.debug(f"Instance {instance_key}, Feature '{feature}': Found NaN(s). Imputing with instance median ({impute_value:.4f}).")
                         feature_arrays[feature].append(impute_value)
                         nan_imputation_counts[(instance_key, feature)] += 1
                    else:
                         feature_arrays[feature].append(feature_val)
            else:
                # This 'else' block now correctly handles TRULY unknown/unmapped residue names
                skipped_residues[residue_name] += 1
                logger.warning(f"Instance {instance_key}, Residue {current_resid}: Encountered truly unknown residue name '{residue_name}'. Skipping residue.")
            # --- *** END: Corrected Residue Check Logic *** ---

        if not valid_instance:
            skipped_instances_non_sequential.add(instance_key)
            continue

        if sequence:
            per_residue_feature_lengths = {f: len(arr) for f, arr in feature_arrays.items()}
            all_lengths_match = (len(sequence) == len(rmsf_values) and
                                 all(len(sequence) == length for length in per_residue_feature_lengths.values()))

            if all_lengths_match:
                processed_data[instance_key] = {
                    'sequence': sequence,
                    'rmsf': np.array(rmsf_values, dtype=np.float32),
                    'temperature': float(temperature)
                }
                for feature, arr in feature_arrays.items():
                    processed_data[instance_key][feature] = np.array(arr, dtype=np.float32)
                if protein_size is not None:
                    processed_data[instance_key]['protein_size'] = protein_size
                processed_count += 1
            else:
                failed_features = {f: length for f, length in per_residue_feature_lengths.items() if len(sequence) != length}
                logger.warning(f"Length mismatch for instance {instance_key} after processing: "
                               f"Sequence={len(sequence)}, RMSF={len(rmsf_values)}, "
                               f"Features={per_residue_feature_lengths}. "
                               f"Mismatch in features: {failed_features}. Skipping.")
                skipped_instances_length_mismatch.add(instance_key)
        else:
            logger.warning(f"Instance {instance_key} resulted in an empty sequence after processing. Skipping.")
            skipped_instances_no_sequence.add(instance_key)

    imputed_instances = len(set(key[0] for key in nan_imputation_counts.keys()))
    total_imputations = sum(nan_imputation_counts.values())
    if total_imputations > 0:
         logger.info(f"Imputed {total_imputations} NaN feature values across {imputed_instances} instances using instance medians.")

    if skipped_residues:
        logger.warning(f"Encountered unknown residues (total counts across all instances): {dict(skipped_residues)}")
    if skipped_instances_missing_cols:
        logger.warning(f"Skipped {len(skipped_instances_missing_cols)} instances due to missing columns.")
    if skipped_instances_non_sequential:
        logger.warning(f"Skipped {len(skipped_instances_non_sequential)} instances due to non-sequential residue numbers (gaps).")
    if skipped_instances_length_mismatch:
        logger.warning(f"Skipped {len(skipped_instances_length_mismatch)} instances due to final length mismatch (likely due to skipped unknown residues).")
    if skipped_instances_no_sequence:
        logger.warning(f"Skipped {len(skipped_instances_no_sequence)} instances due to empty sequence after processing.")

    total_skipped = (len(skipped_instances_missing_cols) +
                     len(skipped_instances_non_sequential) +
                     len(skipped_instances_length_mismatch) +
                     len(skipped_instances_no_sequence))
    logger.info(f"Successfully extracted and validated data for {processed_count} instances.")
    logger.info(f"Total instances skipped due to errors: {total_skipped}")

    return processed_data

def extract_topology(domain_id: str) -> str:
    """
    Extract topology identifier (e.g., PDB ID) from the domain_id part.
    Handles cases like '1xyz', '1xyz_A', '1xyz.A', '1xyz-A', etc.
    """
    if not isinstance(domain_id, str) or len(domain_id) < 4:
        logger.warning(f"Cannot reliably extract topology from short/invalid domain_id: '{domain_id}'. Using fallback.")
        return f"unknown_{hash(domain_id)}"

    match = re.match(r"^(\d[a-zA-Z0-9]{3})", domain_id)
    if match:
        pdb_id = match.group(1).upper()
        return pdb_id
    else:
        base_id_match = re.match(r"^([a-zA-Z0-9]+)", domain_id)
        if base_id_match:
             base_id = base_id_match.group(1)
             topo_candidate = base_id[:4].upper()
             logger.debug(f"Domain ID '{domain_id}' doesn't start with PDB pattern. Using fallback topology candidate: '{topo_candidate}'")
             return topo_candidate
        else:
             logger.warning(f"Could not extract meaningful topology from domain_id: '{domain_id}'. Using hash.")
             return f"unknown_{hash(domain_id)}"


def split_by_topology(data: Dict[str, Dict], train_ratio=0.85, val_ratio=0.075, seed=42) -> Tuple[Dict, Dict, Dict]: # <-- Adjusted defaults
    """
    Split data by topology based on the domain_id part of the instance_key.
    Ensures all temperature instances for a domain go to the same split.
    """
    if not data:
        logger.warning("No data provided to split_by_topology. Returning empty splits.")
        return {}, {}, {}

    random.seed(seed)
    # Use updated ratios in log message
    test_ratio = max(0.0, 1.0 - train_ratio - val_ratio)
    logger.info(f"Splitting {len(data)} instances by topology using seed {seed}. Ratios: Train={train_ratio:.3f}, Val={val_ratio:.3f}, Test={test_ratio:.3f}")

    topology_groups = defaultdict(list)
    instance_keys = list(data.keys())

    for instance_key in instance_keys:
        domain_id = get_domain_id_from_instance_key(instance_key)
        if domain_id:
            topology = extract_topology(domain_id)
            topology_groups[topology].append(instance_key)
        else:
            logger.warning(f"Could not extract domain ID from instance key '{instance_key}'. Skipping for split assignment.")

    unique_topologies_found = list(topology_groups.keys())
    logger.info(f"Found {len(unique_topologies_found)} unique topologies from {len(instance_keys)} instances.")
    if not unique_topologies_found:
         logger.error("No topologies could be extracted. Cannot perform split.")
         return {}, {}, {}

    random.shuffle(unique_topologies_found)

    n_topologies = len(unique_topologies_found)
    if n_topologies < 3:
        logger.warning(f"Very few topologies ({n_topologies}). Splits might be skewed or empty.")

    # Ensure ratios sum to <= 1.0
    if train_ratio + val_ratio > 1.0:
        logger.warning(f"Train ({train_ratio}) + Val ({val_ratio}) ratios exceed 1.0. Adjusting validation ratio.")
        val_ratio = max(0.0, 1.0 - train_ratio)
        test_ratio = 0.0
        logger.warning(f"Adjusted ratios: Train={train_ratio:.3f}, Val={val_ratio:.3f}, Test={test_ratio:.3f}")

    train_idx = int(round(n_topologies * train_ratio)) # Use round for possibly better distribution
    val_idx = train_idx + int(round(n_topologies * val_ratio))

    # Adjust indices to prevent empty splits if possible and respect boundaries
    if n_topologies >= 1:
        if train_idx == 0: train_idx = 1 # Ensure train gets at least one
    if n_topologies >= 2:
        if val_idx == train_idx and val_ratio > 0: val_idx = min(train_idx + 1, n_topologies) # Ensure val gets one if needed
    if n_topologies >= 3:
         if val_idx == n_topologies and test_ratio > 0: # Check if test should get something
              val_idx = max(train_idx, n_topologies - 1) # Give last one to test
              if val_idx == train_idx: # Only train and test possible
                   train_idx = max(0, train_idx -1) # Adjust train boundary if needed


    # Ensure indices are within bounds
    train_idx = max(0, min(train_idx, n_topologies))
    val_idx = max(train_idx, min(val_idx, n_topologies))


    train_topologies_set = set(unique_topologies_found[:train_idx])
    val_topologies_set = set(unique_topologies_found[train_idx:val_idx])
    test_topologies_set = set(unique_topologies_found[val_idx:])

    logger.info(f"Topology split indices: Train end={train_idx}, Val end={val_idx}, Total={n_topologies}")
    logger.info(f"Split topology counts: Train={len(train_topologies_set)}, Val={len(val_topologies_set)}, Test={len(test_topologies_set)}")
    logger.debug(f"Train/Val overlap: {len(train_topologies_set.intersection(val_topologies_set))}")
    logger.debug(f"Train/Test overlap: {len(train_topologies_set.intersection(test_topologies_set))}")
    logger.debug(f"Val/Test overlap: {len(val_topologies_set.intersection(test_topologies_set))}")

    train_data, val_data, test_data = {}, {}, {}
    assigned_instances = 0
    unassigned_instances = []

    for topology, instance_key_list in topology_groups.items():
        assigned_split = False
        # Check in order: Train, Val, Test
        if topology in train_topologies_set:
            target_dict = train_data
        elif topology in val_topologies_set:
            target_dict = val_data
        elif topology in test_topologies_set:
            target_dict = test_data
        else:
            target_dict = None # Should not happen if logic above is correct

        if target_dict is not None:
            for instance_key in instance_key_list:
                if instance_key in data:
                     target_dict[instance_key] = data[instance_key]
                     assigned_instances += 1
                     assigned_split = True # Mark that this topology was assigned
            if not assigned_split and instance_key_list: # Should not happen if target_dict is not None
                 logger.error(f"Internal error: Topology '{topology}' matched a split set but no instances were assigned.")
        else:
             # This case means the topology wasn't in any set, indicates boundary issue or overlap
             logger.warning(f"Topology '{topology}' with {len(instance_key_list)} instances was not assigned to any split! Instances: {instance_key_list[:5]}...")
             unassigned_instances.extend(instance_key_list)


    logger.info(f"Split instances: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")
    if assigned_instances != len(data):
        logger.warning(f"Mismatch in assigned instances ({assigned_instances}) vs total instances ({len(data)}). Unassigned count: {len(unassigned_instances)}")
        logger.debug(f"Unassigned keys sample: {unassigned_instances[:10]}")

    if not train_data: logger.warning("Training set is empty after split!")
    if not val_data and val_ratio > 0: logger.warning("Validation set is empty after split, although val_ratio > 0!")
    if not test_data and test_ratio > 0: logger.warning("Test set is empty after split, although test_ratio > 0!")

    return train_data, val_data, test_data

def calculate_feature_normalization_params(train_data: Dict[str, Dict], features: List[str]) -> Dict[str, Dict[str, float]]:
    """
    Calculate normalization parameters (min, max, mean, std) for each feature based on training data.
    Handles both per-residue (arrays) and global (scalar) features.
    """
    if not train_data or not features:
        logger.warning("No training data or features provided for normalization parameter calculation")
        return {}

    feature_values = {feature: [] for feature in features}
    feature_is_global = {feature: False for feature in features}

    logger.info("Gathering feature values from training data for normalization...")
    sample_keys = list(train_data.keys())[:min(10, len(train_data))]
    global_candidates = set(features)
    for key in sample_keys:
        domain_info = train_data[key]
        for feature in list(global_candidates):
             if feature in domain_info:
                  if isinstance(domain_info[feature], np.ndarray) and domain_info[feature].size > 1:
                       global_candidates.remove(feature)
             else:
                  if feature in global_candidates: global_candidates.remove(feature)

    for key in sample_keys:
         domain_info = train_data[key]
         for feature in list(global_candidates):
              if feature in domain_info:
                   val = domain_info[feature]
                   is_scalar = np.isscalar(val)
                   is_size_one_array = isinstance(val, np.ndarray) and val.size == 1
                   if not (is_scalar or is_size_one_array):
                        global_candidates.remove(feature)

    feature_is_global = {f: (f in global_candidates) for f in features}
    if any(feature_is_global.values()):
         logger.info(f"Identified potential global features: {[f for f, is_g in feature_is_global.items() if is_g]}")

    num_instances_processed = 0
    for instance_key, domain_info in train_data.items():
        num_instances_processed += 1
        for feature in features:
            if feature in domain_info:
                value = domain_info[feature]
                if feature_is_global[feature]:
                     scalar_val = value if np.isscalar(value) else value.item(0)
                     if pd.notna(scalar_val): feature_values[feature].append(scalar_val)
                elif isinstance(value, np.ndarray):
                     valid_values = value[~np.isnan(value)] # Filter NaNs during collection
                     if valid_values.size > 0: feature_values[feature].extend(valid_values.tolist())
                elif np.isscalar(value) and pd.notna(value):
                     feature_values[feature].append(value)

        if num_instances_processed % 1000 == 0:
             logger.debug(f"Processed {num_instances_processed} instances for feature normalization...")


    logger.info("Calculating normalization statistics...")
    norm_params = {}
    for feature in features:
        values_list = feature_values[feature]
        if values_list:
            values_np = np.array(values_list, dtype=np.float64)
            if np.isnan(values_np).any(): # Should be less likely now
                 logger.warning(f"NaNs found in feature '{feature}' during final calculation. Filtering them out.")
                 values_np = values_np[~np.isnan(values_np)]
                 if values_np.size == 0:
                      logger.warning(f"No valid numeric values left for feature '{feature}' after NaN filter.")
                      continue

            if values_np.size == 0:
                 logger.warning(f"No values collected for feature '{feature}'. Skipping normalization.")
                 continue

            feature_min = float(np.min(values_np))
            feature_max = float(np.max(values_np))
            feature_mean = float(np.mean(values_np))
            feature_std = float(np.std(values_np))

            if feature_std < 1e-9:
                logger.warning(f"Feature '{feature}' has near-zero standard deviation ({feature_std:.2e}). Normalization might be unstable. Min={feature_min}, Max={feature_max}")
                feature_std = 0.0

            norm_params[feature] = {
                'min': feature_min,
                'max': feature_max,
                'mean': feature_mean,
                'std': feature_std,
                'count': values_np.size,
                'is_global': feature_is_global[feature]
            }
            log_level = logging.DEBUG if values_np.size > 1000 else logging.INFO
            logger.log(log_level, f"Feature '{feature}' (Global={feature_is_global[feature]}): "
                       f"Count={values_np.size}, Min={feature_min:.4f}, Max={feature_max:.4f}, "
                       f"Mean={feature_mean:.4f}, Std={feature_std:.4f}")
        else:
            logger.warning(f"No values found for feature '{feature}'. Skipping normalization.")

    return norm_params


def save_split_data(data: Dict[str, Dict], output_dir: str, split_name: str, feature_list: Optional[List[str]] = None):
    """
    Save split data (instance list, FASTA, RMSF, Temp, Features) using instance_keys.
    """
    if not data:
        logger.warning(f"No data to save for split '{split_name}'. Skipping save.")
        return

    os.makedirs(output_dir, exist_ok=True)
    instance_keys = sorted(list(data.keys()))

    instance_list_path = os.path.join(output_dir, f"{split_name}_instances.txt")
    try:
        with open(instance_list_path, 'w') as f:
            for key in instance_keys:
                f.write(f"{key}\n")
        logger.info(f"Saved {len(instance_keys)} instance keys to {instance_list_path}")
    except IOError as e:
        logger.error(f"Error writing instance list {instance_list_path}: {e}")

    fasta_path = os.path.join(output_dir, f"{split_name}_sequences.fasta")
    sequences_saved = 0
    try:
        with open(fasta_path, 'w') as f:
            for key in instance_keys:
                instance_info = data.get(key, {})
                if 'sequence' in instance_info and instance_info['sequence']:
                    f.write(f">{key}\n{instance_info['sequence']}\n")
                    sequences_saved += 1
                else:
                    logger.warning(f"Missing or empty 'sequence' key for instance {key} when saving FASTA for split {split_name}.")
        logger.info(f"Saved {sequences_saved} sequences to {fasta_path}")
    except IOError as e:
        logger.error(f"Error writing FASTA file {fasta_path}: {e}")

    rmsf_path = os.path.join(output_dir, f"{split_name}_rmsf.npy")
    rmsf_data_to_save = {}
    for key in instance_keys:
        instance_info = data.get(key, {})
        if 'rmsf' in instance_info:
            rmsf_array = instance_info['rmsf']
            if isinstance(rmsf_array, np.ndarray) and rmsf_array.dtype == np.float32:
                rmsf_data_to_save[key] = rmsf_array
            else:
                try:
                    rmsf_data_to_save[key] = np.array(rmsf_array, dtype=np.float32)
                except Exception as conv_err:
                    logger.error(f"Could not convert RMSF data for instance {key} to numpy array: {conv_err}. Skipping RMSF for this instance.")
                    continue
        else:
            logger.warning(f"Missing 'rmsf' key for instance {key} when saving RMSF data for split {split_name}.")

    if rmsf_data_to_save:
        try:
            np.save(rmsf_path, rmsf_data_to_save, allow_pickle=True)
            logger.info(f"Saved RMSF data for {len(rmsf_data_to_save)} instances to {rmsf_path}")
        except Exception as e:
            logger.error(f"Error saving RMSF numpy file {rmsf_path}: {e}", exc_info=True)
    else:
        logger.warning(f"No valid RMSF data found to save for split {split_name}.")

    temp_path = os.path.join(output_dir, f"{split_name}_temperatures.npy")
    temp_data_to_save = {}
    for key in instance_keys:
        instance_info = data.get(key, {})
        if 'temperature' in instance_info:
            temp_val = instance_info['temperature']
            try:
                temp_data_to_save[key] = float(temp_val)
            except (ValueError, TypeError) as temp_err:
                logger.error(f"Could not convert temperature for instance {key} to float: Value='{temp_val}'. Error: {temp_err}. Skipping temperature for this instance.")
                continue
        else:
            logger.warning(f"Missing 'temperature' key for instance {key} when saving temperature data for split {split_name}.")

    if temp_data_to_save:
        try:
            np.save(temp_path, temp_data_to_save, allow_pickle=True)
            logger.info(f"Saved Temperature data for {len(temp_data_to_save)} instances to {temp_path}")
        except Exception as e:
            logger.error(f"Error saving Temperature numpy file {temp_path}: {e}", exc_info=True)
    else:
        logger.warning(f"No valid Temperature data found to save for split {split_name}.")

    if feature_list:
         logger.info(f"Saving structural features for split {split_name}: {feature_list}")
         features_found_in_data = set()
         if data:
              first_key = next(iter(data))
              features_found_in_data = set(data[first_key].keys())

         for feature in feature_list:
            if feature not in features_found_in_data:
                 logger.debug(f"Feature '{feature}' not found in processed data keys. Skipping save.")
                 continue

            feature_data_to_save = {}
            feature_is_global = None

            for key in instance_keys:
                instance_info = data.get(key, {})
                if feature in instance_info:
                    feature_val = instance_info[feature]

                    if feature_is_global is None:
                        feature_is_global = np.isscalar(feature_val) or (isinstance(feature_val, np.ndarray) and feature_val.size == 1)

                    if feature_is_global:
                         try:
                             scalar_val = float(feature_val.item(0)) if isinstance(feature_val, np.ndarray) else float(feature_val)
                             feature_data_to_save[key] = scalar_val
                         except (ValueError, TypeError) as feat_err:
                             logger.error(f"Could not process global feature '{feature}' for instance {key}. Value='{feature_val}'. Error: {feat_err}. Skipping.")
                             continue
                    else:
                         if isinstance(feature_val, np.ndarray) and feature_val.dtype == np.float32:
                              feature_data_to_save[key] = feature_val
                         else:
                              try:
                                   feature_data_to_save[key] = np.array(feature_val, dtype=np.float32)
                              except Exception as conv_err:
                                   logger.error(f"Could not convert feature '{feature}' for instance {key} to numpy array: {conv_err}. Skipping feature for this instance.")
                                   continue

            if feature_data_to_save:
                feature_path = os.path.join(output_dir, f"{split_name}_{feature}.npy")
                try:
                    np.save(feature_path, feature_data_to_save, allow_pickle=True)
                    logger.info(f"Saved '{feature}' data for {len(feature_data_to_save)} instances to {feature_path}")
                except Exception as e:
                    logger.error(f"Error saving '{feature}' numpy file {feature_path}: {e}", exc_info=True)
            else:
                logger.warning(f"No valid data found for feature '{feature}' in split {split_name}.")
    else:
         logger.info(f"No feature list provided. Skipping saving of individual feature files for split {split_name}.")


def calculate_and_save_temp_scaling(train_data: Dict[str, Dict], output_dir: str, filename: str):
    """
    Calculates min/max temperature from the training data instances and saves them.
    """
    if not train_data:
        logger.error("No training data provided. Cannot calculate temperature scaling parameters.")
        return

    temps = []
    for instance_key, instance_info in train_data.items():
        if 'temperature' in instance_info:
            try:
                temps.append(float(instance_info['temperature']))
            except (ValueError, TypeError):
                 logger.warning(f"Invalid temperature value '{instance_info['temperature']}' for instance {instance_key}. Skipping for scaling calculation.")

    if not temps:
        logger.error("No valid temperature values found in training data. Cannot calculate scaling parameters.")
        return

    temp_min = float(np.min(temps))
    temp_max = float(np.max(temps))
    if temp_min > temp_max: temp_min, temp_max = temp_max, temp_min

    scaling_params = {'temp_min': temp_min, 'temp_max': temp_max}
    save_path = os.path.join(output_dir, filename)

    try:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w') as f:
            json.dump(scaling_params, f, indent=4)
        logger.info(f"Calculated temperature scaling params (Min={temp_min:.2f}, Max={temp_max:.2f}) using {len(temps)} training instance temperatures.")
        logger.info(f"Saved temperature scaling parameters to {save_path}")
    except IOError as e:
        logger.error(f"Error saving temperature scaling parameters to {save_path}: {e}")
    except Exception as e:
        logger.error(f"Unexpected error saving temperature scaling parameters: {e}", exc_info=True)

def save_feature_normalization_params(norm_params: Dict[str, Dict[str, float]], output_dir: str, filename: str):
    """
    Save feature normalization parameters to a JSON file.
    """
    if not norm_params:
        logger.warning("No normalization parameters to save.")
        return

    save_path = os.path.join(output_dir, filename)
    try:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w') as f:
            serializable_params = {}
            for feature, params in norm_params.items():
                 # Convert potentially numpy types (like count) to standard python types
                 serializable_params[feature] = {k: (int(v) if isinstance(v, (np.integer, int)) else float(v))
                                                   if isinstance(v, (int, float, np.number)) else v
                                                   for k, v in params.items()}

            json.dump(serializable_params, f, indent=4)
        logger.info(f"Saved feature normalization parameters for {len(norm_params)} features to {save_path}")
    except IOError as e:
        logger.error(f"Error saving feature normalization parameters to {save_path}: {e}")
    except Exception as e:
        logger.error(f"Unexpected error saving feature normalization parameters: {e}", exc_info=True)

# --- Main Processing Function ---
def process_data(csv_path: str, output_dir: str, temp_scaling_filename: str, config: Dict = None, train_ratio=0.85, val_ratio=0.075, seed=42): # <-- Adjusted defaults
    """Main function to process RMSF data, extract features, create splits, and save normalization params."""
    test_ratio = max(0.0, 1.0 - train_ratio - val_ratio)
    logger.info(f"--- Starting Data Processing Pipeline (Handles Multiple Temps, Imputes NaN, Skips Gaps) ---")
    logger.info(f"Input CSV: {csv_path}")
    logger.info(f"Output Directory: {output_dir}")
    # Use updated ratios in log message
    logger.info(f"Split Ratios (Topology-based): Train={train_ratio:.3f}, Val={val_ratio:.3f}, Test={test_ratio:.3f}")
    logger.info(f"Random Seed: {seed}")
    logger.info(f"Temp Scaling Filename: {temp_scaling_filename}")

    try:
        df = load_data(csv_path, config)
        if df is None: raise ValueError("Failed to load data.")

        instance_groups = group_by_domain_and_temp(df)
        if not instance_groups: raise ValueError("Failed to group data by domain and temperature.")

        # Includes NaN imputation and gap skipping
        data = extract_sequence_rmsf_temp_features(instance_groups, config)
        if not data: raise ValueError("No valid instance data extracted after cleaning.")

        # Pass potentially adjusted ratios
        train_data, val_data, test_data = split_by_topology(data, train_ratio, val_ratio, seed)

        if not train_data:
             logger.error("Training data split is empty. Cannot proceed with normalization or saving.")
             return None, None, None

        # Determine features available for normalization from the actual training data
        first_train_key = next(iter(train_data))
        available_features = [k for k in train_data[first_train_key].keys() if k not in ['sequence', 'rmsf', 'temperature']]
        logger.info(f"Features available for normalization (based on first train instance): {available_features}")

        features_to_normalize = []
        if config and config.get('data', {}).get('features', {}):
            feature_config = config['data']['features']
            # Check config flags against available features
            if feature_config.get('use_position_info', True) and 'normalized_resid' in available_features: features_to_normalize.append('normalized_resid')
            if feature_config.get('use_structure_info', True):
                if 'core_exterior_encoded' in available_features: features_to_normalize.append('core_exterior_encoded')
                if 'secondary_structure_encoded' in available_features: features_to_normalize.append('secondary_structure_encoded')
            if feature_config.get('use_accessibility', True) and 'relative_accessibility' in available_features: features_to_normalize.append('relative_accessibility')
            if feature_config.get('use_backbone_angles', True):
                if 'phi_norm' in available_features: features_to_normalize.append('phi_norm')
                if 'psi_norm' in available_features: features_to_normalize.append('psi_norm')
            if feature_config.get('use_protein_size', True) and 'protein_size' in available_features: features_to_normalize.append('protein_size')
            if feature_config.get('use_voxel_rmsf', True) and 'voxel_rmsf' in available_features: features_to_normalize.append('voxel_rmsf')
            if feature_config.get('use_bfactor', True) and 'bfactor_norm' in available_features: features_to_normalize.append('bfactor_norm')
        else:
            features_to_normalize = available_features

        logger.info(f"Final list of features selected for normalization: {features_to_normalize}")

        norm_params = calculate_feature_normalization_params(train_data, features_to_normalize)

        if norm_params and config and 'data' in config and 'features' in config['data']:
            normalization_params_file = config['data']['features'].get('normalization_params_file', 'feature_normalization.json')
            save_feature_normalization_params(norm_params, output_dir, normalization_params_file)

        # Save Splits
        save_split_data(train_data, output_dir, 'train', features_to_normalize)
        save_split_data(val_data, output_dir, 'val', features_to_normalize)
        save_split_data(test_data, output_dir, 'test', features_to_normalize)

        calculate_and_save_temp_scaling(train_data, output_dir, temp_scaling_filename)

        logger.info("--- Data Processing Completed Successfully ---")
        return train_data, val_data, test_data

    except FileNotFoundError as e:
        logger.error(f"Processing failed: {e}")
        return None, None, None
    except ValueError as e:
        logger.error(f"Processing failed: {e}")
        return None, None, None
    except Exception as e:
        logger.error(f"An unexpected error occurred during data processing: {e}", exc_info=True)
        return None, None, None


if __name__ == "__main__":
    import argparse
    import sys
    parser = argparse.ArgumentParser(description='Process protein RMSF data (multi-temp aware), extract features, split by topology, impute NaNs, skip gaps.')
    parser.add_argument('--config', type=str, default='config.yaml', help='Path to the configuration file.')
    parser.add_argument('--csv', type=str, required=True, help='Path to the input enriched RMSF CSV file.')
    parser.add_argument('--output', type=str, default='data/processed', help='Directory to save the processed data splits and scaling info.')
    parser.add_argument('--scaling_file', type=str, default='temp_scaling_params.json', help='Filename for saving temperature scaling parameters (min/max).')
    # --- Adjusted default split ratios ---
    parser.add_argument('--train_ratio', type=float, default=0.85, help='Fraction of topologies for the training set.')
    parser.add_argument('--val_ratio', type=float, default=0.075, help='Fraction of topologies for the validation set.')
    # Test ratio is inferred: 1.0 - train_ratio - val_ratio
    # --- End adjustment ---
    parser.add_argument('--seed', type=int, default=42, help='Random seed for shuffling topologies.')
    args = parser.parse_args()

    config_data = None
    if args.config:
        if not os.path.exists(args.config):
             logger.error(f"Configuration file not found: {args.config}")
             sys.exit(1)
        try:
            import yaml
            with open(args.config, 'r') as f:
                config_data = yaml.safe_load(f)
            logger.info(f"Successfully loaded configuration from {args.config}")
        except ImportError:
             logger.error("PyYAML is not installed. Cannot load config file. Please install with 'pip install pyyaml'.")
             sys.exit(1)
        except yaml.YAMLError as e:
            logger.error(f"Error parsing config file {args.config}: {e}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"Failed to load config file {args.config}: {e}", exc_info=True)
            logger.warning("Proceeding without loaded configuration (using defaults and CLI args).")

    if not args.csv or not os.path.exists(args.csv):
         logger.error(f"Input CSV file not found or not specified: {args.csv}")
         sys.exit(1)

    # Use CLI args for ratios, falling back to new defaults if not provided
    train_r = args.train_ratio
    val_r = args.val_ratio

    process_data(
        csv_path=args.csv,
        output_dir=args.output,
        temp_scaling_filename=args.scaling_file,
        config=config_data,
        train_ratio=train_r, # Pass value from args (or its default)
        val_ratio=val_r,   # Pass value from args (or its default)
        seed=args.seed
    )


---------------------------------------------------------
===== FILE: ./data/raw/fix_data_.py =====
# This script was originally used to standardize residue names (e.g., HIS variants).
# Keep or adapt if needed for your new aggregated dataset *before* running the main 'process' command.
import logging
import argparse

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

if __name__ == "__main__":
    logger.info("This is a placeholder for the data fixing script.")
    logger.info("If your aggregated CSV requires preprocessing (like standardizing residue names),")
    logger.info("implement the logic here and run it before using 'main.py process'.")
    # Example:
    # parser = argparse.ArgumentParser()
    # parser.add_argument('--input', required=True)
    # parser.add_argument('--output', required=True)
    # args = parser.parse_args()
    # logger.info(f"Processing {args.input} to {args.output}...")
    # # Add processing logic here
    # logger.info("Processing finished (placeholder).")


---------------------------------------------------------
===== FILE: ./dataset.py =====
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import random
import logging
import json
from typing import List, Dict, Tuple, Optional, Any
from collections import defaultdict

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedRMSFDataset(Dataset):
    """
    PyTorch Dataset for Enhanced Temperature-Aware RMSF prediction.

    Handles loading sequences, target RMSF values, temperatures, and structural features.
    """
    def __init__(self,
                 domain_ids: List[str],
                 sequences: Dict[str, str],
                 rmsf_values: Dict[str, np.ndarray],
                 temperatures: Dict[str, float],
                 feature_data: Dict[str, Dict[str, np.ndarray]],
                 feature_norm_params: Optional[Dict[str, Dict[str, float]]] = None,
                 config: Optional[Dict] = None):
        """
        Initialize the enhanced dataset.

        Args:
            domain_ids: Ordered list of domain IDs for this dataset split.
            sequences: Dictionary mapping domain IDs to amino acid sequences.
            rmsf_values: Dictionary mapping domain IDs to target RMSF values (NumPy arrays).
            temperatures: Dictionary mapping domain IDs to temperature values (float).
            feature_data: Dictionary mapping domain IDs to feature data (dictionaries of feature name to NumPy arrays).
            feature_norm_params: Optional dictionary of normalization parameters for each feature.
            config: Optional configuration dictionary.
        """
        self.domain_ids = domain_ids
        self.sequences = sequences
        self.rmsf_values = rmsf_values  # Target RMSF
        self.temperatures = temperatures
        self.feature_data = feature_data
        self.feature_norm_params = feature_norm_params
        self.config = config or {}
        
        # Get list of enabled features from configuration
        self.enabled_features = []
        feature_config = self.config.get('data', {}).get('features', {})
        
        # Position information
        if feature_config.get('use_position_info', True) and 'normalized_resid' in self.get_available_features():
            self.enabled_features.append('normalized_resid')
        
        # Structure information
        if feature_config.get('use_structure_info', True):
            if 'core_exterior_encoded' in self.get_available_features():
                self.enabled_features.append('core_exterior_encoded')
            if 'secondary_structure_encoded' in self.get_available_features():
                self.enabled_features.append('secondary_structure_encoded')
        
        # Accessibility
        if feature_config.get('use_accessibility', True) and 'relative_accessibility' in self.get_available_features():
            self.enabled_features.append('relative_accessibility')
        
        # Backbone angles
        if feature_config.get('use_backbone_angles', True):
            if 'phi_norm' in self.get_available_features():
                self.enabled_features.append('phi_norm')
            if 'psi_norm' in self.get_available_features():
                self.enabled_features.append('psi_norm')
        
        # Protein size (global feature)
        if feature_config.get('use_protein_size', True) and 'protein_size' in self.get_available_features():
            self.enabled_features.append('protein_size')
        
        # Additional predictive features
        if feature_config.get('use_voxel_rmsf', True) and 'voxel_rmsf' in self.get_available_features():
            self.enabled_features.append('voxel_rmsf')
        
        if feature_config.get('use_bfactor', True) and 'bfactor_norm' in self.get_available_features():
            self.enabled_features.append('bfactor_norm')

        logger.info(f"Enabled features: {self.enabled_features}")
        
        # Data Consistency Check
        valid_domain_ids = []
        removed_count = 0
        for did in list(self.domain_ids):  # Iterate over a copy
            # Check for sequence, RMSF, AND temperature
            if did in self.sequences and did in self.rmsf_values and did in self.temperatures:
                # Basic length check remains useful
                if len(self.sequences[did]) != len(self.rmsf_values[did]):
                    logger.warning(f"Length mismatch for {did}: Seq={len(self.sequences[did])}, RMSF={len(self.rmsf_values[did])}. Removing.")
                    removed_count += 1
                # Check if temperature is valid
                elif self.temperatures[did] is None or np.isnan(self.temperatures[did]):
                    logger.warning(f"Invalid temperature for {did}: {self.temperatures[did]}. Removing.")
                    removed_count += 1
                # Check if all enabled features are available for this domain
                elif not self._check_features_available(did):
                    logger.warning(f"Missing features for {did}. Removing.")
                    removed_count += 1
                else:
                    valid_domain_ids.append(did)  # Keep if all checks pass
            else:
                logger.warning(f"Domain ID {did} missing sequence, RMSF, or temperature. Removing.")
                removed_count += 1

        if removed_count > 0:
            logger.info(f"Removed {removed_count} domain IDs from dataset due to missing/inconsistent data.")
            self.domain_ids = valid_domain_ids

        # Calculate and log dataset statistics
        self._log_stats()

    def get_available_features(self) -> List[str]:
        """Get list of available features in the dataset."""
        available_features = set()
        # Check the first domain for available features
        for did in self.domain_ids:
            if did in self.feature_data:
                for feature in self.feature_data[did]:
                    available_features.add(feature)
            break
        return sorted(list(available_features))

    def _check_features_available(self, domain_id: str) -> bool:
        """Check if all enabled features are available for a domain."""
        if domain_id not in self.feature_data:
            return False
        
        for feature in self.enabled_features:
            if feature not in self.feature_data[domain_id]:
                return False
        
        return True

    def _normalize_feature(self, feature_name: str, feature_value: np.ndarray) -> np.ndarray:
        """
        Normalize a feature using stored normalization parameters.
        
        Args:
            feature_name: Name of the feature to normalize
            feature_value: Feature value array or scalar
            
        Returns:
            Normalized feature value
        """
        if not self.feature_norm_params or feature_name not in self.feature_norm_params:
            return feature_value
        
        params = self.feature_norm_params[feature_name]
        
        # Min-max normalization
        if 'min' in params and 'max' in params:
            feature_min = params['min']
            feature_max = params['max']
            
            # Handle case where min == max (avoid division by zero)
            if feature_max - feature_min < 1e-8:
                return np.zeros_like(feature_value, dtype=np.float32)
            
            return (feature_value - feature_min) / (feature_max - feature_min)
        
        # Z-score normalization
        elif 'mean' in params and 'std' in params:
            feature_mean = params['mean']
            feature_std = params['std']
            
            # Handle case where std is near zero
            if feature_std < 1e-8:
                return np.zeros_like(feature_value, dtype=np.float32)
            
            return (feature_value - feature_mean) / feature_std
        
        # No normalization if parameters are incomplete
        return feature_value

    def _log_stats(self):
        """Log statistics about the loaded dataset."""
        if not self.domain_ids:
            logger.warning("Dataset created with 0 proteins.")
            return

        num_proteins = len(self.domain_ids)
        logger.info(f"Dataset created with {num_proteins} proteins.")
        try:
            seq_lengths = [len(self.sequences[did]) for did in self.domain_ids]
            rmsf_lengths = [len(self.rmsf_values[did]) for did in self.domain_ids]
            temp_values = [self.temperatures[did] for did in self.domain_ids]

            logger.info(f"  Sequence length stats: Min={min(seq_lengths)}, Max={max(seq_lengths)}, " +
                        f"Mean={np.mean(seq_lengths):.1f}, Median={np.median(seq_lengths):.1f}")
            logger.info(f"  RMSF length stats:     Min={min(rmsf_lengths)}, Max={max(rmsf_lengths)}, " +
                        f"Mean={np.mean(rmsf_lengths):.1f}, Median={np.median(rmsf_lengths):.1f}")
            logger.info(f"  Temperature stats:     Min={min(temp_values):.1f}, Max={max(temp_values):.1f}, " +
                        f"Mean={np.mean(temp_values):.1f}, Median={np.median(temp_values):.1f}")

            if np.mean(seq_lengths) != np.mean(rmsf_lengths):
                logger.warning("Mean sequence length differs from mean RMSF length. Verify processing.")
                
            # Log feature statistics
            for feature in self.enabled_features:
                feature_stats = []
                for did in self.domain_ids:
                    if did in self.feature_data and feature in self.feature_data[did]:
                        feature_val = self.feature_data[did][feature]
                        if isinstance(feature_val, np.ndarray) and feature_val.size > 0:
                            feature_stats.append(np.mean(feature_val))
                        elif np.isscalar(feature_val):
                            feature_stats.append(feature_val)
                
                if feature_stats:
                    logger.info(f"  {feature} stats:     Min={min(feature_stats):.4f}, Max={max(feature_stats):.4f}, " +
                                f"Mean={np.mean(feature_stats):.4f}, Median={np.median(feature_stats):.4f}")
                
        except Exception as e:
            logger.error(f"Error calculating dataset statistics: {e}", exc_info=True)

    def __len__(self) -> int:
        return len(self.domain_ids)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        Get data for a single protein by index.

        Args:
            idx: Index of the protein.

        Returns:
            Dictionary containing:
              - 'domain_id': The domain identifier (string).
              - 'sequence': The amino acid sequence (string).
              - 'rmsf': The target RMSF values (NumPy array of float32).
              - 'temperature': The temperature value (float).
              - 'features': Dictionary of feature arrays (if enabled).
        """
        if idx < 0 or idx >= len(self.domain_ids):
            raise IndexError(f"Index {idx} out of bounds for dataset size {len(self.domain_ids)}")

        domain_id = self.domain_ids[idx]

        # Retrieve data, handling potential KeyError if consistency check failed unexpectedly
        try:
            sequence = self.sequences[domain_id]
            rmsf = self.rmsf_values[domain_id]
            temperature = self.temperatures[domain_id]
        except KeyError as e:
            logger.error(f"Data inconsistency: Cannot find '{e}' for domain ID {domain_id} at index {idx}. Was it filtered out?")
            raise RuntimeError(f"Inconsistent dataset state: Missing data for {domain_id}") from e

        # Ensure RMSF is float32
        if rmsf.dtype != np.float32:
            rmsf = rmsf.astype(np.float32)
            
        # Collect and normalize feature data
        features = {}
        for feature in self.enabled_features:
            if domain_id in self.feature_data and feature in self.feature_data[domain_id]:
                feature_val = self.feature_data[domain_id][feature]
                
                # Ensure feature value is a numpy array of float32
                if np.isscalar(feature_val):
                    # Handle global features (like protein_size)
                    feature_val = np.array([feature_val], dtype=np.float32)
                elif not isinstance(feature_val, np.ndarray):
                    feature_val = np.array(feature_val, dtype=np.float32)
                elif feature_val.dtype != np.float32:
                    feature_val = feature_val.astype(np.float32)
                
                # Normalize the feature if normalization parameters are available
                normalized_val = self._normalize_feature(feature, feature_val)
                features[feature] = normalized_val

        return {
            'domain_id': domain_id,
            'sequence': sequence,
            'rmsf': rmsf,  # This is the TARGET RMSF
            'temperature': float(temperature),  # Ensure float type
            'features': features  # Dictionary of feature arrays
        }

def load_sequences_from_fasta(fasta_path: str) -> Dict[str, str]:
    """Loads sequences from a FASTA file."""
    sequences = {}
    current_id = None
    current_seq = ""
    try:
        with open(fasta_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                if line.startswith('>'):
                    if current_id is not None:
                        sequences[current_id] = current_seq
                    current_id = line[1:].split()[0]  # Use ID before first space
                    current_seq = ""
                else:
                    current_seq += line.upper()
            if current_id is not None:  # Add last sequence
                sequences[current_id] = current_seq
        logger.info(f"Loaded {len(sequences)} sequences from {fasta_path}")
    except FileNotFoundError:
        logger.error(f"FASTA file not found: {fasta_path}")
        raise
    except Exception as e:
        logger.error(f"Error reading FASTA file {fasta_path}: {e}")
        raise
    return sequences

def load_numpy_dict(npy_path: str) -> Dict[str, Any]:
    """Loads a dictionary saved as a NumPy file."""
    if not os.path.exists(npy_path):
        logger.error(f"NumPy file not found: {npy_path}")
        raise FileNotFoundError(f"NumPy file not found: {npy_path}")
    try:
        # allow_pickle=True is required for loading dictionaries
        loaded_data = np.load(npy_path, allow_pickle=True).item()
        # Ensure keys are strings for consistency
        string_key_data = {str(k): v for k, v in loaded_data.items()}
        logger.info(f"Loaded {len(string_key_data)} entries from {npy_path}")
        return string_key_data
    except Exception as e:
        logger.error(f"Error loading or processing NumPy dictionary from {npy_path}: {e}")
        raise

def load_feature_norm_params(json_path: str) -> Dict[str, Dict[str, float]]:
    """Load feature normalization parameters from a JSON file."""
    if not os.path.exists(json_path):
        logger.warning(f"Feature normalization parameters file not found: {json_path}")
        return {}
    
    try:
        with open(json_path, 'r') as f:
            norm_params = json.load(f)
        logger.info(f"Loaded normalization parameters for {len(norm_params)} features from {json_path}")
        return norm_params
    except Exception as e:
        logger.error(f"Error loading feature normalization parameters from {json_path}: {e}")
        return {}

def load_split_data(data_dir: str, split: str, config: Optional[Dict] = None) -> Tuple[List[str], Dict[str, str], Dict[str, np.ndarray], Dict[str, float], Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]:
    """
    Load data (domain IDs, sequences, RMSF values, temperatures, feature data) for a specific split.

    Args:
        data_dir: Directory containing the processed data files.
        split: Split name ('train', 'val', or 'test').
        config: Optional configuration dictionary.

    Returns:
        Tuple of (domain_ids, sequences, rmsf_values, temperatures, feature_data, feature_norm_params).
        Returns ([], {}, {}, {}, {}, {}) if data loading fails for essential components.
    """
    logger.info(f"--- Loading {split} data from directory: {data_dir} ---")
    sequences, rmsf_values, temperatures = {}, {}, {}
    feature_data = {}
    domain_ids = []
    feature_norm_params = {}

    try:
        # --- Load normalization parameters if available ---
        norm_params_file = None
        if config and 'data' in config and 'features' in config['data']:
            norm_params_file = config['data']['features'].get('normalization_params_file', 'feature_normalization.json')
        
        if norm_params_file:
            norm_params_path = os.path.join(data_dir, norm_params_file)
            feature_norm_params = load_feature_norm_params(norm_params_path)
        
        # --- Load domain IDs (essential) ---
                # --- Load instance keys (essential) --- # Changed comment for clarity
        instance_keys_path = os.path.join(data_dir, f"{split}_instances.txt") # <-- CHANGED FILENAME
        if not os.path.exists(instance_keys_path):
            logger.error(f"Instance key file not found: {instance_keys_path}") # <-- CHANGED ERROR MSG
            return [], {}, {}, {}, {}, {}
        with open(instance_keys_path, 'r') as f:
            # Still store in variable called domain_ids internally, or rename if preferred (e.g., instance_keys_list)
            # Sticking with domain_ids is fine as long as we remember it holds 'domain_id@temp' keys
            domain_ids = [line.strip() for line in f if line.strip()]
        if not domain_ids:
            logger.warning(f"Instance key file is empty: {instance_keys_path}") # <-- CHANGED WARNING MSG
        logger.info(f"Loaded {len(domain_ids)} instance keys from {instance_keys_path}") # <-- CHANGED INFO MSG

        # --- Load sequences (essential) ---
        sequences_path = os.path.join(data_dir, f"{split}_sequences.fasta")
        sequences = load_sequences_from_fasta(sequences_path)
        if not sequences:
            logger.error(f"No sequences loaded from required file: {sequences_path}")
            return [], {}, {}, {}, {}, {}  # Treat as fatal if no sequences

        # --- Load RMSF values (essential) ---
        rmsf_path = os.path.join(data_dir, f"{split}_rmsf.npy")
        rmsf_dict = load_numpy_dict(rmsf_path)
        # Ensure values are float32 numpy arrays
        rmsf_values = {k: np.array(v, dtype=np.float32) for k, v in rmsf_dict.items()}
        if not rmsf_values:
            logger.error(f"No RMSF data loaded from required file: {rmsf_path}")
            return [], {}, {}, {}, {}, {}  # Treat as fatal

        # --- Load Temperatures (essential) ---
        temperatures_path = os.path.join(data_dir, f"{split}_temperatures.npy")
        temp_dict = load_numpy_dict(temperatures_path)
        # Ensure values are floats
        temperatures = {k: float(v) for k, v in temp_dict.items()}
        if not temperatures:
            logger.error(f"No Temperature data loaded from required file: {temperatures_path}")
            return [], {}, {}, {}, {}, {}  # Treat as fatal
        
        # --- Load Structural Features (if available) ---
        # Initialize feature_data dictionary for each domain
        for domain_id in domain_ids:
            feature_data[domain_id] = {}
        
        # Determine which features to load based on config
        features_to_load = []
        if config and config.get('data', {}).get('features', {}):
            feature_config = config['data']['features']
            
            if feature_config.get('use_position_info', True):
                features_to_load.append('normalized_resid')
            
            if feature_config.get('use_structure_info', True):
                features_to_load.extend(['core_exterior_encoded', 'secondary_structure_encoded'])
            
            if feature_config.get('use_accessibility', True):
                features_to_load.append('relative_accessibility')
            
            if feature_config.get('use_backbone_angles', True):
                features_to_load.extend(['phi_norm', 'psi_norm'])
            
            if feature_config.get('use_protein_size', True):
                features_to_load.append('protein_size')
            
            if feature_config.get('use_voxel_rmsf', True):
                features_to_load.append('voxel_rmsf')
            
            if feature_config.get('use_bfactor', True):
                features_to_load.append('bfactor_norm')
        else:
            # If no config, try to load all common structural features
            features_to_load = [
                'normalized_resid', 'core_exterior_encoded', 'secondary_structure_encoded',
                'relative_accessibility', 'phi_norm', 'psi_norm', 'protein_size',
                'voxel_rmsf', 'bfactor_norm'
            ]
        
        # Load each feature file if it exists
        for feature in features_to_load:
            feature_path = os.path.join(data_dir, f"{split}_{feature}.npy")
            if os.path.exists(feature_path):
                try:
                    feature_dict = load_numpy_dict(feature_path)
                    for domain_id, feature_val in feature_dict.items():
                        if domain_id in feature_data:
                            feature_data[domain_id][feature] = feature_val
                    logger.info(f"Loaded {feature} data for {len(feature_dict)} domains")
                except Exception as e:
                    logger.warning(f"Error loading {feature} data: {e}")
            else:
                logger.warning(f"{feature} data file not found: {feature_path}")

    except FileNotFoundError as e:
        logger.error(f"Failed to load essential data file: {e}")
        return [], {}, {}, {}, {}, {}
    except Exception as e:
        logger.error(f"An error occurred during data loading for split '{split}': {e}", exc_info=True)
        return [], {}, {}, {}, {}, {}

    # --- Verify data consistency across all loaded components ---
    logger.info("Verifying data consistency for split '{}'...".format(split))
    original_domain_count = len(domain_ids)
    valid_domain_ids = []
    missing_data_counts = defaultdict(int)
    length_mismatches = 0

    for did in domain_ids:
        has_seq = did in sequences
        has_rmsf = did in rmsf_values
        has_temp = did in temperatures
        has_features = did in feature_data and len(feature_data[did]) > 0

        if has_seq and has_rmsf and has_temp:
            # Check sequence-RMSF length consistency
            seq_len = len(sequences[did])
            rmsf_len = len(rmsf_values[did])
            if seq_len == rmsf_len:
                # Check temperature validity
                if temperatures[did] is not None and not np.isnan(temperatures[did]):
                    # If we require features, check they are available
                    if config and config.get('model', {}).get('architecture', {}).get('use_enhanced_features', True):
                        if has_features:
                            valid_domain_ids.append(did)
                        else:
                            missing_data_counts['missing_features'] += 1
                            logger.debug(f"Missing features for {did}. Removing.")
                    else:
                        # Features not required
                        valid_domain_ids.append(did)
                else:
                    missing_data_counts['invalid_temp'] += 1
                    logger.debug(f"Invalid temperature for {did}. Removing.")
            else:
                length_mismatches += 1
                logger.debug(f"Length mismatch for {did}: seq={seq_len}, RMSF={rmsf_len}. Removing.")
        else:
            if not has_seq: missing_data_counts['sequence'] += 1; logger.debug(f"Missing sequence for {did}")
            if not has_rmsf: missing_data_counts['rmsf'] += 1; logger.debug(f"Missing RMSF for {did}")
            if not has_temp: missing_data_counts['temperature'] += 1; logger.debug(f"Missing temperature for {did}")

    logger.info(f"Initial domain IDs in list: {original_domain_count}")
    if sum(missing_data_counts.values()) > 0:
        logger.warning(f"Missing data counts: {dict(missing_data_counts)}")
    if length_mismatches > 0:
        logger.warning(f"Found {length_mismatches} domains with sequence-RMSF length mismatches.")

    final_domain_count = len(valid_domain_ids)
    if final_domain_count != original_domain_count:
        removed_count = original_domain_count - final_domain_count
        logger.info(f"Removed {removed_count} domains due to inconsistencies.")
        logger.info(f"Final number of valid, consistent domains for split '{split}': {final_domain_count}")

    # Filter all dictionaries to only include valid domains
    final_sequences = {did: sequences[did] for did in valid_domain_ids if did in sequences}
    final_rmsf_values = {did: rmsf_values[did] for did in valid_domain_ids if did in rmsf_values}
    final_temperatures = {did: temperatures[did] for did in valid_domain_ids if did in temperatures}
    final_feature_data = {did: feature_data.get(did, {}) for did in valid_domain_ids}

    if final_domain_count == 0:
        logger.error(f"No valid, consistent data found for split '{split}' after filtering. Please check the processed data files in {data_dir}.")
        # Return empty structures to avoid downstream errors
        return [], {}, {}, {}, {}, {}

    logger.info(f"--- Successfully loaded and verified {final_domain_count} samples for split '{split}' ---")
    return valid_domain_ids, final_sequences, final_rmsf_values, final_temperatures, final_feature_data, feature_norm_params

def enhanced_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Custom collate function for the Enhanced DataLoader.

    Batches domain IDs, sequences, RMSF values (as Tensors), temperatures (as Tensors),
    and feature data. Padding is NOT done here.

    Args:
        batch: List of items from the EnhancedRMSFDataset

    Returns:
        Dictionary of batched data
    """
    domain_ids = [item['domain_id'] for item in batch]
    sequences = [item['sequence'] for item in batch]
    # Convert RMSF numpy arrays to tensors (target values)
    rmsf_values = [torch.tensor(item['rmsf'], dtype=torch.float32) for item in batch]
    # Extract and convert temperatures to a tensor
    temperatures = torch.tensor([item['temperature'] for item in batch], dtype=torch.float32)
    
    # Process feature data
    # First, determine which features are available in all batch items
    feature_names = set()
    for item in batch:
        feature_names.update(item['features'].keys())
    feature_names = sorted(list(feature_names))
    
    # Initialize feature tensors
    feature_tensors = {}
    for feature in feature_names:
        # Check if this is a per-residue feature or a global feature
        is_global_feature = all(
            feature in item['features'] and len(item['features'][feature].shape) == 1 and item['features'][feature].shape[0] == 1
            for item in batch if feature in item['features']
        )
        
        if is_global_feature:
            # Global feature - create a batch tensor
            global_values = []
            for item in batch:
                if feature in item['features']:
                    # Extract scalar value from the single-element array
                    global_values.append(float(item['features'][feature][0]))
                else:
                    # Use 0 as a placeholder if feature is missing
                    global_values.append(0.0)
            feature_tensors[feature] = torch.tensor(global_values, dtype=torch.float32)
        else:
            # Per-residue feature - store as list of tensors
            per_residue_values = []
            for item in batch:
                if feature in item['features']:
                    per_residue_values.append(torch.tensor(item['features'][feature], dtype=torch.float32))
                else:
                    # Use empty tensor if feature is missing for this item
                    # Length will be aligned with sequence in the model
                    per_residue_values.append(torch.tensor([], dtype=torch.float32))
            feature_tensors[feature] = per_residue_values

    return {
        'domain_ids': domain_ids,
        'sequences': sequences,
        'rmsf_values': rmsf_values,  # List of Tensors (targets)
        'temperatures': temperatures,  # Tensor of shape [batch_size] (input features)
        'features': feature_tensors  # Dictionary of feature tensors
    }

def create_enhanced_dataloader(
    data_dir: str,
    split: str,
    batch_size: int,
    shuffle: bool = True,
    max_length: Optional[int] = None,
    length_bucket_size: int = 50,
    num_workers: int = 0,
    config: Optional[Dict] = None
) -> Optional[DataLoader]:
    """
    Creates a PyTorch DataLoader for the Enhanced RMSF Dataset with length-based batching.

    Args:
        data_dir: Directory containing the processed data splits.
        split: Split name ('train', 'val', or 'test').
        batch_size: Target number of sequences per batch.
        shuffle: Whether to shuffle data.
        max_length: Optional maximum sequence length for filtering.
        length_bucket_size: Size of length ranges for grouping.
        num_workers: Number of worker processes.
        config: Optional configuration dictionary.

    Returns:
        A PyTorch DataLoader instance, or None if data loading fails.
    """
    # 1. Load data (including temperatures and features)
    domain_ids, sequences, rmsf_values, temperatures, feature_data, feature_norm_params = load_split_data(data_dir, split, config)

    if not domain_ids:
        logger.error(f"Failed to load any valid data for split '{split}'. Cannot create DataLoader.")
        return None

    # 2. Filter by max length if specified
    if max_length is not None and max_length > 0:
        original_count = len(domain_ids)
        # Keep only IDs whose sequences are <= max_length
        filtered_domain_ids = [
            did for did in domain_ids if len(sequences.get(did, '')) <= max_length
        ]
        filtered_count = len(filtered_domain_ids)
        if filtered_count < original_count:
            logger.info(f"Filtered out {original_count - filtered_count} sequences " +
                       f"longer than {max_length} residues for split '{split}'.")
            domain_ids = filtered_domain_ids
            # Filter all dictionaries based on the remaining domain_ids
            sequences = {did: sequences[did] for did in domain_ids if did in sequences}
            rmsf_values = {did: rmsf_values[did] for did in domain_ids if did in rmsf_values}
            temperatures = {did: temperatures[did] for did in domain_ids if did in temperatures}
            feature_data = {did: feature_data[did] for did in domain_ids if did in feature_data}

        if not domain_ids:
            logger.warning(f"No sequences remaining after filtering by max_length={max_length} for split '{split}'. Cannot create DataLoader.")
            return None

    # 3. Group domain IDs by length buckets
    length_buckets = defaultdict(list)
    for did in domain_ids:
        # Use sequence length for bucketing
        seq_len = len(sequences.get(did, ''))
        if seq_len > 0:  # Avoid bucketing empty sequences if any slipped through
            bucket_idx = seq_len // length_bucket_size
            length_buckets[bucket_idx].append(did)
        else:
            logger.warning(f"Domain ID {did} has zero length sequence during bucketing. Skipping.")

    if not length_buckets:
        logger.error(f"No non-empty sequences found to create length buckets for split '{split}'. Cannot create DataLoader.")
        return None

    logger.info(f"Grouped {len(domain_ids)} sequences into {len(length_buckets)} length buckets.")

    # 4. Create batches within buckets
    all_batches = []
    sorted_bucket_indices = sorted(length_buckets.keys())

    for bucket_idx in sorted_bucket_indices:
        bucket_domain_ids = length_buckets[bucket_idx]
        if shuffle:
            random.shuffle(bucket_domain_ids)

        for i in range(0, len(bucket_domain_ids), batch_size):
            batch_domain_ids = bucket_domain_ids[i: i + batch_size]
            all_batches.append(batch_domain_ids)

    # 5. Shuffle the order of batches for training
    if shuffle:
        random.shuffle(all_batches)

    # 6. Flatten the batches to get the final ordered list of domain IDs for the epoch
    ordered_domain_ids = [did for batch in all_batches for did in batch]

    # 7. Create the Dataset with the final order and all required data dicts
    dataset = EnhancedRMSFDataset(
        ordered_domain_ids, 
        sequences, 
        rmsf_values, 
        temperatures, 
        feature_data, 
        feature_norm_params,
        config
    )

    if len(dataset) == 0:
        logger.error(f"Final dataset for split '{split}' is empty after processing. Cannot create DataLoader.")
        return None

    # 8. Create the DataLoader
    logger.info(f"Creating DataLoader for {len(dataset)} samples for split '{split}' with batch size {batch_size}")
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,  # Shuffling is handled by length batching strategy
        collate_fn=enhanced_collate_fn,  # Use enhanced collate function
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        drop_last=False  # Keep all data
    )

# For backward compatibility
def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Legacy collate function for compatibility.
    
    This redirects to enhanced_collate_fn for consistent behavior.
    """
    return enhanced_collate_fn(batch)

# For backward compatibility
def create_length_batched_dataloader(
    data_dir: str,
    split: str,
    batch_size: int,
    shuffle: bool = True,
    max_length: Optional[int] = None,
    length_bucket_size: int = 50,
    num_workers: int = 0
) -> Optional[DataLoader]:
    """
    Legacy function for backward compatibility.
    
    This redirects to create_enhanced_dataloader with default config.
    """
    return create_enhanced_dataloader(
        data_dir, split, batch_size, shuffle, max_length, length_bucket_size, num_workers, None
    )

# Example Usage (if script is run directly)
if __name__ == "__main__":
    logger.info("Testing Enhanced DataLoader creation...")
    
    # Create dummy data for testing
    dummy_data_dir = "data/processed_dummy_enhanced"
    os.makedirs(dummy_data_dir, exist_ok=True)

    # Sample configuration
    config = {
        'data': {
            'features': {
                'use_position_info': True,
                'use_structure_info': True,
                'use_accessibility': True,
                'use_backbone_angles': True,
                'use_protein_size': True,
                'use_voxel_rmsf': True,
                'use_bfactor': True,
                'normalization_params_file': 'feature_normalization.json'
            }
        },
        'model': {
            'architecture': {
                'use_enhanced_features': True
            }
        }
    }
    
    # Create dummy data
    dummy_domains = [f"D{i:03d}" for i in range(10)]
    dummy_sequences = {}
    dummy_rmsf = {}
    dummy_temps = {}
    dummy_features = {
        'normalized_resid': {},
        'core_exterior_encoded': {},
        'secondary_structure_encoded': {},
        'relative_accessibility': {},
        'phi_norm': {},
        'psi_norm': {},
        'protein_size': {},
        'voxel_rmsf': {}
    }
    
    for i, did in enumerate(dummy_domains):
        length = random.randint(50, 250)
        dummy_sequences[did] = "A" * length
        dummy_rmsf[did] = np.random.rand(length).astype(np.float32) * 2.0
        dummy_temps[did] = random.choice([298.0, 310.0, 320.0, 330.0])
        
        # Per-residue features
        dummy_features['normalized_resid'][did] = np.linspace(0, 1, length).astype(np.float32)
        dummy_features['core_exterior_encoded'][did] = np.random.randint(0, 2, length).astype(np.float32)
        dummy_features['secondary_structure_encoded'][did] = np.random.randint(0, 3, length).astype(np.float32)
        dummy_features['relative_accessibility'][did] = np.random.rand(length).astype(np.float32)
        dummy_features['phi_norm'][did] = np.random.rand(length).astype(np.float32)
        dummy_features['psi_norm'][did] = np.random.rand(length).astype(np.float32)
        dummy_features['voxel_rmsf'][did] = np.random.rand(length).astype(np.float32)
        
        # Global features
        dummy_features['protein_size'][did] = float(length)
    
    # Save dummy data
    with open(os.path.join(dummy_data_dir, "train_domains.txt"), "w") as f: 
        f.write("\n".join(dummy_domains))
    
    with open(os.path.join(dummy_data_dir, "train_sequences.fasta"), "w") as f:
        for did, seq in dummy_sequences.items(): 
            f.write(f">{did}\n{seq}\n")
    
    np.save(os.path.join(dummy_data_dir, "train_rmsf.npy"), dummy_rmsf)
    np.save(os.path.join(dummy_data_dir, "train_temperatures.npy"), dummy_temps)
    
    # Save feature files
    for feature, feature_dict in dummy_features.items():
        np.save(os.path.join(dummy_data_dir, f"train_{feature}.npy"), feature_dict)
    
    # Save feature normalization parameters
    norm_params = {
        'normalized_resid': {'min': 0.0, 'max': 1.0},
        'core_exterior_encoded': {'min': 0.0, 'max': 1.0},
        'secondary_structure_encoded': {'min': 0.0, 'max': 2.0},
        'relative_accessibility': {'min': 0.0, 'max': 1.0},
        'phi_norm': {'min': -1.0, 'max': 1.0},
        'psi_norm': {'min': -1.0, 'max': 1.0},
        'protein_size': {'min': 50.0, 'max': 250.0},
        'voxel_rmsf': {'min': 0.0, 'max': 2.0}
    }
    
    with open(os.path.join(dummy_data_dir, "feature_normalization.json"), "w") as f:
        json.dump(norm_params, f, indent=2)
    
    # Test dataloader creation
    train_loader = create_enhanced_dataloader(
        data_dir=dummy_data_dir,
        split='train',
        batch_size=2,
        shuffle=True,
        max_length=200,
        length_bucket_size=25,
        config=config
    )

    if train_loader:
        logger.info("DataLoader created successfully. Iterating through a few batches...")
        batch_count = 0
        max_batches_to_show = 2
        for i, batch in enumerate(train_loader):
            if i >= max_batches_to_show: break
            logger.info(f"Batch {i+1}:")
            logger.info(f"  Domain IDs: {batch['domain_ids']}")
            logger.info(f"  Num sequences: {len(batch['sequences'])}")
            logger.info(f"  Seq lengths: {[len(s) for s in batch['sequences']]}")
            logger.info(f"  RMSF Tensors: {[t.shape for t in batch['rmsf_values']]}")
            logger.info(f"  Temperatures Tensor: {batch['temperatures']}")
            logger.info(f"  Temperatures Tensor Shape: {batch['temperatures'].shape}")
            
            logger.info("  Features:")
            for feature, data in batch['features'].items():
                if isinstance(data, list):
                    logger.info(f"    {feature}: List of {len(data)} tensors with shapes {[t.shape for t in data]}")
                else:
                    logger.info(f"    {feature}: Tensor with shape {data.shape}")
            
            batch_count += 1
        logger.info(f"Iterated through {batch_count} batches.")
    else:
        logger.error("Failed to create DataLoader.")

    # Clean up dummy data if desired
    # import shutil
    # shutil.rmtree(dummy_data_dir)
    # logger.info(f"Cleaned up dummy data directory: {dummy_data_dir}")
---------------------------------------------------------
===== FILE: ./data/utils/filter_by_temperature.py =====
import pandas as pd
from pathlib import Path

# ==== Configuration ====
INPUT_CSV_PATH = Path("/home/s_felix/FINAL_PROJECT/packages/DeepFlex/data/raw/aggregated_train_dataset.csv")
OUTPUT_DIR = Path("/home/s_felix/FINAL_PROJECT/packages/DeepFlex/data/raw")
TEMPERATURES = [320, 348, 379, 413, 450]
BASENAME_TEMPLATE = "aggregated_train_{}k_dataset.csv"

# ==== Load Dataset ====
df = pd.read_csv(INPUT_CSV_PATH)

# ==== Filter and Save by Temperature ====
for temp in TEMPERATURES:
    filtered = df[df["temperature"] == temp]
    output_path = OUTPUT_DIR / BASENAME_TEMPLATE.format(temp)
    filtered.to_csv(output_path, index=False)
    print(f"Saved {len(filtered)} rows to {output_path}")

---------------------------------------------------------
===== FILE: ./evaluate_predictions.py =====
# evaluate_predictions.py

import os
import sys
import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from sklearn.metrics import mean_squared_error, mean_absolute_error
import json
import logging
import argparse
from collections import defaultdict
import time
from typing import Optional
from tqdm import tqdm

# Setup logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler(sys.stdout)])
logger = logging.getLogger(__name__)

def safe_pearsonr(x, y):
    """Calculates Pearson correlation safely, returning NaN on error or insufficient data."""
    try:
        # Ensure numpy arrays and handle potential all-NaN slices after filtering
        x_np = np.asarray(x).astype(np.float64)
        y_np = np.asarray(y).astype(np.float64)
        valid_mask = ~np.isnan(x_np) & ~np.isnan(y_np)
        x_clean = x_np[valid_mask]
        y_clean = y_np[valid_mask]

        if len(x_clean) < 2:
            return np.nan
        # Check for near-zero variance AFTER cleaning NaNs
        if np.std(x_clean) < 1e-8 or np.std(y_clean) < 1e-8:
            return np.nan

        corr, _ = pearsonr(x_clean, y_clean)
        return corr if not np.isnan(corr) else np.nan
    except (ValueError, FloatingPointError):
        return np.nan
    except Exception as e:
        # Log unexpected errors during correlation calculation
        logger.error(f"Unexpected error during pearsonr calculation: {e}", exc_info=True)
        return np.nan

def evaluate(predictions_csv_path: str, ground_truth_rmsf_npy: str, target_temp: float, output_json_path: Optional[str] = None):
    """
    Evaluates predictions against ground truth for a specific temperature.

    Args:
        predictions_csv_path: Path to the prediction CSV file.
        ground_truth_rmsf_npy: Path to the test_rmsf.npy file.
        target_temp: The specific temperature (float) to evaluate.
        output_json_path: Optional path to save evaluation metrics as JSON.
    """
    logger.info(f"--- Starting Evaluation for Temperature: {target_temp:.1f}K ---")
    logger.info(f"Prediction file: {predictions_csv_path}")
    logger.info(f"Ground truth file: {ground_truth_rmsf_npy}")

    # 1. Load Predictions
    try:
        pred_df = pd.read_csv(predictions_csv_path)
        logger.info(f"Loaded {len(pred_df):,} prediction rows.")
        # The prediction script saves the instance key in the 'domain_id' column
        if 'domain_id' not in pred_df.columns or 'resid' not in pred_df.columns or 'rmsf_pred' not in pred_df.columns:
             raise ValueError("Prediction CSV missing required columns ('domain_id', 'resid', 'rmsf_pred')")
        # Rename for clarity and consistency
        pred_df.rename(columns={'domain_id': 'instance_key'}, inplace=True)
        # Ensure correct types
        pred_df['resid'] = pd.to_numeric(pred_df['resid'], errors='coerce').astype('Int64') # Use Int64 to handle potential NaNs during coerce
        pred_df['rmsf_pred'] = pd.to_numeric(pred_df['rmsf_pred'], errors='coerce')
        pred_df.dropna(subset=['instance_key', 'resid', 'rmsf_pred'], inplace=True)
        if pred_df.empty:
             logger.error("No valid prediction rows after loading and cleaning.")
             return
    except FileNotFoundError:
        logger.error(f"Prediction file not found: {predictions_csv_path}")
        return
    except Exception as e:
        logger.error(f"Error loading prediction file: {e}", exc_info=True)
        return

    # 2. Load Ground Truth
    try:
        gt_rmsf_dict = np.load(ground_truth_rmsf_npy, allow_pickle=True).item()
        logger.info(f"Loaded ground truth RMSF for {len(gt_rmsf_dict):,} instances.")
    except FileNotFoundError:
        logger.error(f"Ground truth RMSF file not found: {ground_truth_rmsf_npy}")
        return
    except Exception as e:
        logger.error(f"Error loading ground truth RMSF file: {e}", exc_info=True)
        return

    # 3. Prepare Ground Truth DataFrame for the Target Temperature
    gt_data = []
    skipped_keys = 0
    processed_keys = 0
    logger.info(f"Filtering ground truth for target temperature {target_temp:.1f}K...")
    for instance_key, rmsf_array in gt_rmsf_dict.items():
        try:
            # Expect key like 'domain_id@temp_float'
            domain_part, temp_part = instance_key.rsplit('@', 1)
            temp_val = float(temp_part)
        except (ValueError, AttributeError):
            logger.warning(f"Skipping malformed instance key in ground truth: {instance_key}")
            skipped_keys += 1
            continue

        # Check if the temperature matches the target
        if abs(temp_val - target_temp) < 1e-6: # Robust float comparison
            processed_keys += 1
            if isinstance(rmsf_array, np.ndarray):
                for i, rmsf_val in enumerate(rmsf_array):
                    if not np.isnan(rmsf_val): # Check for NaN in ground truth RMSF
                        gt_data.append({
                            'instance_key': instance_key,
                            'resid': i + 1, # Generate 1-based residue index
                            'target_rmsf': float(rmsf_val) # Ensure float
                        })
            # else: logger.warning(f"RMSF data for {instance_key} is not a numpy array.")

    if not gt_data:
        logger.error(f"No ground truth data found for target temperature {target_temp:.1f}K. Cannot evaluate.")
        if skipped_keys > 0: logger.warning(f"Skipped {skipped_keys} potentially malformed keys during filtering.")
        logger.info(f"Processed {processed_keys} keys matching temperature {target_temp:.1f}K.")

        return

    gt_df = pd.DataFrame(gt_data)
    # Ensure correct types
    gt_df['resid'] = gt_df['resid'].astype('Int64')
    gt_df['target_rmsf'] = pd.to_numeric(gt_df['target_rmsf'], errors='coerce')
    gt_df.dropna(inplace=True)
    logger.info(f"Created ground truth DataFrame with {len(gt_df):,} rows for temperature {target_temp:.1f}K.")

    # 4. Merge Predictions and Ground Truth
    logger.info("Merging predictions and ground truth...")
    try:
        # Ensure resid types match before merge (already done above)
        eval_df = pd.merge(pred_df[['instance_key', 'resid', 'rmsf_pred']],
                           gt_df[['instance_key', 'resid', 'target_rmsf']],
                           on=['instance_key', 'resid'],
                           how='inner') # Inner join ensures we only evaluate matching residues
        logger.info(f"Merged DataFrame contains {len(eval_df):,} aligned residue predictions.")

        if eval_df.empty:
             logger.error("Merging predictions and ground truth resulted in an empty DataFrame. Check instance keys and residue IDs.")
             return

    except Exception as e:
         logger.error(f"Error merging DataFrames: {e}", exc_info=True)
         return

    # 5. Calculate Overall Metrics
    logger.info("Calculating overall metrics...")
    results = {'evaluation_temperature': target_temp, 'overall': {}, 'per_instance_summary': {}, 'per_instance_detail': {}}
    try:
        if len(eval_df) < 2:
            logger.warning("Fewer than 2 aligned data points. Cannot calculate overall correlation.")
            results['overall']['pearson_correlation'] = None
        else:
            overall_corr = safe_pearsonr(eval_df['target_rmsf'], eval_df['rmsf_pred'])
            results['overall']['pearson_correlation'] = float(overall_corr) if not np.isnan(overall_corr) else None

        overall_mse = mean_squared_error(eval_df['target_rmsf'], eval_df['rmsf_pred'])
        overall_rmse = np.sqrt(overall_mse)
        overall_mae = mean_absolute_error(eval_df['target_rmsf'], eval_df['rmsf_pred'])

        results['overall']['rmse'] = float(overall_rmse)
        results['overall']['mae'] = float(overall_mae)
        results['overall']['mse'] = float(overall_mse)
        results['overall']['num_residues'] = len(eval_df)

        logger.info(f"--- Overall Metrics (Temp: {target_temp:.1f}K) ---")
        logger.info(f"  Pearson Correlation: {results['overall']['pearson_correlation']:.4f}" if results['overall']['pearson_correlation'] is not None else "  Pearson Correlation: N/A (<2 points)")
        logger.info(f"  RMSE: {overall_rmse:.4f}")
        logger.info(f"  MAE: {overall_mae:.4f}")
        logger.info(f"  MSE: {overall_mse:.4f}")
        logger.info(f"  Evaluated on: {len(eval_df):,} residues")

    except Exception as e:
        logger.error(f"Error calculating overall metrics: {e}", exc_info=True)

    # 6. Calculate Per-Instance Metrics
    logger.info("Calculating per-instance metrics...")
    per_instance_metrics_list = []
    instance_groups = eval_df.groupby('instance_key')
    num_instances_eval = 0

    for instance_key, group in tqdm(instance_groups, desc="Per-Instance Metrics", total=len(instance_groups)):
        num_residues = len(group)
        if num_residues < 2: # Need at least 2 points for correlation
            continue

        num_instances_eval += 1
        try:
            corr = safe_pearsonr(group['target_rmsf'], group['rmsf_pred'])
            rmse = np.sqrt(mean_squared_error(group['target_rmsf'], group['rmsf_pred']))
            mae = mean_absolute_error(group['target_rmsf'], group['rmsf_pred'])

            instance_metrics = {
                'correlation': float(corr) if not np.isnan(corr) else None,
                'rmse': float(rmse),
                'mae': float(mae),
                'num_residues': num_residues
            }
            per_instance_metrics_list.append(instance_metrics)
            results['per_instance_detail'][instance_key] = instance_metrics

        except Exception as e:
            logger.warning(f"Error calculating metrics for instance {instance_key}: {e}")

    # Summarize Per-Instance Metrics
    if per_instance_metrics_list:
        per_instance_df = pd.DataFrame(per_instance_metrics_list)
        # Calculate summary stats, ignoring NaNs for correlation
        summary = {
            'mean_correlation': float(per_instance_df['correlation'].mean(skipna=True)),
            'median_correlation': float(per_instance_df['correlation'].median(skipna=True)),
            'std_dev_correlation': float(per_instance_df['correlation'].std(skipna=True)),
            'mean_rmse': float(per_instance_df['rmse'].mean()),
            'median_rmse': float(per_instance_df['rmse'].median()),
            'mean_mae': float(per_instance_df['mae'].mean()),
            'median_mae': float(per_instance_df['mae'].median()),
            'num_instances': num_instances_eval
        }
        results['per_instance_summary'] = summary

        logger.info("\n--- Per-Protein Instance Metrics Summary ---")
        logger.info(f"  Instances Evaluated: {num_instances_eval}")
        logger.info(f"  Mean Correlation: {summary['mean_correlation']:.4f} +/- {summary['std_dev_correlation']:.4f}")
        logger.info(f"  Median Correlation: {summary['median_correlation']:.4f}")
        logger.info(f"  Mean RMSE: {summary['mean_rmse']:.4f}")
        logger.info(f"  Median RMSE: {summary['median_rmse']:.4f}")
        logger.info(f"  Mean MAE: {summary['mean_mae']:.4f}")
        logger.info(f"  Median MAE: {summary['median_mae']:.4f}")
    else:
        logger.warning("No instances had enough data points (>=2) for per-instance metric calculation.")
        results['per_instance_summary'] = {'num_instances': 0}


    # 7. Save Results to JSON if path provided
    if output_json_path:
        logger.info(f"Saving evaluation results to: {output_json_path}")
        try:
            os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
            # Use a custom encoder for potential numpy types if any sneak through
            class NpEncoder(json.JSONEncoder):
                def default(self, obj):
                    if isinstance(obj, np.integer): return int(obj)
                    if isinstance(obj, np.floating): return float(obj)
                    if isinstance(obj, np.ndarray): return obj.tolist()
                    return super(NpEncoder, self).default(obj)
            with open(output_json_path, 'w') as f:
                json.dump(results, f, indent=4, cls=NpEncoder)
            logger.info("Results saved successfully.")
        except Exception as e:
            logger.error(f"Failed to save results JSON: {e}", exc_info=True)

    logger.info(f"--- Evaluation Complete for Temperature: {target_temp:.1f}K ---")
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Evaluate RMSF predictions against ground truth.')
    parser.add_argument('--predictions_csv', type=str, required=True,
                        help='Path to the prediction CSV file generated by predict.py (e.g., predictions/test_set_results/320K/predictions_320K.csv)')
    parser.add_argument('--ground_truth_npy', type=str, required=True,
                        help='Path to the ground truth test set RMSF .npy file (e.g., data/processed/test_rmsf.npy)')
    parser.add_argument('--temperature', type=float, required=True,
                        help='The specific temperature (in Kelvin) for which the predictions were made and should be evaluated.')
    parser.add_argument('--output_json', type=str, default=None,
                        help='Optional: Path to save the evaluation metrics as a JSON file.')

    args = parser.parse_args()

    # Basic validation
    if not os.path.exists(args.predictions_csv):
        logger.error(f"Prediction file not found: {args.predictions_csv}")
        sys.exit(1)
    if not os.path.exists(args.ground_truth_npy):
        logger.error(f"Ground truth file not found: {args.ground_truth_npy}")
        sys.exit(1)

    evaluate(
        predictions_csv_path=args.predictions_csv,
        ground_truth_rmsf_npy=args.ground_truth_npy,
        target_temp=args.temperature,
        output_json_path=args.output_json
    )
---------------------------------------------------------
===== FILE: ./main.py =====
#!/usr/bin/env python3
import argparse
import os
import sys
import logging
import json
from typing import Dict
import yaml

# Set project root directory relative to this script file
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
# Add project root to Python path to allow importing modules
if PROJECT_ROOT not in sys.path:
     sys.path.insert(0, PROJECT_ROOT)

# Now import project modules
try:
    # Ensure imports happen after path modification
    from data_processor import process_data
    from train import train
    from predict import predict
except ImportError as e:
     # Provide more context on import error
     print(f"Error: Failed to import project modules.")
     print(f"PROJECT_ROOT={PROJECT_ROOT}")
     print(f"sys.path={sys.path}")
     print(f"Error details: {e}")
     sys.exit(1)


# Setup basic logging for the main script orchestrator
# Use a more detailed format for the main orchestrator
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s [%(module)s:%(funcName)s:%(lineno)d] - %(message)s',
                    handlers=[logging.StreamHandler(sys.stdout)]) # Log to stdout by default
logger = logging.getLogger(__name__) # Get logger for this module


def load_config(config_path: str) -> Dict:
    """Loads YAML configuration file."""
    logger.info(f"Loading configuration from: {config_path}")
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        logger.info("Configuration loaded successfully.")
        logger.debug(f"Config content: {json.dumps(config, indent=2)}")
        return config
    except FileNotFoundError:
        logger.error(f"Configuration file not found: {config_path}")
        sys.exit(1)
    except yaml.YAMLError as e:
        logger.error(f"Error parsing configuration file {config_path}: {e}")
        sys.exit(1)
    except Exception as e:
         logger.error(f"An unexpected error occurred loading config {config_path}: {e}", exc_info=True)
         sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description='Enhanced ESM-Flex: Temperature-Aware Protein Flexibility (RMSF) Prediction Pipeline.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    subparsers = parser.add_subparsers(
        dest='command',
        help='Select the command: process, train, or predict.',
        required=True
    )

    # === Process data command ===
    process_parser = subparsers.add_parser(
        'process',
        help='Process enhanced RMSF/Temperature/Features CSV data into standardized splits.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    process_parser.add_argument('--config', type=str, default='config.yaml', help='Path to the main YAML configuration file.')
    # Allow overriding config file paths via CLI for flexibility
    process_parser.add_argument('--csv', type=str, default=None, help='Override path to the input enriched CSV file.')
    process_parser.add_argument('--output', type=str, default=None, help='Override output directory for processed data.')
    process_parser.add_argument('--train_ratio', type=float, default=None, help='Override fraction for training set topology split.')
    process_parser.add_argument('--val_ratio', type=float, default=None, help='Override fraction for validation set topology split.')
    process_parser.add_argument('--seed', type=int, default=None, help='Override random seed for splitting.')

    # === Train command ===
    train_parser = subparsers.add_parser(
        'train',
        help='Train the Enhanced Temperature-Aware ESM Regression model.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    train_parser.add_argument('--config', type=str, default='config.yaml', help='Path to the YAML configuration file.')

    # === Predict command ===
    predict_parser = subparsers.add_parser(
        'predict',
        help='Predict RMSF using a trained model, optionally with per-instance temperatures and uncertainty.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    predict_parser.add_argument('--model_checkpoint', type=str, required=True, help='Path to the trained model checkpoint (.pt file).')
    predict_parser.add_argument('--fasta_path', type=str, required=True, help='Path to the input FASTA file (headers MUST be instance_keys like domain@temp.1f if using --temperature_npy).')
    predict_parser.add_argument('--output_dir', type=str, default='predictions', help='Base directory to save prediction results.')
    # Temperature arguments - one must be provided
    predict_parser.add_argument('--temperature', type=float, default=None, help='(Optional) Target temperature (Kelvin) to use for ALL sequences if --temperature_npy is not provided.')
    predict_parser.add_argument('--temperature_npy', type=str, default=None, help='(Optional) Path to .npy file mapping instance_keys (from FASTA) to RAW temperatures. Overrides --temperature.')
    # Uncertainty argument
    predict_parser.add_argument('--mc_samples', type=int, default=0, help='Number of Monte Carlo Dropout samples for uncertainty estimation (e.g., 10-50). Default 0 disables MC Dropout.')
    # Other args
    predict_parser.add_argument('--batch_size', type=int, default=8, help='Batch size for prediction.')
    predict_parser.add_argument('--max_length', type=int, default=None, help='Optional: Max sequence length filter.')
    predict_parser.add_argument('--plot_predictions', action=argparse.BooleanOptionalAction, default=True, help='Generate plots.')
    predict_parser.add_argument('--smoothing_window', type=int, default=1, help='Smoothing window for plots (1=none).')
    # Config file (useful for defaults like batch size if not specified)
    predict_parser.add_argument('--config', type=str, default='config.yaml', help='Path to the main YAML configuration file (used for defaults).')

    # Parse arguments
    args = parser.parse_args()
    logger.info(f"Executing command: {args.command}")
    # === Predict command ===
    # predict_parser = subparsers.add_parser(
    #     'predict',
    #     help='Predict RMSF for sequences at a specific temperature using a trained model.',
    #     formatter_class=argparse.ArgumentDefaultsHelpFormatter
    # )
    # # Prediction doesn't use the main config file directly, takes specific inputs
    # predict_parser.add_argument('--model_checkpoint', type=str, required=True, help='Path to the trained model checkpoint (.pt file).')
    # predict_parser.add_argument('--fasta_path', type=str, required=True, help='Path to the input FASTA file.')
    # predict_parser.add_argument('--temperature', type=float, required=True, help='Target temperature (in Kelvin) for prediction.')
    # predict_parser.add_argument('--output_dir', type=str, default='predictions', help='Base directory to save prediction results.')
    # predict_parser.add_argument('--batch_size', type=int, default=8, help='Batch size for prediction.')
    # predict_parser.add_argument('--max_length', type=int, default=None, help='Optional: Max sequence length filter.')
    # predict_parser.add_argument('--plot_predictions', action=argparse.BooleanOptionalAction, default=True, help='Generate plots.')
    # predict_parser.add_argument('--smoothing_window', type=int, default=1, help='Smoothing window for plots (1=none).')

    # # Parse arguments
    # args = parser.parse_args()
    # logger.info(f"Executing command: {args.command}")

    # === Execute Command ===
    if args.command == 'process':
        logger.info(f"Loading config for 'process' command from: {args.config}")
        config = load_config(args.config)

        # Override config values if provided via CLI
        csv_path = args.csv if args.csv is not None else config.get('data', {}).get('raw_csv_path')
        output_dir = args.output if args.output is not None else config.get('data', {}).get('data_dir', 'data/processed')
        train_ratio = args.train_ratio if args.train_ratio is not None else config.get('training', {}).get('train_ratio', 0.7)
        val_ratio = args.val_ratio if args.val_ratio is not None else config.get('training', {}).get('val_ratio', 0.15)
        seed = args.seed if args.seed is not None else config.get('training', {}).get('seed', 42)
        scaling_file = config.get('data', {}).get('temp_scaling_filename', 'temp_scaling_params.json')

        if not csv_path:
             logger.error("Raw CSV data path ('data.raw_csv_path') not found in config or provided via --csv.")
             sys.exit(1)

        logger.info(f"Starting data processing...")
        process_data(
            csv_path=csv_path,
            output_dir=output_dir,
            temp_scaling_filename=scaling_file,
            config=config, # Pass the entire config to use feature settings
            train_ratio=train_ratio,
            val_ratio=val_ratio,
            seed=seed
        )
        logger.info("Data processing finished.")

    elif args.command == 'train':
        logger.info(f"Loading config for 'train' command from: {args.config}")
        config = load_config(args.config)
        logger.info("Starting model training...")
        try:
            train(config) # Pass the loaded config dictionary
            logger.info("Training finished.")
        except Exception as e:
             # Catch potential errors during the train function execution
             logger.error(f"An unexpected error occurred during training execution: {e}", exc_info=True)
             sys.exit(1)
    
    elif args.command == 'predict':
        logger.info(f"Starting prediction...")

        # Load config file to get defaults if needed
        predict_defaults = {}
        if args.config and os.path.exists(args.config):
            try:
                full_config = load_config(args.config)
                predict_defaults = full_config.get('prediction', {})
            except Exception as e:
                logger.warning(f"Could not load config {args.config} for prediction defaults: {e}")

        # Prepare the configuration dictionary, prioritizing CLI args over config file defaults
        predict_config = {
            'model_checkpoint': args.model_checkpoint,
            'fasta_path': args.fasta_path,
            'temperature': args.temperature, # Will be None if npy is used
            'temperature_npy': args.temperature_npy, # Add the new arg
            'mc_samples': args.mc_samples, # Add the new arg
            'output_dir': args.output_dir,
            'batch_size': args.batch_size if args.batch_size is not None else predict_defaults.get('batch_size', 8),
            'max_length': args.max_length if args.max_length is not None else predict_defaults.get('max_length'),
            'plot_predictions': args.plot_predictions if args.plot_predictions is not None else predict_defaults.get('plot_predictions', True),
            'smoothing_window': args.smoothing_window if args.smoothing_window is not None else predict_defaults.get('smoothing_window', 1)
        }

        # --- Add Validation for Temperature Args ---
        if predict_config['temperature'] is None and predict_config['temperature_npy'] is None:
            logger.critical("Prediction requires either --temperature or --temperature_npy to be specified.")
            sys.exit(1)
        if predict_config['temperature'] is not None and predict_config['temperature_npy'] is not None:
            logger.warning("Both --temperature and --temperature_npy provided. Prioritizing --temperature_npy.")
            predict_config['temperature'] = None # Nullify the single temp if npy is given
        if predict_config['temperature_npy'] and not os.path.exists(predict_config['temperature_npy']):
             logger.critical(f"Specified temperature file not found: {predict_config['temperature_npy']}")
             sys.exit(1)
        # --- End Validation ---

        try:
            # Make sure predict function is imported from the updated predict.py
            from predict import predict # Re-import in case main was loaded first
            predict(predict_config) # Pass the constructed config dict
            logger.info("Prediction finished.")
        except Exception as e:
             logger.error(f"An unexpected error occurred during prediction: {e}", exc_info=True)
             sys.exit(1)

    # elif args.command == 'predict':
    #     logger.info(f"Starting prediction...")
    #     # Prepare the configuration dictionary directly from args for the predict function
    #     predict_config = {
    #         'model_checkpoint': args.model_checkpoint,
    #         'fasta_path': args.fasta_path,
    #         'temperature': args.temperature,
    #         'output_dir': args.output_dir,
    #         'batch_size': args.batch_size,
    #         'max_length': args.max_length,
    #         'plot_predictions': args.plot_predictions,
    #         'smoothing_window': args.smoothing_window
    #     }
    #     try:
    #         predict(predict_config)
    #         logger.info("Prediction finished.")
    #     except Exception as e:
    #          logger.error(f"An unexpected error occurred during prediction: {e}", exc_info=True)
    #          sys.exit(1)

    else:
        # Should be unreachable due to 'required=True'
        logger.error(f"Unknown command: {args.command}")
        parser.print_help()
        sys.exit(1)

if __name__ == "__main__":
    main()
---------------------------------------------------------
===== FILE: ./merge_predictions.py =====
#!/usr/bin/env python3
import pandas as pd
import numpy as np
import os
import logging
from typing import Optional

# --- Configuration (Define Paths and Columns Here) ---

# Input Files
# ORIGINAL_CSV_PATH = "/home/s_felix/ESM-Flex-2/data/raw/aggregated_holdout_dataset.csv"
ORIGINAL_CSV_PATH = "/home/s_felix/drDataScience/data/analysis_complete_holdout_dataset.csv"
PREDICTIONS_CSV_PATH = "/home/s_felix/ESM-Flex-2/predictions/holdout_set_results_latest/prediction_from_npy_train_temperatures_mc10/predictions_prediction_from_npy_train_temperatures_mc10.csv"

# Output File
OUTPUT_CSV_PATH = "/home/s_felix/drDataScience/data/final_analysis_dataset.csv"

# Column Names
# In Original CSV (input)
ORIG_DOMAIN_ID_COL = 'domain_id'
ORIG_TEMP_COL = 'temperature' # Adjust if it's 'temperature_feature' in your raw file
ORIG_RESID_COL = 'resid'
# In Prediction CSV (input)
PRED_INSTANCE_KEY_COL = 'instance_key'
PRED_RESID_COL = 'resid' # This is the 1-based relative index
PRED_RMSF_COL = 'rmsf_pred'
PRED_UNCERTAINTY_COL = 'uncertainty'
# In Merged CSV (output)
NEW_RMSF_COL = 'Attention_ESM_rmsf'
NEW_UNCERTAINTY_COL = 'Attention_ESM_uncertainty'

# Internal temporary column name for relative index
RELATIVE_RESID_IDX_COL = '_relative_resid_idx'
TEMP_INSTANCE_KEY_COL = '_instance_key' # Temporary key column in original df

# Separator (Should match data_processor.py)
INSTANCE_KEY_SEPARATOR = "@"

# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Helper Function ---
def create_instance_key(domain_id: str, temperature: float) -> Optional[str]:
    """Creates a unique key combining domain ID and temperature."""
    try:
        # Ensure temperature is float before formatting
        temp_float = float(temperature)
        # Use consistent formatting (e.g., one decimal place)
        return f"{str(domain_id)}{INSTANCE_KEY_SEPARATOR}{temp_float:.1f}"
    except (ValueError, TypeError):
        logger.error(f"Could not format instance key for domain '{domain_id}', temp '{temperature}'")
        return None # Return None on error

# --- Main Merge Logic ---
def merge_data():
    """
    Loads original data and predictions, merges them based on instance key
    and relative residue index, and saves the combined dataset.
    """
    # --- Load Original Data ---
    logger.info(f"Loading original data from: {ORIGINAL_CSV_PATH}")
    if not os.path.exists(ORIGINAL_CSV_PATH):
        logger.error(f"Original data file not found: {ORIGINAL_CSV_PATH}")
        return
    try:
        df_orig = pd.read_csv(ORIGINAL_CSV_PATH)
        logger.info(f"Loaded {len(df_orig)} rows from original data.")
    except Exception as e:
        logger.error(f"Error loading original data: {e}", exc_info=True)
        return

    # --- Load Prediction Data ---
    logger.info(f"Loading prediction data from: {PREDICTIONS_CSV_PATH}")
    if not os.path.exists(PREDICTIONS_CSV_PATH):
        logger.error(f"Predictions data file not found: {PREDICTIONS_CSV_PATH}")
        return
    try:
        df_pred = pd.read_csv(PREDICTIONS_CSV_PATH)
        logger.info(f"Loaded {len(df_pred)} rows from predictions.")
    except Exception as e:
        logger.error(f"Error loading predictions data: {e}", exc_info=True)
        return

    # --- Prepare Original DataFrame ---
    logger.info("Preparing original DataFrame...")
    try:
        # 1. Check and rename temperature column
        if ORIG_TEMP_COL not in df_orig.columns:
            logger.error(f"Original CSV missing specified temperature column: '{ORIG_TEMP_COL}'")
            return
        # No need to rename internally if ORIG_TEMP_COL is correct

        # 2. Ensure essential columns exist and clean types/NaNs
        essential_orig_cols = [ORIG_DOMAIN_ID_COL, ORIG_TEMP_COL, ORIG_RESID_COL]
        if not all(col in df_orig.columns for col in essential_orig_cols):
            missing = [c for c in essential_orig_cols if c not in df_orig.columns]
            logger.error(f"Original CSV missing essential columns for merging: {missing}")
            return

        df_orig[ORIG_TEMP_COL] = pd.to_numeric(df_orig[ORIG_TEMP_COL], errors='coerce')
        df_orig[ORIG_RESID_COL] = pd.to_numeric(df_orig[ORIG_RESID_COL], errors='coerce')
        df_orig.dropna(subset=[ORIG_DOMAIN_ID_COL, ORIG_TEMP_COL, ORIG_RESID_COL], inplace=True)
        df_orig[ORIG_RESID_COL] = df_orig[ORIG_RESID_COL].astype(int)
        df_orig[ORIG_DOMAIN_ID_COL] = df_orig[ORIG_DOMAIN_ID_COL].astype(str)
        logger.info(f"{len(df_orig)} original rows remaining after cleaning essential columns.")
        if df_orig.empty:
             logger.error("No valid rows left in original data after cleaning.")
             return

        # 3. Create temporary 'instance_key' column
        df_orig[TEMP_INSTANCE_KEY_COL] = df_orig.apply(
            lambda row: create_instance_key(row[ORIG_DOMAIN_ID_COL], row[ORIG_TEMP_COL]),
            axis=1
        )
        orig_len = len(df_orig)
        df_orig.dropna(subset=[TEMP_INSTANCE_KEY_COL], inplace=True) # Drop rows where key creation failed
        if len(df_orig) < orig_len:
            logger.warning(f"Dropped {orig_len - len(df_orig)} original rows due to invalid instance key creation (check domain IDs/temps).")

        # 4. Create relative 1-based residue index within each instance group
        logger.info("Calculating relative residue index...")
        # Sort first to ensure correct ordering for rank/cumcount
        df_orig = df_orig.sort_values(by=[TEMP_INSTANCE_KEY_COL, ORIG_RESID_COL])
        # Calculate 0-based count within group, add 1 for 1-based index
        df_orig[RELATIVE_RESID_IDX_COL] = df_orig.groupby(TEMP_INSTANCE_KEY_COL).cumcount() + 1
        logger.info("Relative residue index calculated.")

    except Exception as e:
        logger.error(f"Error preparing original DataFrame: {e}", exc_info=True)
        return

    # --- Prepare Prediction DataFrame ---
    logger.info("Preparing prediction DataFrame...")
    try:
        # 1. Check required columns
        required_pred_cols = [PRED_INSTANCE_KEY_COL, PRED_RESID_COL, PRED_RMSF_COL, PRED_UNCERTAINTY_COL]
        if not all(col in df_pred.columns for col in required_pred_cols):
            missing = [c for c in required_pred_cols if c not in df_pred.columns]
            logger.error(f"Prediction CSV missing required columns: {missing}")
            return

        # 2. Rename columns for merging and final output
        df_pred = df_pred.rename(columns={
            # Keep PRED_INSTANCE_KEY_COL for merging
            PRED_RESID_COL: RELATIVE_RESID_IDX_COL, # Match the relative index column
            PRED_RMSF_COL: NEW_RMSF_COL,
            PRED_UNCERTAINTY_COL: NEW_UNCERTAINTY_COL
        })

        # 3. Ensure correct types for merge keys and data
        df_pred[PRED_INSTANCE_KEY_COL] = df_pred[PRED_INSTANCE_KEY_COL].astype(str)
        df_pred[RELATIVE_RESID_IDX_COL] = pd.to_numeric(df_pred[RELATIVE_RESID_IDX_COL], errors='coerce')
        df_pred.dropna(subset=[RELATIVE_RESID_IDX_COL], inplace=True) # Drop if resid conversion failed
        df_pred[RELATIVE_RESID_IDX_COL] = df_pred[RELATIVE_RESID_IDX_COL].astype(int)
        df_pred[NEW_RMSF_COL] = pd.to_numeric(df_pred[NEW_RMSF_COL], errors='coerce')
        df_pred[NEW_UNCERTAINTY_COL] = pd.to_numeric(df_pred[NEW_UNCERTAINTY_COL], errors='coerce')

        # Select only necessary columns for merge to avoid duplicate columns like 'temperature'
        df_pred_to_merge = df_pred[[PRED_INSTANCE_KEY_COL, RELATIVE_RESID_IDX_COL, NEW_RMSF_COL, NEW_UNCERTAINTY_COL]]

    except Exception as e:
        logger.error(f"Error preparing prediction DataFrame: {e}", exc_info=True)
        return

    # --- Perform Merge ---
    logger.info("Merging original data with predictions...")
    try:
        # Use left merge to keep all rows from the original dataframe
        df_merged = pd.merge(
            df_orig,
            df_pred_to_merge,
            left_on=[TEMP_INSTANCE_KEY_COL, RELATIVE_RESID_IDX_COL], # Keys from original df
            right_on=[PRED_INSTANCE_KEY_COL, RELATIVE_RESID_IDX_COL], # Keys from prediction df
            how='left',
            suffixes=('', '_pred') # Add suffix to prediction key if it overlaps, though we selected columns
        )
        logger.info(f"Merge completed. Resulting DataFrame has {len(df_merged)} rows.")

        # Verify merge quality (optional but recommended)
        num_matched = df_merged[NEW_RMSF_COL].notna().sum()
        num_unmatched_orig = len(df_orig) - num_matched # Rows in orig without a direct pred match
        num_unmatched_pred = len(df_pred) - num_matched # Rows in pred without a direct orig match (should be less common with left merge)
        logger.info(f"Successfully merged predictions for {num_matched} rows.")
        if num_unmatched_orig > 0:
            logger.warning(f"{num_unmatched_orig} rows from the original dataset did not have a corresponding prediction entry (NaNs added).")
        if num_unmatched_pred > 0:
            logger.warning(f"{num_unmatched_pred} rows from the prediction dataset did not match any entry in the original dataset (check keys/indices).")


        # Clean up temporary columns used for merging
        df_merged = df_merged.drop(columns=[TEMP_INSTANCE_KEY_COL, RELATIVE_RESID_IDX_COL])
        # Drop potentially duplicated instance key from predictions if suffix was added (unlikely with column selection)
        if f"{PRED_INSTANCE_KEY_COL}_pred" in df_merged.columns:
             df_merged = df_merged.drop(columns=[f"{PRED_INSTANCE_KEY_COL}_pred"])


    except Exception as e:
        logger.error(f"Error during merge operation: {e}", exc_info=True)
        return

    # --- Save Merged Data ---
    logger.info(f"Saving merged data to: {OUTPUT_CSV_PATH}")
    try:
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(OUTPUT_CSV_PATH), exist_ok=True)
        # Save with specific float formatting
        df_merged.to_csv(OUTPUT_CSV_PATH, index=False, float_format='%.6f')
        logger.info("Merged data saved successfully.")
    except Exception as e:
        logger.error(f"Error saving merged data: {e}", exc_info=True)
        return

# --- Run the Merge ---
if __name__ == "__main__":
    logger.info("--- Starting Prediction Merge Script ---")
    merge_data()
    logger.info("--- Prediction Merge Script Finished ---")
---------------------------------------------------------
===== FILE: ./model.py =====
import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
import numpy as np
import math
from typing import List, Dict, Tuple, Optional, Any

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    from esm.models.esmc import ESMC
    from esm.sdk.api import LogitsConfig, ESMProtein
except ImportError:
    logger.error("Failed to import from 'esm' library. Please install `pip install esm`.", exc_info=True)
    raise

class PositionalEncoding(nn.Module):
    """
    Positional encoding for sequences.
    Based on the sine/cosine encoding from "Attention Is All You Need".
    """
    def __init__(self, d_model: int, max_len: int = 2000, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Create positional encoding matrix
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # Register buffer (not a parameter, but part of the module's state)
        self.register_buffer('pe', pe)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Add positional encoding to input tensor.
        
        Args:
            x: Tensor of shape [seq_len, batch_size, d_model] or [seq_len, d_model]
            
        Returns:
            Tensor with positional encoding added
        """
        if x.dim() == 3:  # [seq_len, batch_size, d_model]
            seq_len = x.size(0)
            x = x + self.pe[:seq_len, :]
        elif x.dim() == 2:  # [seq_len, d_model]
            seq_len = x.size(0)
            x = x + self.pe[:seq_len, :]
        else:
            raise ValueError(f"Expected 2D or 3D tensor, got {x.dim()}D")
            
        return self.dropout(x)

class TemperatureEncoding(nn.Module):
    """
    Advanced temperature encoding module that creates a richer
    representation of temperature for integration with protein features.
    """
    def __init__(self, embedding_dim: int = 16, dropout: float = 0.1):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.dropout = nn.Dropout(p=dropout)
        
        # Project scalar temperature to a higher dimensional space
        self.temp_projection = nn.Sequential(
            nn.Linear(1, embedding_dim),
            nn.GELU(),
            nn.LayerNorm(embedding_dim)
        )
        
    def forward(self, temperature: torch.Tensor) -> torch.Tensor:
        """
        Convert scalar temperature values to rich embeddings.
        
        Args:
            temperature: Tensor of shape [batch_size] containing scaled temperature values
            
        Returns:
            Tensor of shape [batch_size, embedding_dim]
        """
        # Reshape to [batch_size, 1]
        temp = temperature.unsqueeze(1)
        
        # Project to higher dimension
        temp_embedding = self.temp_projection(temp)
        
        return self.dropout(temp_embedding)

class FeatureProcessor(nn.Module):
    """
    Process structural features before combining with ESM embeddings.
    """
    def __init__(self, feature_dims: Dict[str, int], output_dim: int, dropout: float = 0.1):
        super().__init__()
        self.feature_dims = feature_dims
        self.output_dim = output_dim
        
        # Create linear projections for each feature
        self.projections = nn.ModuleDict()
        
        # Process each feature type
        for feature_name, feature_dim in feature_dims.items():
            # Create projection for this feature
            self.projections[feature_name] = nn.Sequential(
                nn.Linear(feature_dim, output_dim),
                nn.GELU(),
                nn.LayerNorm(output_dim),
                nn.Dropout(dropout)
            )
        
        # Final feature fusion layer
        self.feature_fusion = nn.Sequential(
            nn.Linear(output_dim * len(feature_dims), output_dim),
            nn.GELU(),
            nn.LayerNorm(output_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Process and combine structural features.
        
        Args:
            features: Dictionary of feature tensors or lists of tensors
            
        Returns:
            Tensor of processed features [batch_size, seq_len, output_dim]
        """
        # Process each feature and collect
        processed_features = []
        
        for feature_name, projection in self.projections.items():
            if feature_name in features:
                feature_data = features[feature_name]
                
                # Process this feature
                processed = projection(feature_data)
                processed_features.append(processed)
            else:
                logger.warning(f"Feature {feature_name} not found in input features")
        
        # Combine all features
        if not processed_features:
            raise ValueError("No valid features were processed")
            
        # Concatenate feature tensors along the feature dimension
        combined = torch.cat(processed_features, dim=-1)
        
        # Final fusion
        return self.feature_fusion(combined)

class AttentionWithTemperature(nn.Module):
    """
    Multi-head self-attention module with temperature conditioning.
    """
    def __init__(self, embed_dim: int, num_heads: int = 8, dropout: float = 0.1, temp_dim: int = 16):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.temp_dim = temp_dim
        
        # Temperature conditioning for attention
        self.temp_to_qkv = nn.Linear(temp_dim, 3 * embed_dim)
        
        # Multi-head attention
        self.self_attn = nn.MultiheadAttention(
            embed_dim, 
            num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Output projection
        self.out_proj = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.Dropout(dropout)
        )
        
        # Layer norm
        self.norm = nn.LayerNorm(embed_dim)
        
    def forward(self, 
                x: torch.Tensor, 
                temp_embedding: torch.Tensor,
                key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Apply temperature-conditioned self-attention.
        
        Args:
            x: Input tensor [batch_size, seq_len, embed_dim]
            temp_embedding: Temperature embedding [batch_size, temp_dim]
            key_padding_mask: Optional mask for padding positions
            
        Returns:
            Attended tensor [batch_size, seq_len, embed_dim]
        """
        # Apply layer norm first (pre-norm formulation)
        residual = x
        x = self.norm(x)
        
        # Temperature conditioning
        batch_size = x.size(0)
        seq_len = x.size(1)
        
        # Generate QKV biases from temperature
        temp_qkv_bias = self.temp_to_qkv(temp_embedding)  # [batch_size, 3*embed_dim]
        temp_q, temp_k, temp_v = torch.chunk(temp_qkv_bias, 3, dim=-1)
        
        # Apply temperature biases to the input before attention
        # This effectively conditions attention on temperature
        q_bias = temp_q.unsqueeze(1).expand(-1, seq_len, -1)
        k_bias = temp_k.unsqueeze(1).expand(-1, seq_len, -1)
        v_bias = temp_v.unsqueeze(1).expand(-1, seq_len, -1)
        
        q = x + q_bias
        k = x + k_bias
        v = x + v_bias
        
        # Multi-head attention
        attn_output, _ = self.self_attn(q, k, v, key_padding_mask=key_padding_mask)
        
        # Output projection
        output = self.out_proj(attn_output)
        
        # Residual connection
        return output + residual

class EnhancedTemperatureAwareESMModel(nn.Module):
    """
    Enhanced ESM-C based model for Temperature-Aware RMSF prediction.

    Uses ESM-C embeddings with structural features, attention mechanism,
    and improved temperature integration.
    """
    def __init__(self,
                 esm_model_name: str = "esmc_600m",
                 regression_hidden_dim: int = 128,
                 regression_dropout: float = 0.1,
                 temp_embedding_dim: int = 16,
                 use_attention: bool = True,
                 attention_heads: int = 8,
                 attention_dropout: float = 0.1,
                 use_positional_encoding: bool = True,
                 use_enhanced_features: bool = True,
                 improved_temp_integration: bool = True):
        super().__init__()

        logger.info(f"Initializing EnhancedTemperatureAwareESMModel...")
        logger.info(f"Loading base ESM-C Model: {esm_model_name}")
        try:
            # Load the base ESMC model object
            self.esm_model = ESMC.from_pretrained(esm_model_name)
        except Exception as e:
            logger.error(f"Failed to load ESM-C model '{esm_model_name}'. Error: {e}")
            raise

        self.esm_model.eval()  # Set base model to evaluation mode
        self.esm_model_name = esm_model_name

        # --- Freeze ESM-C parameters ---
        logger.info("Freezing ESM-C model parameters...")
        for param in self.esm_model.parameters():
            param.requires_grad = False

        # --- Detect embedding dimension ---
        # Do a dummy forward pass on CPU first to avoid moving large model prematurely
        temp_cpu_model = ESMC.from_pretrained(esm_model_name)
        embedding_dim = -1
        try:
            with torch.no_grad():
                test_protein = ESMProtein(sequence="A")  # Minimal sequence
                encoded = temp_cpu_model.encode(test_protein)
                logits_output = temp_cpu_model.logits(
                    encoded, LogitsConfig(sequence=True, return_embeddings=True)
                )
                embedding_dim = logits_output.embeddings.size(-1)
                logger.info(f"Detected ESM embedding dimension: {embedding_dim}")
        except Exception as e:
            logger.error(f"Error during embedding dimension detection: {e}")
            raise ValueError(f"Could not determine embedding dimension for {esm_model_name}.")
        finally:
            del temp_cpu_model  # Clean up temporary model

        if embedding_dim <= 0:
            raise ValueError("Failed to detect a valid embedding dimension.")

        self.esm_hidden_dim = embedding_dim
        
        # Store configuration
        self.use_attention = use_attention
        self.use_positional_encoding = use_positional_encoding
        self.use_enhanced_features = use_enhanced_features
        self.improved_temp_integration = improved_temp_integration
        
        # --- Advanced Temperature Integration ---
        if improved_temp_integration:
            self.temp_encoder = TemperatureEncoding(
                embedding_dim=temp_embedding_dim,
                dropout=regression_dropout
            )
            temp_conditioning_dim = temp_embedding_dim
        else:
            # Simple temperature feature
            temp_conditioning_dim = 1
        
        # --- Positional Encoding ---
        if use_positional_encoding:
            self.positional_encoding = PositionalEncoding(
                d_model=embedding_dim,
                dropout=regression_dropout
            )
        
        # --- Feature Processing ---
        if use_enhanced_features:
            # Define feature dimensions
            # Default dimensions for common features (can be customized)
            self.feature_dims = {
                'normalized_resid': 1,
                'core_exterior_encoded': 1,
                'secondary_structure_encoded': 1,
                'relative_accessibility': 1,
                'phi_norm': 1,
                'psi_norm': 1,
                'voxel_rmsf': 1,
                'bfactor_norm': 1
            }
            
            feature_output_dim = min(64, embedding_dim // 2)  # Make it smaller than ESM embeddings
            
            self.feature_processor = FeatureProcessor(
                feature_dims=self.feature_dims,
                output_dim=feature_output_dim,
                dropout=regression_dropout
            )
            
            # Combine ESM embeddings with processed features
            combined_dim = embedding_dim + feature_output_dim
            
            self.feature_esm_fusion = nn.Sequential(
                nn.Linear(combined_dim, embedding_dim),
                nn.GELU(),
                nn.LayerNorm(embedding_dim),
                nn.Dropout(regression_dropout)
            )
        else:
            combined_dim = embedding_dim
        
        # --- Attention Mechanism ---
        if use_attention:
            self.attention = AttentionWithTemperature(
                embed_dim=embedding_dim,
                num_heads=attention_heads,
                dropout=attention_dropout,
                temp_dim=temp_conditioning_dim
            )
        
        # --- Create Temperature-Aware Regression Head ---
        regression_input_dim = embedding_dim
        
        logger.info(f"Creating regression head. Input dimension: {regression_input_dim}")

        if regression_hidden_dim > 0:
            self.regression_head = nn.Sequential(
                nn.LayerNorm(regression_input_dim),
                nn.Linear(regression_input_dim, regression_hidden_dim),
                nn.GELU(),
                nn.Dropout(regression_dropout),
                nn.Linear(regression_hidden_dim, 1)  # Output single RMSF value
            )
            logger.info(f"Using MLP regression head (LayerNorm -> Linear({regression_input_dim},{regression_hidden_dim}) -> GELU -> Dropout -> Linear({regression_hidden_dim},1))")
        else:  # Direct linear layer after LayerNorm
            self.regression_head = nn.Sequential(
                nn.LayerNorm(regression_input_dim),
                nn.Dropout(regression_dropout),
                nn.Linear(regression_input_dim, 1)
            )
            logger.info(f"Using Linear regression head (LayerNorm -> Dropout -> Linear({regression_input_dim},1))")

        self._log_parameter_counts()
        logger.info("EnhancedTemperatureAwareESMModel initialized successfully.")

    def _log_parameter_counts(self):
        total_params = sum(p.numel() for p in self.parameters())
        esm_params = sum(p.numel() for p in self.esm_model.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        logger.info(f"Parameter Counts:")
        logger.info(f"  Total parameters: {total_params:,}")
        logger.info(f"  ESM-C parameters (frozen): {esm_params:,}")
        logger.info(f"  Trainable parameters: {trainable_params:,}")
        if total_params > 0:
            logger.info(f"  Trainable percentage: {trainable_params/total_params:.4%}")

    def _process_protein_features(self, 
                                  batch_features: Dict[str, Any], 
                                  sequences: List[str], 
                                  device: torch.device) -> List[torch.Tensor]:
        """
        Process and align structural features to sequence lengths.
        
        Args:
            batch_features: Dictionary of feature tensors from dataloader
            sequences: List of sequence strings
            device: Device to place tensors on
            
        Returns:
            List of processed feature tensors aligned to sequence lengths
        """
        # Initialize list of per-sequence feature tensors
        processed_features = []
        batch_size = len(sequences)
        
        for i in range(batch_size):
            seq_length = len(sequences[i])
            seq_features = {}
            
            # Process each feature type
            for feature_name, feature_values in batch_features.items():
                if isinstance(feature_values, list):
                    # Per-residue feature
                    if i < len(feature_values):
                        feature_tensor = feature_values[i]
                        
                        # Align length with sequence if needed
                        if feature_tensor.size(0) > 0:  # Non-empty tensor
                            if feature_tensor.size(0) != seq_length:
                                # Trim or pad as needed
                                if feature_tensor.size(0) > seq_length:
                                    # Trim
                                    feature_tensor = feature_tensor[:seq_length]
                                else:
                                    # Pad with zeros
                                    padding = torch.zeros(seq_length - feature_tensor.size(0), 
                                                        dtype=feature_tensor.dtype, 
                                                        device=feature_tensor.device)
                                    feature_tensor = torch.cat([feature_tensor, padding], dim=0)
                            
                            # Ensure feature is on the correct device
                            feature_tensor = feature_tensor.to(device)
                            seq_features[feature_name] = feature_tensor
                        else:
                            # Create zero tensor for empty features
                            seq_features[feature_name] = torch.zeros(seq_length, dtype=torch.float32, device=device)
                    else:
                        # Create zero tensor if feature is missing for this sequence
                        seq_features[feature_name] = torch.zeros(seq_length, dtype=torch.float32, device=device)
                else:
                    # Global feature - replicate across sequence length
                    if feature_values.size(0) > i:
                        value = feature_values[i].to(device)
                        seq_features[feature_name] = value.expand(seq_length)
                    else:
                        # Create zero tensor if feature is missing for this sequence
                        seq_features[feature_name] = torch.zeros(seq_length, dtype=torch.float32, device=device)
            
            # Stack features for this sequence
            feature_list = []
            for feature_name in self.feature_dims.keys():
                if feature_name in seq_features:
                    # Add dimension if needed
                    if seq_features[feature_name].dim() == 1:
                        feature_tensor = seq_features[feature_name].unsqueeze(1)
                    else:
                        feature_tensor = seq_features[feature_name]
                    feature_list.append(feature_tensor)
                else:
                    # Create zero tensor if feature is completely missing
                    feature_list.append(torch.zeros(seq_length, 1, dtype=torch.float32, device=device))
            
            # Concatenate features for this sequence
            if feature_list:
                seq_feature_tensor = torch.cat(feature_list, dim=1)
                processed_features.append(seq_feature_tensor)
            else:
                # Empty feature tensor if no features available
                seq_feature_tensor = torch.zeros(seq_length, len(self.feature_dims), dtype=torch.float32, device=device)
                processed_features.append(seq_feature_tensor)
        
        return processed_features

    def forward(self,
                sequences: List[str],
                temperatures: torch.Tensor,  # Expecting a BATCHED tensor of SCALED temperatures
                target_rmsf_values: Optional[List[torch.Tensor]] = None,  # Optional targets
                features: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Forward pass incorporating sequence embeddings, temperature, and structural features.

        Args:
            sequences: List of amino acid sequence strings (batch_size).
            temperatures: Tensor of SCALED temperature values for each sequence,
                          shape [batch_size]. MUST be pre-scaled.
            target_rmsf_values: Optional list of target RMSF tensors for loss calculation.
            features: Optional dictionary of structural features from dataloader.

        Returns:
            Dictionary containing 'predictions', 'loss', 'metrics'.
        """
        # --- Basic Input Checks ---
        if len(sequences) != len(temperatures):
            msg = f"Batch size mismatch: {len(sequences)} sequences vs {len(temperatures)} temperatures."
            logger.error(msg)
            raise ValueError(msg)
        if target_rmsf_values is not None and len(sequences) != len(target_rmsf_values):
            msg = f"Batch size mismatch: {len(sequences)} sequences vs {len(target_rmsf_values)} target RMSF values."
            logger.error(msg)
            raise ValueError(msg)

        # --- Setup Device ---
        # Infer device from parameters
        device = next(self.parameters()).device
        # Ensure ESM base model is on the correct device
        if next(self.esm_model.parameters()).device != device:
            self.esm_model.to(device)

        # --- Prepare ESMProtein objects ---
        proteins = []
        original_indices = []  # Store original batch index for each valid protein
        skipped_indices = []

        for i, seq_str in enumerate(sequences):
            if not seq_str or len(seq_str) == 0:
                logger.debug(f"Skipping empty sequence at original batch index {i}.")
                skipped_indices.append(i)
                continue
            try:
                proteins.append(ESMProtein(sequence=seq_str))
                original_indices.append(i)
            except Exception as e_prot:
                logger.warning(f"Could not create ESMProtein for sequence at index {i}. Error: {e_prot}. Skipping.")
                skipped_indices.append(i)

        if not proteins:
            logger.warning("No valid sequences found in the batch to process.")
            # Return structure consistent with successful run but empty preds/zero loss
            return {
                'predictions': [torch.tensor([], device=device) for _ in sequences],  # Match input batch size
                'loss': torch.tensor(0.0, device=device, requires_grad=True if self.training else False),
                'metrics': {'pearson_correlation': 0.0}
            }

        # --- Process Proteins ---
        all_predictions = []  # Store final per-residue predictions for each protein
        processed_indices_map = {}  # Map index in `all_predictions` back to original batch index

        try:
            # --- Advanced Temperature Processing ---
            if self.improved_temp_integration:
                # Create rich temperature embeddings (batch_size, temp_embedding_dim)
                batch_temp_embeddings = self.temp_encoder(temperatures.to(device))
            else:
                # Simple temperature feature (batch_size, 1)
                batch_temp_embeddings = temperatures.to(device).unsqueeze(1)
            
            # --- Process Structural Features ---
            batch_processed_features = None
            if self.use_enhanced_features and features is not None:
                # Process and align features with sequences
                batch_processed_features = self._process_protein_features(features, sequences, device)
            
            # --- Per-Protein Processing ---
            for protein_idx, protein in enumerate(proteins):
                original_batch_idx = original_indices[protein_idx]
                current_temp_embedding = batch_temp_embeddings[original_batch_idx]  # Get temperature embedding

                try:
                    # 1. Get ESM Embeddings (No Gradients for ESM part)
                    with torch.no_grad():
                        encoded_protein = self.esm_model.encode(protein)
                        logits_output = self.esm_model.logits(
                            encoded_protein,
                            LogitsConfig(sequence=True, return_embeddings=True)
                        )

                    if logits_output.embeddings is None:
                        logger.warning(f"No embeddings returned for protein {original_batch_idx}. Skipping.")
                        continue

                    # Embeddings shape: [1, seq_len_with_tokens, hidden_dim]
                    embeddings = logits_output.embeddings.to(device)
                    # Remove batch dimension: [seq_len_with_tokens, hidden_dim]
                    embeddings_tokens = embeddings.squeeze(0)
                    
                    # 2. Extract Residue Embeddings (Remove BOS/EOS tokens)
                    original_seq_len = len(protein.sequence)  # Length of the actual AA sequence
                    expected_tokens = original_seq_len + 2  # Assuming BOS and EOS tokens

                    if len(embeddings_tokens) >= expected_tokens:
                        # Slice: Start after BOS (index 1), end before EOS
                        residue_embeddings = embeddings_tokens[1:expected_tokens-1]
                    else:
                        logger.warning(f"Embedding tensor length ({len(embeddings_tokens)}) is shorter than "
                                      f"expected seq+BOS+EOS ({expected_tokens}) for original sequence {original_batch_idx}. "
                                      "Cannot reliably extract residue embeddings. Skipping.")
                        continue
                    
                    # Apply positional encoding if enabled
                    if self.use_positional_encoding:
                        residue_embeddings = self.positional_encoding(residue_embeddings)
                    
                    # 3. Process structural features if available
                    combined_features = residue_embeddings
                    if self.use_enhanced_features and batch_processed_features is not None and protein_idx < len(batch_processed_features):
                        # Get features for this protein
                        protein_features = batch_processed_features[protein_idx]
                        
                        # Ensure features align with residue embeddings
                        if protein_features.size(0) != residue_embeddings.size(0):
                            logger.warning(f"Feature length mismatch for protein {original_batch_idx}: "
                                         f"features={protein_features.size(0)}, embeddings={residue_embeddings.size(0)}. "
                                         "Adjusting feature length.")
                            
                            # Adjust feature length to match embeddings
                            if protein_features.size(0) > residue_embeddings.size(0):
                                protein_features = protein_features[:residue_embeddings.size(0)]
                            else:
                                padding = torch.zeros(
                                    residue_embeddings.size(0) - protein_features.size(0),
                                    protein_features.size(1),
                                    dtype=protein_features.dtype,
                                    device=protein_features.device
                                )
                                protein_features = torch.cat([protein_features, padding], dim=0)
                        
                        # Combine ESM embeddings with structural features
                        # Use feature processor to get aligned features
                        processed_features = self.feature_processor({
                            feature_name: protein_features[:, i].unsqueeze(1)
                            for i, feature_name in enumerate(self.feature_dims.keys())
                        })
                        
                        # Concatenate with ESM embeddings
                        combined_input = torch.cat([residue_embeddings, processed_features], dim=1)
                        
                        # Fuse ESM embeddings and features
                        combined_features = self.feature_esm_fusion(combined_input)
                    
                    # 4. Apply attention with temperature conditioning if enabled
                    if self.use_attention:
                        # Add batch dimension for attention
                        features_batch = combined_features.unsqueeze(0)  # [1, seq_len, hidden_dim]
                        temp_embed_batch = current_temp_embedding.unsqueeze(0)  # [1, temp_dim]
                        
                        # Apply attention with temperature conditioning
                        attended_features = self.attention(features_batch, temp_embed_batch)
                        
                        # Remove batch dimension
                        processed_embeddings = attended_features.squeeze(0)  # [seq_len, hidden_dim]
                    else:
                        # No attention - use combined features directly
                        processed_embeddings = combined_features
                    
                    # 5. Apply regression head to get predictions
                    # Ensure head is in correct mode (train/eval) based on model state
                    self.regression_head.train(self.training)
                    # Get per-residue predictions
                    token_predictions = self.regression_head(processed_embeddings).squeeze(-1)
                    
                    # Store predictions
                    all_predictions.append(token_predictions)
                    processed_indices_map[len(all_predictions)-1] = original_batch_idx

                except Exception as e_inner:
                    logger.error(f"Error processing protein at original batch index {original_batch_idx}: {e_inner}", exc_info=True)
                    # Continue to the next protein in the batch

        except Exception as e_outer:
            logger.error(f"Error during main forward loop: {e_outer}", exc_info=True)
            # Return empty/zero structure if outer loop fails catastrophically
            return {
                'predictions': [torch.tensor([], device=device) for _ in sequences],
                'loss': torch.tensor(0.0, device=device, requires_grad=True if self.training else False),
                'metrics': {'pearson_correlation': 0.0}
            }

        # --- Loss Calculation (Optional) ---
        loss = None
        metrics = {'pearson_correlation': 0.0}  # Default metrics

        if target_rmsf_values is not None:
            valid_losses = []
            valid_correlations = []
            num_valid_pairs = 0

            # Iterate through the predictions we successfully generated
            for pred_idx, prediction_tensor in enumerate(all_predictions):
                original_batch_idx = processed_indices_map[pred_idx]
                target_tensor = target_rmsf_values[original_batch_idx].to(device)

                # Align lengths (prediction might be slightly off if slicing warning occurred)
                min_len = min(len(prediction_tensor), len(target_tensor))
                if min_len <= 1:  # Need at least 2 points for correlation
                    continue

                pred_aligned = prediction_tensor[:min_len]
                target_aligned = target_tensor[:min_len]

                # Calculate standard MSE Loss
                mse = F.mse_loss(pred_aligned, target_aligned, reduction='mean')
                if not torch.isnan(mse) and not torch.isinf(mse):
                    valid_losses.append(mse)

                # Calculate Pearson Correlation for metrics reporting only
                # (not used in loss calculation)
                pearson_corr = self.safe_pearson_correlation(pred_aligned, target_aligned)
                if not torch.isnan(pearson_corr):
                    valid_correlations.append(pearson_corr)

                num_valid_pairs += 1

            # Average loss and correlation over valid pairs in the batch
            if valid_losses:
                # Average the loss across samples in the batch
                loss = torch.stack(valid_losses).mean()
                if torch.isnan(loss):  # Handle potential NaN if all losses were somehow NaN
                    loss = torch.tensor(0.0, device=device, requires_grad=True if self.training else False)
            else:  # No valid pairs, set loss to 0
                loss = torch.tensor(0.0, device=device, requires_grad=True if self.training else False)

            if valid_correlations:
                # Average correlation across samples for metrics reporting
                metrics['pearson_correlation'] = torch.stack(valid_correlations).mean().item()
            else:  # No valid correlations calculated
                metrics['pearson_correlation'] = 0.0  # Keep as float

        # Ensure loss is always a tensor, required by training loop
        if loss is None:
            loss = torch.tensor(0.0, device=device, requires_grad=True if self.training else False)

        # --- Reconstruct Output List ---
        # Create a list of tensors matching the original batch size, filling with
        # predictions where available and empty tensors otherwise.
        final_predictions_list = [torch.tensor([], device=device) for _ in sequences]
        for pred_idx, pred_tensor in enumerate(all_predictions):
            original_batch_idx = processed_indices_map[pred_idx]
            final_predictions_list[original_batch_idx] = pred_tensor

        return {'predictions': final_predictions_list, 'loss': loss, 'metrics': metrics}

    @staticmethod
    def safe_pearson_correlation(x: torch.Tensor, y: torch.Tensor, epsilon: float = 1e-8) -> torch.Tensor:
        """
        Calculate Pearson correlation safely, returning 0 for std dev near zero or len < 2.
        Used for metrics reporting only, not for optimization.
        """
        # Ensure float type
        x = x.float()
        y = y.float()

        # Check for conditions where correlation is undefined or unstable
        if len(x) < 2 or torch.std(x) < epsilon or torch.std(y) < epsilon:
            return torch.tensor(0.0, device=x.device, dtype=torch.float32)

        vx = x - torch.mean(x)
        vy = y - torch.mean(y)

        # Use matrix multiplication for covariance calculation for efficiency if needed,
        # but direct sum is fine for typical sequence lengths here.
        cov = torch.sum(vx * vy)
        sx = torch.sqrt(torch.sum(vx ** 2))
        sy = torch.sqrt(torch.sum(vy ** 2))
        denominator = sx * sy

        # Check for near-zero denominator
        if denominator < epsilon:
            return torch.tensor(0.0, device=x.device, dtype=torch.float32)

        corr = cov / denominator
        # Clamp to handle potential floating point inaccuracies near +/- 1
        corr = torch.clamp(corr, -1.0, 1.0)

        # Final NaN check (should be rare after previous checks, but just in case)
        if torch.isnan(corr):
            logger.warning("NaN detected during Pearson Correlation calculation despite checks. Returning 0.")
            return torch.tensor(0.0, device=x.device, dtype=torch.float32)

        return corr

    @torch.no_grad()
    def predict(self,
                sequences: List[str],
                scaled_temperatures: torch.Tensor,  # Expecting tensor shape [batch_size]
                features: Optional[Dict[str, Any]] = None) -> List[np.ndarray]:
        """
        Predict RMSF values for sequences at given SCALED temperatures.

        Args:
            sequences: List of amino acid sequences.
            scaled_temperatures: Tensor of SCALED temperatures (one per sequence).
            features: Optional dictionary of structural features from dataloader.

        Returns:
            List of NumPy arrays containing predicted RMSF values for each sequence.
        """
        self.eval()  # Ensure evaluation mode

        if len(sequences) != len(scaled_temperatures):
            raise ValueError("Number of sequences must match number of temperatures for prediction.")

        # Pass sequences and scaled temperatures to the forward method
        outputs = self.forward(
            sequences=sequences,
            temperatures=scaled_temperatures.to(next(self.parameters()).device),
            target_rmsf_values=None,
            features=features
        )

        # Convert predictions tensor list to list of numpy arrays
        np_predictions = []
        for pred_tensor in outputs['predictions']:
            if pred_tensor is not None and pred_tensor.numel() > 0:
                np_predictions.append(pred_tensor.cpu().numpy())
            else:  # Handle cases where prediction failed for a sequence
                np_predictions.append(np.array([], dtype=np.float32))

        return np_predictions


# Factory function to create model based on config
def create_model_from_config(config: Dict[str, Any]) -> nn.Module:
    """
    Create a model instance based on configuration.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Model instance
    """
    model_config = config.get('model', {})
    esm_version = model_config.get('esm_version', 'esmc_600m')
    
    regression_config = model_config.get('regression', {})
    hidden_dim = regression_config.get('hidden_dim', 64)
    dropout = regression_config.get('dropout', 0.1)
    
    architecture_config = model_config.get('architecture', {})
    use_enhanced_features = architecture_config.get('use_enhanced_features', True)
    use_attention = architecture_config.get('use_attention', True)
    attention_heads = architecture_config.get('attention_heads', 8)
    attention_dropout = architecture_config.get('attention_dropout', 0.1)
    improved_temp_integration = architecture_config.get('improved_temp_integration', True)
    
    logger.info(f"Creating model with config: esm_version={esm_version}, hidden_dim={hidden_dim}")
    logger.info(f"Enhanced features: {use_enhanced_features}, Attention: {use_attention}, "
               f"Improved temperature integration: {improved_temp_integration}")
    
    return EnhancedTemperatureAwareESMModel(
        esm_model_name=esm_version,
        regression_hidden_dim=hidden_dim,
        regression_dropout=dropout,
        use_attention=use_attention,
        attention_heads=attention_heads,
        attention_dropout=attention_dropout,
        use_positional_encoding=True,
        use_enhanced_features=use_enhanced_features,
        improved_temp_integration=improved_temp_integration
    )
---------------------------------------------------------
===== FILE: ./predict.py =====
import os
import torch
import argparse
import yaml
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import pandas as pd
import logging
from pathlib import Path
import time
import json
from typing import Dict, List, Optional, Tuple, Any, Callable
from collections import defaultdict
import torch.nn as nn
from dataset import load_numpy_dict
import re # ensure re is imported


# Import our new model and dataset functions
from model import EnhancedTemperatureAwareESMModel, create_model_from_config
from dataset import load_sequences_from_fasta, load_numpy_dict, load_feature_norm_params
from train import log_gpu_memory, get_temperature_scaler
from data_processor import create_instance_key, get_domain_id_from_instance_key, INSTANCE_KEY_SEPARATOR

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s [%(filename)s:%(lineno)d] - %(message)s')
logger = logging.getLogger(__name__)

def load_model_for_prediction(checkpoint_path: str, device: torch.device) -> Tuple[Optional[EnhancedTemperatureAwareESMModel], Optional[Dict]]:
    """Load a trained enhanced model from checkpoint."""
    logger.info(f"Loading model checkpoint from: {checkpoint_path}")
    if not os.path.exists(checkpoint_path):
        logger.error(f"Checkpoint file not found: {checkpoint_path}")
        return None, None

    try:
        checkpoint = torch.load(checkpoint_path, map_location=device)
    except Exception as e:
         logger.error(f"Failed to load checkpoint file {checkpoint_path}: {e}", exc_info=True)
         return None, None

    required_keys = ['config', 'model_state_dict', 'epoch']
    if not all(key in checkpoint for key in required_keys):
         missing = [k for k in required_keys if k not in checkpoint]
         logger.error(f"Checkpoint {checkpoint_path} is missing required keys: {missing}. Found: {list(checkpoint.keys())}")
         return None, None

    config_from_ckpt = checkpoint['config']
    logger.info("Config loaded from checkpoint.")
    logger.debug(f"Checkpoint Config: {json.dumps(config_from_ckpt, indent=2)}")

    try:
        logger.info(f"Creating model from checkpoint config")
        model = create_model_from_config(config_from_ckpt)
        logger.info("Model instance created.")
    except Exception as e:
         logger.error(f"Error creating model from config: {e}", exc_info=True)
         return None, None

    # Load model state dictionary
    try:
        missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        if missing_keys:
             logger.warning(f"State dict missing keys: {missing_keys}")
        if unexpected_keys:
             logger.warning(f"State dict has unexpected keys: {unexpected_keys}")

        logger.info(f"Model weights loaded successfully.")
    except Exception as e:
         logger.error(f"Error loading state_dict into model: {e}", exc_info=True)
         return None, None

    model = model.to(device)
    model.eval()

    logger.info(f"Model loaded to {device} and set to eval mode.")
    logger.info(f"  Trained for {checkpoint['epoch']+1} epochs.")
    if 'val_corr' in checkpoint:
        logger.info(f"  Best Validation Corr at save time: {checkpoint.get('val_corr', 'N/A'):.6f}")

    return model, config_from_ckpt

def group_sequences_by_length(sequences: Dict[str, str], batch_size: int, bucket_size: int = 50) -> List[List[Tuple[str, str]]]:
    """Groups sequences by length into batches for efficient prediction."""
    if not sequences: return []
    length_buckets = defaultdict(list)
    for seq_id, seq in sequences.items():
        bucket_idx = len(seq) // bucket_size
        length_buckets[bucket_idx].append((seq_id, seq))

    all_batches = []
    # Process shortest first generally helps memory management
    for bucket_idx in sorted(length_buckets.keys()):
        bucket_items = length_buckets[bucket_idx]
        for i in range(0, len(bucket_items), batch_size):
            batch = bucket_items[i : i + batch_size]
            all_batches.append(batch)
    logger.info(f"Grouped {len(sequences)} sequences into {len(all_batches)} batches for prediction.")
    return all_batches

def load_feature_data(
    features_to_load: List[str],
    data_dir: str,
    instance_keys_to_load: List[str], # Renamed for clarity
    file_prefix: str = "predict" # Added prefix argument
) -> Dict[str, Dict[str, np.ndarray]]:
    """
    Load structural feature data for specific instance keys.

    Args:
        features_to_load: List of feature names to load (e.g., 'normalized_resid').
        data_dir: Directory containing feature files.
        instance_keys_to_load: List of instance keys to load features for.
        file_prefix: Prefix of the feature files (e.g., 'train', 'predict', 'holdout').

    Returns:
        Dictionary mapping instance keys to their feature dictionaries.
    """
    # Initialize dict for all requested keys, even if features aren't found for some
    feature_data = {key: {} for key in instance_keys_to_load}
    loaded_some_feature = False

    for feature in features_to_load:
        # Construct filename using the prefix
        feature_filename = f"{file_prefix}_{feature}.npy"
        feature_path = os.path.join(data_dir, feature_filename)

        if os.path.exists(feature_path):
            try:
                # Load the entire dictionary for this feature
                feature_dict = load_numpy_dict(feature_path)
                found_count = 0
                # Populate the main feature_data dict for the requested keys
                for key in instance_keys_to_load:
                    if key in feature_dict:
                        feature_data[key][feature] = feature_dict[key]
                        found_count += 1
                        loaded_some_feature = True
                if found_count > 0:
                     logger.debug(f"Loaded '{feature}' data for {found_count} requested instances from {feature_filename}")
                # else: logger.debug(f"Feature file {feature_filename} loaded, but contained no data for the requested instance keys.")

            except FileNotFoundError:
                logger.warning(f"Feature file not found during load attempt (should not happen after exists check): {feature_path}")
            except Exception as e:
                logger.warning(f"Error loading or processing feature file {feature_path}: {e}")
        else:
            logger.warning(f"Required feature file not found: {feature_path}")

    # Clean up keys that have no features loaded at all
    if loaded_some_feature:
        # Keep keys only if they have *some* feature data associated
        # This check might be too strict if *some* features are expected missing for certain keys
        # Let's keep all original keys requested, even if empty feature dicts
        # feature_data = {k: v for k, v in feature_data.items() if v}
        pass # Keep all keys in the dict for simplicity downstream
    else:
         logger.warning(f"No features loaded (either none expected or none found matching prefix '{file_prefix}' in {data_dir}).")


    return feature_data
# def load_feature_data(features_to_load: List[str], data_dir: str, domain_ids: List[str]) -> Dict[str, Dict[str, np.ndarray]]:
#     """
#     Load structural feature data for sequences.
    
#     Args:
#         features_to_load: List of feature names to load
#         data_dir: Directory containing feature files
#         domain_ids: List of domain IDs to load features for
        
#     Returns:
#         Dictionary mapping domain IDs to feature dictionaries
#     """
#     feature_data = {domain_id: {} for domain_id in domain_ids}
    
#     for feature in features_to_load:
#         feature_path = os.path.join(data_dir, f"predict_{feature}.npy")
#         if os.path.exists(feature_path):
#             try:
#                 feature_dict = load_numpy_dict(feature_path)
#                 for domain_id in domain_ids:
#                     if domain_id in feature_dict:
#                         feature_data[domain_id][feature] = feature_dict[domain_id]
#                 logger.info(f"Loaded {feature} data for {len(feature_dict)} domains")
#             except Exception as e:
#                 logger.warning(f"Error loading {feature} data: {e}")
#         else:
#             logger.warning(f"Feature file not found: {feature_path}")
    
#     return feature_data


def activate_mc_dropout(model: nn.Module):
    """Activates dropout layers for Monte Carlo Dropout inference."""
    for module in model.modules():
        if isinstance(module, nn.Dropout):
            module.train()
            logger.debug(f"Activated MC Dropout for: {module}")

def prepare_prediction_features(
    feature_lookup_keys: List[str], # Renamed from domain_ids for clarity
    model_config: Dict[str, Any],
    scaling_params: Dict[str, Dict[str, float]],
    feature_data: Dict[str, Dict[str, np.ndarray]],
    target_sequence_lengths: Optional[List[int]] = None # Optional: To ensure feature length matches sequence
) -> Optional[Dict[str, Any]]:
    """
    Prepare and normalize features for prediction using specific lookup keys.

    Args:
        feature_lookup_keys: List of keys (expected to be instance_keys) to use for lookup in feature_data.
        model_config: Model configuration dictionary.
        scaling_params: Feature normalization parameters (e.g., from feature_normalization.json).
        feature_data: Dictionary mapping instance_keys to raw feature dictionaries.
        target_sequence_lengths: Optional list of sequence lengths corresponding to feature_lookup_keys,
                                 used to ensure feature arrays match sequence length.

    Returns:
        Dictionary of processed feature tensors ready for the model, or None if features are not used/available.
    """
    # Check if features are enabled in the model config
    if not model_config or not model_config.get('model', {}).get('architecture', {}).get('use_enhanced_features', False):
        logger.debug("Enhanced features not enabled in model config. Skipping feature preparation.")
        return None
    # Check if necessary data structures are provided
    if not feature_data:
        logger.warning("Feature data dictionary not provided, but enhanced features are enabled. Cannot prepare features.")
        return None
    if not scaling_params:
         logger.warning("Feature normalization parameters not provided. Features will be used without normalization.")
         # Continue without normalization if scaling_params is missing

    processed_batch_features = {} # Dictionary to hold prepared feature tensors/lists

    # Determine which features the model expects based on config
    feature_config = model_config.get('data', {}).get('features', {})
    expected_features = []
    if feature_config.get('use_position_info', True): expected_features.append('normalized_resid')
    if feature_config.get('use_structure_info', True): expected_features.extend(['core_exterior_encoded', 'secondary_structure_encoded'])
    if feature_config.get('use_accessibility', True): expected_features.append('relative_accessibility')
    if feature_config.get('use_backbone_angles', True): expected_features.extend(['phi_norm', 'psi_norm'])
    if feature_config.get('use_protein_size', True): expected_features.append('protein_size')
    if feature_config.get('use_voxel_rmsf', True): expected_features.append('voxel_rmsf')
    if feature_config.get('use_bfactor', True): expected_features.append('bfactor_norm')

    # Check which expected features are actually available in feature_data and have normalization params (if provided)
    # Use the first key to check general availability, specific lookups happen in the loop
    available_feature_keys = set()
    if feature_data:
         first_key_data = next(iter(feature_data.values()), {})
         available_feature_keys = set(first_key_data.keys())

    # Process each expected feature
    for feature_name in expected_features:
        if feature_name not in available_feature_keys:
             logger.debug(f"Expected feature '{feature_name}' not found in available feature data keys. Skipping.")
             continue

        is_global = scaling_params.get(feature_name, {}).get('is_global', False) if scaling_params else False
        feature_batch_values = [] # Will hold tensors for this feature across the batch

        for i, lookup_key in enumerate(feature_lookup_keys):
            seq_len = target_sequence_lengths[i] if target_sequence_lengths and i < len(target_sequence_lengths) else None

            # Default value if feature is missing for this specific key or is NaN
            # For per-residue: zero array of appropriate length
            # For global: zero scalar
            default_value = 0.0 if is_global else np.zeros(seq_len if seq_len is not None else 1, dtype=np.float32) # Default length 1 if seq_len unknown

            feature_val = feature_data.get(lookup_key, {}).get(feature_name, default_value)

            # Handle potential NaNs in the loaded feature value
            is_nan = pd.isna(feature_val)
            # Check if *any* value is NaN (for arrays) OR if the scalar itself is NaN
            if (isinstance(is_nan, np.ndarray) and is_nan.any()) or \
            (not isinstance(is_nan, np.ndarray) and is_nan): # Check if scalar is True (meaning NaN)
                logger.warning(f"NaN found for feature '{feature_name}' in instance '{lookup_key}'. Replacing with default ({default_value}).")
                feature_val = default_value # Replace the original value

            # --- Normalization ---
            normalized_val = feature_val # Default to original value
            if scaling_params and feature_name in scaling_params:
                params = scaling_params[feature_name]
                norm_min = params.get('min')
                norm_max = params.get('max')
                if norm_min is not None and norm_max is not None:
                    range_val = norm_max - norm_min
                    if abs(range_val) < 1e-8: # Avoid division by zero if min == max
                         normalized_val = np.zeros_like(feature_val, dtype=np.float32) if isinstance(feature_val, np.ndarray) else 0.0
                    else:
                         # Apply min-max scaling
                         # Ensure feature_val is float for the calculation if it's scalar
                         current_val_float = np.array(feature_val, dtype=np.float32)
                         normalized_val = (current_val_float - norm_min) / range_val
                # else: logger.debug(f"Min/max not found for '{feature_name}' in scaling params. Using raw value.") # Optional debug
            # else: logger.debug(f"No scaling params found for '{feature_name}'. Using raw value.") # Optional debug


            # --- Type Conversion and Length Adjustment ---
            if is_global:
                # Ensure global features are single-element tensors
                tensor_val = torch.tensor([normalized_val], dtype=torch.float32)
            else:
                # Ensure per-residue features are numpy arrays first
                if not isinstance(normalized_val, np.ndarray):
                    normalized_val = np.array([normalized_val], dtype=np.float32) # Make it an array if scalar slipped through

                # Ensure length matches sequence length
                if seq_len is not None and normalized_val.shape[0] != seq_len:
                     logger.warning(f"Adjusting length of feature '{feature_name}' for instance '{lookup_key}'. "
                                    f"Expected {seq_len}, got {normalized_val.shape[0]}.")
                     if normalized_val.shape[0] > seq_len:
                          normalized_val = normalized_val[:seq_len] # Truncate
                     else: # Pad
                          padding = np.zeros(seq_len - normalized_val.shape[0], dtype=np.float32)
                          normalized_val = np.concatenate((normalized_val, padding))

                tensor_val = torch.tensor(normalized_val, dtype=torch.float32)

            feature_batch_values.append(tensor_val)

        # Store the list of tensors (for per-residue) or create a stacked tensor (for global)
        # The model's _process_protein_features expects this structure
        if is_global:
             # Stack global features into a single tensor for the batch [batch_size] (or [batch_size, 1])
             try:
                  processed_batch_features[feature_name] = torch.stack(feature_batch_values).squeeze(-1) # Shape [batch_size]
             except Exception as e:
                  logger.error(f"Error stacking global feature '{feature_name}': {e}. Values: {feature_batch_values}")
                  # Fallback: store as list
                  processed_batch_features[feature_name] = feature_batch_values
        else:
             # Keep per-residue features as a list of tensors
             processed_batch_features[feature_name] = feature_batch_values


    if not processed_batch_features:
         logger.warning("No features were prepared, although enhanced features seem enabled.")
         return None

    return processed_batch_features

# def predict_rmsf_at_temperature(
#     model: EnhancedTemperatureAwareESMModel,
#     sequences: Dict[str, str],
#     target_temperature: float,
#     temp_scaler: Callable[[float], float],
#     batch_size: int,
#     device: torch.device,
#     feature_data: Optional[Dict[str, Dict[str, np.ndarray]]] = None,
#     feature_norm_params: Optional[Dict[str, Dict[str, float]]] = None,
#     model_config: Optional[Dict[str, Any]] = None,
#     use_amp: bool = True
# ) -> Tuple[Dict[str, np.ndarray], Dict[str, str]]: # Return results and id_map
#     """
#     Predict RMSF values for sequences at a specific target temperature. Handles different input ID formats.

#     Args:
#         model: The trained model
#         sequences: Dictionary mapping sequence IDs (from FASTA header) to sequences
#         target_temperature: The single temperature (raw, unscaled) to predict at
#         temp_scaler: The function to scale the raw target temperature
#         batch_size: Batch size for inference
#         device: Device ('cuda' or 'cpu')
#         feature_data: Optional dictionary of structural feature data (keyed by instance_key)
#         feature_norm_params: Optional dictionary of feature normalization parameters
#         model_config: Optional model configuration dictionary from checkpoint
#         use_amp: Whether to use Automatic Mixed Precision (GPU only)

#     Returns:
#         Tuple containing:
#           - results: Dictionary mapping original sequence IDs (from FASTA) to predicted RMSF values (NumPy array)
#           - id_to_instance_key_map: Dictionary mapping original sequence IDs to the corresponding instance_key used for prediction/feature lookup.
#     """
#     model.eval()
#     if not sequences: return {}, {}

#     # Scale the target temperature ONCE
#     scaled_target_temp = temp_scaler(target_temperature)
#     logger.info(f"Predicting for raw temperature {target_temperature:.1f}K (scaled: {scaled_target_temp:.4f})")

#     # Prepare batches based on length
#     batches = group_sequences_by_length(sequences, batch_size)
#     results = {} # Keyed by original fasta ID
#     id_to_instance_key_map = {} # Map original fasta ID to instance_key used
#     prediction_start_time = time.time()
#     autocast_device_type = device.type
#     amp_enabled = (device.type == 'cuda' and use_amp)
#     # Import helper function defined in data_processor
#     from data_processor import create_instance_key, INSTANCE_KEY_SEPARATOR

#     # Check if model uses enhanced features
#     uses_features = model_config and model_config.get('model', {}).get('architecture', {}).get('use_enhanced_features', False)

#     with torch.no_grad():
#         for batch_data in tqdm(batches, desc=f"Predicting @ {target_temperature:.0f}K", leave=False):
#             # batch_ids are the original IDs from the FASTA headers
#             batch_ids = [item[0] for item in batch_data]
#             batch_seqs = [item[1] for item in batch_data]
#             batch_seq_lengths = [len(seq) for seq in batch_seqs] # Get sequence lengths
#             current_batch_size = len(batch_ids)

#             # Create tensor of the same scaled temperature for the whole batch
#             scaled_temps_batch = torch.tensor([scaled_target_temp] * current_batch_size,
#                                               device=device, dtype=torch.float32)

#             # --- Feature Preparation ---
#             batch_features_for_model = None
#             feature_lookup_keys_batch = [] # Keys used to look up features (guaranteed instance_key format)

#             if uses_features:
#                 if not feature_data:
#                      logger.warning("Model expects features, but no feature_data was provided. Predictions will be sequence-only.")
#                 else:
#                     # Determine the correct keys to use for feature lookup
#                     for fasta_id in batch_ids:
#                         # Check if the fasta_id already looks like an instance_key
#                         if INSTANCE_KEY_SEPARATOR in fasta_id:
#                             # Assume it's correct, maybe warn if temp doesn't match target?
#                             try:
#                                 _, temp_part = fasta_id.rsplit(INSTANCE_KEY_SEPARATOR, 1)
#                                 temp_in_key = float(temp_part)
#                                 if abs(temp_in_key - target_temperature) > 1: # Allow some tolerance (e.g., 450.0 vs 450)
#                                      logger.warning(f"FASTA ID '{fasta_id}' looks like an instance key, but its temperature ({temp_in_key:.1f}K) "
#                                                     f"differs significantly from the target prediction temperature ({target_temperature:.1f}K). "
#                                                     f"Using features associated with '{fasta_id}'.")
#                             except ValueError:
#                                  logger.warning(f"Could not parse temperature from FASTA ID '{fasta_id}'. Constructing key using target temperature.")
#                                  fasta_id_base = fasta_id # Use the whole thing if parsing fails
#                                  constructed_key = create_instance_key(fasta_id_base, target_temperature)
#                                  feature_lookup_keys_batch.append(constructed_key)
#                                  id_to_instance_key_map[fasta_id] = constructed_key
#                                  continue # Skip to next id

#                             # Use the original fasta_id as the lookup key
#                             feature_lookup_keys_batch.append(fasta_id)
#                             id_to_instance_key_map[fasta_id] = fasta_id # Map to itself

#                         else:
#                             # Construct the instance key using the target temperature
#                             constructed_key = create_instance_key(fasta_id, target_temperature)
#                             feature_lookup_keys_batch.append(constructed_key)
#                             id_to_instance_key_map[fasta_id] = constructed_key # Map original ID to constructed key

#                     # Prepare features using the determined lookup keys
#                     batch_features_for_model = prepare_prediction_features(
#                         feature_lookup_keys=feature_lookup_keys_batch,
#                         model_config=model_config,
#                         scaling_params=feature_norm_params,
#                         feature_data=feature_data,
#                         target_sequence_lengths=batch_seq_lengths # Pass sequence lengths
#                     )
#             else:
#                  # If not using features, still populate the map (mapping ID to itself, no temp needed)
#                  for fasta_id in batch_ids:
#                       id_to_instance_key_map[fasta_id] = fasta_id # Or maybe construct key anyway for consistency? Let's construct it.
#                       id_to_instance_key_map[fasta_id] = create_instance_key(fasta_id, target_temperature)


#             # --- Model Prediction ---
#             try:
#                 # Forward pass with optional AMP
#                 with torch.amp.autocast(device_type=autocast_device_type, enabled=amp_enabled):
#                     # Pass sequences, scaled temperatures, and prepared features
#                     batch_predictions_np = model.predict(
#                         sequences=batch_seqs,
#                         scaled_temperatures=scaled_temps_batch,
#                         features=batch_features_for_model # Pass the prepared features
#                     )

#                 # Store results using the *original* FASTA IDs as keys
#                 if len(batch_predictions_np) == len(batch_ids):
#                     for seq_id, pred_np in zip(batch_ids, batch_predictions_np):
#                         results[seq_id] = pred_np
#                 else:
#                     logger.error(f"Prediction output length mismatch: {len(batch_predictions_np)} preds vs {len(batch_ids)} IDs.")
#                     # Handle partial assignment or error as needed
#                     for seq_id in batch_ids:
#                          if seq_id not in results: results[seq_id] = np.array([]) # Ensure all original IDs have an entry

#             except Exception as e:
#                  logger.error(f"Error predicting batch starting with {batch_ids[0]}: {e}", exc_info=True)
#                  # Add placeholder or skip IDs in this batch
#                  for seq_id in batch_ids: results[seq_id] = np.array([]) # Example: empty array on error

#             # Optional: Periodic GPU cache clearing
#             if device.type == 'cuda' and len(results) % (10 * batch_size) == 0:
#                  torch.cuda.empty_cache()

#     prediction_duration = time.time() - prediction_start_time
#     num_predicted = sum(1 for r in results.values() if r.size > 0) # Count successful predictions
#     logger.info(f"Prediction completed for {num_predicted}/{len(sequences)} sequences in {prediction_duration:.2f}s.")
#     if num_predicted > 0: logger.info(f"Avg time per sequence: {prediction_duration / num_predicted:.4f}s")

#     return results, id_to_instance_key_map # Return both results and the map




# Replace the existing function with this one:
def predict_rmsf_at_temperature(
    model: EnhancedTemperatureAwareESMModel,
    sequences: Dict[str, str],
    target_temperature_override: Optional[float], # Changed: Can be None if using npy
    temperatures_from_npy: Optional[Dict[str, float]], # Added: Dict from _temperatures.npy
    temp_scaler: Callable[[float], float],
    batch_size: int,
    device: torch.device,
    feature_data: Optional[Dict[str, Dict[str, np.ndarray]]] = None,
    feature_norm_params: Optional[Dict[str, Dict[str, float]]] = None,
    model_config: Optional[Dict[str, Any]] = None,
    use_amp: bool = True,
    mc_dropout_samples: int = 0 # Added: Number of MC Dropout samples
) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, str]]: # Return results, uncertainties, id_map
    """
    Predict RMSF values and uncertainty for sequences.
    Uses temperatures from .npy file if provided, otherwise uses the single override.
    Handles different input ID formats. Estimates uncertainty via MC Dropout if mc_dropout_samples > 1.

    Args:
        model: The trained model.
        sequences: Dictionary mapping sequence IDs (from FASTA header) to sequences.
        target_temperature_override: Single temperature (raw) OR None if using npy file.
        temperatures_from_npy: Optional dict mapping instance_keys to raw temperatures.
        temp_scaler: The function to scale the raw temperature(s).
        batch_size: Batch size for inference.
        device: Device ('cuda' or 'cpu').
        feature_data: Optional dictionary of structural feature data (keyed by instance_key).
        feature_norm_params: Optional dictionary of feature normalization parameters.
        model_config: Optional model configuration dictionary from checkpoint.
        use_amp: Whether to use Automatic Mixed Precision (GPU only).
        mc_dropout_samples: Number of forward passes for MC Dropout uncertainty estimation. If <= 1, performs standard prediction.

    Returns:
        Tuple containing:
          - results: Dictionary mapping original sequence IDs (from FASTA) to predicted RMSF mean values (NumPy array).
          - uncertainties: Dictionary mapping original sequence IDs to predicted RMSF uncertainty (std dev) (NumPy array). Returns zero array if mc_dropout_samples <= 1.
          - id_to_instance_key_map: Dictionary mapping original sequence IDs to the corresponding instance_key used for prediction/feature lookup.
    """
    if not sequences: return {}, {}, {}

    if temperatures_from_npy is None and target_temperature_override is None:
        raise ValueError("Must provide either target_temperature_override or temperatures_from_npy.")
    if temperatures_from_npy is not None and target_temperature_override is not None:
        logger.warning("Both target_temperature_override and temperatures_from_npy provided. Using temperatures_from_npy.")
        target_temperature_override = None # Prioritize npy file

    # Determine if we are doing MC Dropout
    do_mc_dropout = mc_dropout_samples > 1
    if do_mc_dropout:
        logger.info(f"Performing prediction with MC Dropout using {mc_dropout_samples} samples.")
        # Activate dropout layers specifically
        model.eval() # Start in eval mode
        activate_mc_dropout(model) # Turn *only* dropout layers back to train mode
    else:
        logger.info("Performing standard prediction (MC Dropout disabled).")
        model.eval() # Ensure model is in standard eval mode

    # Scale the single override temperature ONCE if needed
    scaled_target_temp_override = None
    if target_temperature_override is not None:
        scaled_target_temp_override = temp_scaler(target_temperature_override)
        logger.info(f"Using OVERRIDE raw temperature {target_temperature_override:.1f}K (scaled: {scaled_target_temp_override:.4f}) for all sequences.")
    else:
        logger.info("Using instance-specific temperatures loaded from .npy file.")

    # Prepare batches based on length
    batches = group_sequences_by_length(sequences, batch_size)
    results_mean = {} # Keyed by original fasta ID -> Mean prediction
    results_uncertainty = {} # Keyed by original fasta ID -> Std Dev prediction
    id_to_instance_key_map = {} # Map original fasta ID to instance_key used
    prediction_start_time = time.time()
    autocast_device_type = device.type
    amp_enabled = (device.type == 'cuda' and use_amp)

    # Check if model uses enhanced features
    uses_features = model_config and model_config.get('model', {}).get('architecture', {}).get('use_enhanced_features', False)

    with torch.no_grad(): # Still disable gradient calculation overall
        for batch_data in tqdm(batches, desc=f"Predicting", leave=False):
            batch_ids = [item[0] for item in batch_data]
            batch_seqs = [item[1] for item in batch_data]
            batch_seq_lengths = [len(seq) for seq in batch_seqs] # Get sequence lengths
            current_batch_size = len(batch_ids)

            # --- Determine temperatures and instance keys for the batch ---
            # We do this once per batch, regardless of MC samples
            batch_raw_temps_list = []
            batch_scaled_temps_list = []
            batch_feature_lookup_keys = []
            valid_indices_in_batch = [] # Indices within the current batch that are valid

            for i, fasta_id in enumerate(batch_ids):
                raw_temp_for_instance = None
                instance_key_for_lookup = fasta_id # Default assumption

                if temperatures_from_npy is not None:
                    if fasta_id not in temperatures_from_npy:
                         logger.error(f"FASTA ID / Instance Key '{fasta_id}' not found in provided temperatures .npy file. Skipping sequence.")
                         results_mean[fasta_id] = np.array([], dtype=np.float32) # Add placeholder
                         results_uncertainty[fasta_id] = np.array([], dtype=np.float32)
                         id_to_instance_key_map[fasta_id] = fasta_id # Map anyway
                         continue # Skip processing this sequence in the batch
                    raw_temp_for_instance = temperatures_from_npy[fasta_id]
                    instance_key_for_lookup = fasta_id # Key is already correct

                elif target_temperature_override is not None:
                    raw_temp_for_instance = target_temperature_override
                    if INSTANCE_KEY_SEPARATOR not in fasta_id:
                        instance_key_for_lookup = create_instance_key(fasta_id, raw_temp_for_instance)
                    else: # Already looks like a key, check temp consistency
                         try:
                             _, temp_part = fasta_id.rsplit(INSTANCE_KEY_SEPARATOR, 1)
                             temp_in_key = float(temp_part)
                             if abs(temp_in_key - raw_temp_for_instance) > 1:
                                 logger.warning(f"FASTA ID '{fasta_id}' temp ({temp_in_key:.1f}K) differs from override ({raw_temp_for_instance:.1f}K). Using override.")
                         except ValueError: logger.warning(f"Could not parse temp from FASTA ID '{fasta_id}'.")
                         instance_key_for_lookup = fasta_id # Use original key anyway

                else: # Should not happen
                     logger.error(f"Internal logic error: No temperature source for {fasta_id}.")
                     results_mean[fasta_id] = np.array([], dtype=np.float32); results_uncertainty[fasta_id] = np.array([], dtype=np.float32); id_to_instance_key_map[fasta_id] = fasta_id
                     continue

                # Store valid data for this item
                batch_raw_temps_list.append(raw_temp_for_instance)
                batch_scaled_temps_list.append(temp_scaler(raw_temp_for_instance))
                batch_feature_lookup_keys.append(instance_key_for_lookup)
                id_to_instance_key_map[fasta_id] = instance_key_for_lookup # Store mapping
                valid_indices_in_batch.append(i) # Store the original index within this batch

            # Skip batch if no valid sequences remained
            if not valid_indices_in_batch: continue

            # Prepare tensors and sequences only for the valid items
            valid_batch_seqs = [batch_seqs[i] for i in valid_indices_in_batch]
            valid_batch_ids = [batch_ids[i] for i in valid_indices_in_batch]
            valid_batch_seq_lengths = [batch_seq_lengths[i] for i in valid_indices_in_batch]
            scaled_temps_tensor = torch.tensor(batch_scaled_temps_list, device=device, dtype=torch.float32)

            # --- Feature Preparation (for valid items) ---
            batch_features_for_model = None
            if uses_features:
                if not feature_data: logger.warning("Model expects features, but no feature_data was provided.")
                else:
                    batch_features_for_model = prepare_prediction_features(
                        feature_lookup_keys=batch_feature_lookup_keys, # Keys for valid items
                        model_config=model_config,
                        scaling_params=feature_norm_params,
                        feature_data=feature_data,
                        target_sequence_lengths=valid_batch_seq_lengths # Lengths for valid items
                    )

            # --- Model Prediction (potentially multiple passes for MC Dropout) ---
            all_mc_preds_np = [] # List to store predictions from each MC sample run

            num_passes = mc_dropout_samples if do_mc_dropout else 1
            for mc_run in range(num_passes):
                try:
                    with torch.amp.autocast(device_type=autocast_device_type, enabled=amp_enabled):
                        # We use model.forward directly here to get raw tensor outputs easily
                        # Ensure inputs match model.forward signature
                        outputs_dict = model.forward(
                            sequences=valid_batch_seqs,
                            temperatures=scaled_temps_tensor,
                            features=batch_features_for_model,
                            target_rmsf_values=None # No targets needed for prediction
                        )
                        # Extract list of prediction tensors
                        batch_predictions_tensors = outputs_dict['predictions']

                    # Convert current pass predictions to numpy and store
                    current_pass_preds_np = []
                    for pred_tensor in batch_predictions_tensors:
                         if pred_tensor is not None and pred_tensor.numel() > 0:
                              current_pass_preds_np.append(pred_tensor.cpu().numpy())
                         else:
                              current_pass_preds_np.append(np.array([], dtype=np.float32)) # Handle errors
                    all_mc_preds_np.append(current_pass_preds_np)

                except Exception as e:
                    mc_status = f"MC run {mc_run+1}/{num_passes}" if do_mc_dropout else "Standard prediction"
                    logger.error(f"Error during {mc_status} for batch starting with {valid_batch_ids[0]}: {e}", exc_info=True)
                    # If one MC pass fails, we likely can't continue for this batch
                    all_mc_preds_np = [] # Clear any partial results for this batch
                    for seq_id in valid_batch_ids: # Mark predictions as failed
                        results_mean[seq_id] = np.array([])
                        results_uncertainty[seq_id] = np.array([])
                    break # Break MC loop for this batch

            # --- Aggregate MC Dropout results ---
            if all_mc_preds_np: # Check if prediction runs were successful
                # Stack predictions along a new dimension (samples, batch_item, seq_len)
                # Need to handle potentially different lengths if sequences weren't bucketed perfectly
                # For simplicity, assume sequences in a batch have same length (due to bucketing) or handle padding/unpadding
                # Let's assume length consistency within MC samples for a given sequence

                # Iterate through each item in the batch
                for i, original_fasta_id in enumerate(valid_batch_ids):
                     preds_for_item = [mc_pass_results[i] for mc_pass_results in all_mc_preds_np if i < len(mc_pass_results) and mc_pass_results[i].size > 0]

                     if not preds_for_item: # Handle case where all MC passes failed for this item
                          results_mean[original_fasta_id] = np.array([])
                          results_uncertainty[original_fasta_id] = np.array([])
                          continue

                     # Stack the predictions for this item
                     try:
                          stacked_preds = np.stack(preds_for_item, axis=0) # Shape (num_samples, seq_len)
                          mean_pred = np.mean(stacked_preds, axis=0)
                          std_dev_pred = np.std(stacked_preds, axis=0) if do_mc_dropout else np.zeros_like(mean_pred) # Uncertainty is 0 if not MC dropout

                          results_mean[original_fasta_id] = mean_pred.astype(np.float32)
                          results_uncertainty[original_fasta_id] = std_dev_pred.astype(np.float32)
                     except ValueError as e: # Handle potential stacking errors if lengths mismatch unexpectedly
                          logger.error(f"Error stacking predictions for {original_fasta_id} (lengths={[p.shape for p in preds_for_item]}): {e}")
                          results_mean[original_fasta_id] = np.array([])
                          results_uncertainty[original_fasta_id] = np.array([])


            # Optional: Periodic GPU cache clearing
            if device.type == 'cuda' and len(results_mean) % (10 * batch_size) == 0:
                 torch.cuda.empty_cache()

    # Ensure model is back in standard eval mode if MC Dropout was used
    if do_mc_dropout: model.eval()

    prediction_duration = time.time() - prediction_start_time
    num_predicted = sum(1 for r in results_mean.values() if r.size > 0) # Count successful predictions
    logger.info(f"Prediction completed for {num_predicted}/{len(sequences)} sequences in {prediction_duration:.2f}s.")
    if num_predicted > 0: logger.info(f"Avg time per sequence: {prediction_duration / num_predicted:.4f}s")

    return results_mean, results_uncertainty, id_to_instance_key_map # Return all dicts

def plot_rmsf(
    sequence: str,
    predictions: np.ndarray,
    title: str,
    output_path: str,
    window_size: int = 1,
    figsize: Tuple[int, int] = (15, 6)
):
    """Plot predicted RMSF values against residue position."""
    
    return 
    # if predictions is None or len(predictions) == 0:
    #     logger.warning(f"No prediction data to plot for '{title}'. Skipping plot.")
    #     return

    # plt.style.use('seaborn-v0_8-whitegrid')
    # fig, ax = plt.subplots(figsize=figsize)

    # pred_len = len(predictions)
    # residue_indices = np.arange(1, pred_len + 1)

    # if window_size > 1:
    #     s = pd.Series(predictions)
    #     plot_data = s.rolling(window=window_size, center=True, min_periods=1).mean().to_numpy()
    #     plot_label = f'RMSF Prediction (Smoothed, win={window_size})'
    # else:
    #     plot_data = predictions
    #     plot_label = 'RMSF Prediction'

    # ax.plot(residue_indices, plot_data, '-', color='dodgerblue', linewidth=1.5, label=plot_label)

    # ax.set_xlabel('Residue Position')
    # ax.set_ylabel('Predicted RMSF')
    # ax.set_title(f'Predicted RMSF for {title} (Length: {pred_len})') # Title now includes Temp
    # ax.set_xlim(0, pred_len + 1)
    # ax.grid(True, linestyle=':', alpha=0.7)

    # # Add stats text box
    # mean_rmsf = np.mean(predictions)
    # median_rmsf = np.median(predictions)
    # stats_text = (f'Mean: {mean_rmsf:.3f}\n'
    #               f'Median: {median_rmsf:.3f}')
    # ax.text(0.02, 0.95, stats_text, transform=ax.transAxes, fontsize=9,
    #         verticalalignment='top', bbox=dict(boxstyle='round,pad=0.4', fc='wheat', alpha=0.5))

    # ax.legend(loc='upper right')
    # plt.tight_layout()

    # try:
    #     os.makedirs(os.path.dirname(output_path), exist_ok=True)
    #     plt.savefig(output_path, dpi=100, bbox_inches='tight') # Lower DPI for potentially many plots
    # except Exception as e:
    #     logger.error(f"Failed to save plot to {output_path}: {e}")
    # finally:
    #     plt.close(fig)


def save_predictions(
    predictions: Dict[str, np.ndarray],
    uncertainties: Dict[str, np.ndarray], # Added uncertainties
    id_to_instance_key_map: Dict[str, str],
    output_path: str,
    # target_temperature: float # Removed - temp info is in instance_key now
    ):
    """Save predictions and uncertainties to a CSV file, using instance_key."""
    if not predictions:
        logger.warning("No predictions provided to save.")
        return

    data_to_save = []
    saved_count = 0
    missing_key_map_count = 0
    missing_uncertainty_count = 0

    # Iterate through predictions dictionary (keyed by original fasta ID)
    for fasta_id, rmsf_values in predictions.items():
        if rmsf_values is None or len(rmsf_values) == 0:
             logger.debug(f"Skipping save for '{fasta_id}' due to empty prediction.")
             continue

        # Get the corresponding instance_key from the map
        instance_key = id_to_instance_key_map.get(fasta_id)
        if instance_key is None:
             logger.warning(f"Could not find instance_key mapping for FASTA ID '{fasta_id}'. Skipping save for this sequence.")
             missing_key_map_count += 1
             continue

        # Get the corresponding uncertainty array
        uncertainty_values = uncertainties.get(fasta_id)
        if uncertainty_values is None or len(uncertainty_values) != len(rmsf_values):
            logger.warning(f"Uncertainty data missing or length mismatch for '{fasta_id}'. Saving uncertainty as NaN.")
            missing_uncertainty_count += 1
            uncertainty_values = np.full_like(rmsf_values, np.nan) # Fill with NaN

        # Get raw temperature from instance key for reporting (optional, but good)
        try:
            raw_temp = float(instance_key.split(INSTANCE_KEY_SEPARATOR)[-1])
        except:
            raw_temp = np.nan # Fallback if key format is weird

        # Append data for each residue, using the instance_key
        for i, rmsf in enumerate(rmsf_values):
            uncertainty = uncertainty_values[i] if i < len(uncertainty_values) else np.nan
            data_to_save.append({
                # Use the instance_key for the 'domain_id' column for compatibility downstream
                'instance_key': instance_key, # Changed column name for clarity
                'resid': i + 1,
                'rmsf_pred': rmsf,
                'uncertainty': uncertainty, # Added uncertainty column
                'temperature': raw_temp # Added original temperature column
            })
        saved_count += 1

    if missing_key_map_count > 0:
         logger.error(f"Failed to find instance_key mapping for {missing_key_map_count} FASTA IDs during saving.")
    if missing_uncertainty_count > 0:
         logger.warning(f"Uncertainty missing or mismatched for {missing_uncertainty_count} sequences.")

    if not data_to_save:
        logger.warning("No valid prediction data points found to save in CSV.")
        return

    try:
        df = pd.DataFrame(data_to_save)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # Format floats nicely
        df.to_csv(output_path, index=False, float_format='%.6f')
        logger.info(f"Predictions for {saved_count} sequences saved to {output_path}")
    except Exception as e:
        logger.error(f"Failed to save predictions DataFrame to {output_path}: {e}")
        
# def save_predictions(
#     predictions: Dict[str, np.ndarray],
#     id_to_instance_key_map: Dict[str, str], # Added map parameter
#     output_path: str,
#     target_temperature: float):
#     """Save predictions to a CSV file, ensuring instance_key is used."""
#     if not predictions:
#         logger.warning("No predictions provided to save.")
#         return

#     data_to_save = []
#     saved_count = 0
#     missing_key_map_count = 0

#     # Iterate through predictions dictionary (keyed by original fasta ID)
#     for fasta_id, rmsf_values in predictions.items():
#         if rmsf_values is None or len(rmsf_values) == 0:
#              logger.debug(f"Skipping save for '{fasta_id}' due to empty prediction.")
#              continue

#         # Get the corresponding instance_key from the map
#         instance_key = id_to_instance_key_map.get(fasta_id)
#         if instance_key is None:
#              logger.warning(f"Could not find instance_key mapping for FASTA ID '{fasta_id}'. Skipping save for this sequence.")
#              missing_key_map_count += 1
#              continue

#         # Append data for each residue, using the instance_key
#         for i, rmsf in enumerate(rmsf_values):
#             data_to_save.append({
#                 # Use the instance_key for the 'domain_id' column for compatibility downstream
#                 'domain_id': instance_key,
#                 'resid': i + 1,
#                 'rmsf_pred': rmsf,
#                 'predicted_at_temp': target_temperature
#             })
#         saved_count += 1

#     if missing_key_map_count > 0:
#          logger.error(f"Failed to find instance_key mapping for {missing_key_map_count} FASTA IDs during saving.")

#     if not data_to_save:
#         logger.warning("No valid prediction data points found to save in CSV.")
#         return

#     try:
#         df = pd.DataFrame(data_to_save)
#         os.makedirs(os.path.dirname(output_path), exist_ok=True)
#         df.to_csv(output_path, index=False, float_format='%.6f')
#         logger.info(f"Predictions for {saved_count} sequences (T={target_temperature:.0f}K) saved to {output_path}")
#     except Exception as e:
#         logger.error(f"Failed to save predictions DataFrame to {output_path}: {e}")
        
# def predict(config: Dict[str, Any]):
#     """Main prediction function."""
#     predict_start_time = time.time()
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     logger.info(f"Using device: {device}")
#     if device.type == 'cuda': log_gpu_memory()

#     # --- Get Required Config ---
#     model_checkpoint = config.get('model_checkpoint')
#     fasta_path = config.get('fasta_path')
#     output_dir = config.get('output_dir', 'predictions')
#     target_temperature = config.get('temperature') # Raw temperature

#     if not model_checkpoint or not fasta_path or target_temperature is None:
#         logger.critical("Missing required config: 'model_checkpoint', 'fasta_path', or 'temperature'.")
#         return

#     # --- Output Dir & Logging ---
#     # Include temperature in output subdir for organization
#     try:
#          temp_str = f"{target_temperature:.0f}K"
#     except TypeError: # Handle if temperature is somehow not a number
#          logger.error(f"Invalid target temperature provided: {target_temperature}. Must be numeric.")
#          return
#     output_dir_temp = os.path.join(output_dir, temp_str)
#     os.makedirs(output_dir_temp, exist_ok=True)

#     log_path = os.path.join(output_dir_temp, 'prediction.log')
#     # Remove existing handlers to avoid duplicate logging if run multiple times
#     for handler in logger.handlers[:]:
#         if isinstance(handler, logging.FileHandler) and handler.baseFilename == log_path:
#             logger.removeHandler(handler)
#             handler.close()
#     # Add new file handler for this run
#     file_handler = logging.FileHandler(log_path, mode='w')
#     file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s [%(filename)s:%(lineno)d] - %(message)s'))
#     logger.addHandler(file_handler)
#     logger.info(f"--- Starting Prediction Run for T={target_temperature:.1f}K ---")
#     logger.info(f"Prediction Config: {json.dumps(config, indent=2)}")
#     logger.info(f"Saving results to: {output_dir_temp}")

#     # --- Load Model ---
#     model, model_config_from_ckpt = load_model_for_prediction(model_checkpoint, device)
#     if model is None:
#         logger.error("Failed to load model. Aborting prediction.")
#         logger.removeHandler(file_handler); file_handler.close()
#         return
#     if device.type == 'cuda': log_gpu_memory()

#     # --- Load Temperature Scaler ---
#     checkpoint_dir = os.path.dirname(model_checkpoint)
#     # Try finding scaling/norm params relative to checkpoint first, then from config model_dir
#     potential_param_dirs = [checkpoint_dir]
#     config_model_dir = model_config_from_ckpt.get('output',{}).get('model_dir')
#     if config_model_dir and os.path.abspath(config_model_dir) != os.path.abspath(checkpoint_dir):
#          potential_param_dirs.append(config_model_dir)

#     temp_scaler = None
#     scaling_filename = model_config_from_ckpt.get('data', {}).get('temp_scaling_filename', 'temp_scaling_params.json')
#     for p_dir in potential_param_dirs:
#         temp_scaling_path = os.path.join(p_dir, scaling_filename)
#         if os.path.exists(temp_scaling_path):
#             logger.info(f"Found temperature scaling file: {temp_scaling_path}")
#             try:
#                 temp_scaler = get_temperature_scaler(temp_scaling_path)
#                 break # Found and loaded successfully
#             except Exception as e:
#                  logger.error(f"Failed to load temperature scaler from {temp_scaling_path}: {e}. Aborting.")
#                  logger.removeHandler(file_handler); file_handler.close()
#                  return
#     if temp_scaler is None:
#          logger.error(f"Temperature scaling file '{scaling_filename}' not found in potential directories: {potential_param_dirs}. Aborting.")
#          logger.removeHandler(file_handler); file_handler.close()
#          return

#     # --- Load Feature Normalization Parameters ---
#     feature_norm_params = None
#     if model_config_from_ckpt.get('model', {}).get('architecture', {}).get('use_enhanced_features', True):
#         norm_params_filename = model_config_from_ckpt.get('data', {}).get('features', {}).get('normalization_params_file', 'feature_normalization.json')
#         found_norm_params = False
#         for p_dir in potential_param_dirs:
#             norm_params_path = os.path.join(p_dir, norm_params_filename)
#             if os.path.exists(norm_params_path):
#                 logger.info(f"Found feature normalization file: {norm_params_path}")
#                 try:
#                     feature_norm_params = load_feature_norm_params(norm_params_path)
#                     if feature_norm_params:
#                          logger.info(f"Loaded normalization parameters for {len(feature_norm_params)} features")
#                          found_norm_params = True
#                          break # Found and loaded
#                     else:
#                          logger.warning(f"Loaded feature normalization file {norm_params_path}, but it was empty.")
#                 except Exception as e:
#                     logger.warning(f"Error loading feature normalization parameters from {norm_params_path}: {e}. Continuing without normalization.")
#         if not found_norm_params:
#              logger.warning(f"Feature normalization file '{norm_params_filename}' not found or empty in potential directories: {potential_param_dirs}. Feature normalization will not be applied.")

#     # --- Load Sequences ---
#     try:
#         sequences = load_sequences_from_fasta(fasta_path)
#         if not sequences: raise ValueError("No sequences found in FASTA file.")
#         logger.info(f"Loaded {len(sequences)} sequences from {fasta_path}")
#     except Exception as e:
#          logger.critical(f"Error loading sequences from {fasta_path}: {e}", exc_info=True)
#          logger.removeHandler(file_handler); file_handler.close()
#          return

#     # --- Filter Sequences by Max Length (Optional) ---
#     max_length = config.get('max_length')
#     if max_length is not None and max_length > 0:
#         original_count = len(sequences)
#         sequences = {sid: seq for sid, seq in sequences.items() if len(seq) <= max_length}
#         filtered_count = len(sequences)
#         if filtered_count < original_count:
#             logger.info(f"Filtered out {original_count - filtered_count} sequences longer than {max_length}.")
#         if not sequences:
#             logger.critical(f"No sequences remaining after filtering by max_length={max_length}. Aborting.")
#             logger.removeHandler(file_handler); file_handler.close()
#             return

#     # --- Load Structural Features if model uses them ---
#     feature_data = None
#     if model_config_from_ckpt.get('model', {}).get('architecture', {}).get('use_enhanced_features', True):
#         logger.info("Model uses enhanced features. Attempting to load feature data...")

#         # Features should have been saved during processing with 'predict_' prefix
#         # We need the same list of features the model expects
#         expected_features = []
#         feature_config = model_config_from_ckpt.get('data', {}).get('features', {})
#         if feature_config.get('use_position_info', True): expected_features.append('normalized_resid')
#         if feature_config.get('use_structure_info', True): expected_features.extend(['core_exterior_encoded', 'secondary_structure_encoded'])
#         if feature_config.get('use_accessibility', True): expected_features.append('relative_accessibility')
#         if feature_config.get('use_backbone_angles', True): expected_features.extend(['phi_norm', 'psi_norm'])
#         if feature_config.get('use_protein_size', True): expected_features.append('protein_size')
#         if feature_config.get('use_voxel_rmsf', True): expected_features.append('voxel_rmsf')
#         if feature_config.get('use_bfactor', True): expected_features.append('bfactor_norm')

#         # Load features if expected. Assume they are in the same dir as the FASTA by default.
#         # Or should we load from data/processed? Assume same dir as fasta for now.
#         # --> Using data_dir from config might be more robust if 'concatenate_fastas' was used.
#         # Let's use the directory containing the FASTA file.
#         fasta_dir = os.path.dirname(fasta_path)
#         logger.info(f"Looking for feature .npy files (e.g., predict_*.npy) in: {fasta_dir}")

#         # We need all instance keys to potentially load features for them
#         # Construct potential instance keys from the input fasta sequences
#         from data_processor import create_instance_key, INSTANCE_KEY_SEPARATOR
#         potential_instance_keys = []
#         for fasta_id in sequences.keys():
#              if INSTANCE_KEY_SEPARATOR in fasta_id:
#                   potential_instance_keys.append(fasta_id)
#              else:
#                   potential_instance_keys.append(create_instance_key(fasta_id, target_temperature))


#         if expected_features:
#             # This function needs modification if predict_*.npy files don't exist.
#             # Let's simplify: Assume predict_*.npy files were created by concatenate_fastas
#             # and are in data_dir (data/processed by default)
#             processed_data_dir = model_config_from_ckpt.get('data', {}).get('data_dir', 'data/processed')
#             logger.info(f"Loading pre-concatenated features (predict_*.npy) from: {processed_data_dir}")
#             feature_data = load_feature_data(expected_features, processed_data_dir, potential_instance_keys)

#             if feature_data:
#                  loaded_feature_count = sum(len(f_dict) > 0 for f_dict in feature_data.values())
#                  logger.info(f"Loaded feature data for {loaded_feature_count} potential instance keys.")
#                  if loaded_feature_count == 0:
#                       logger.warning("No feature data loaded, although features are expected. Check predict_*.npy files.")
#                       feature_data = None # Treat as if no data was loaded
#             else:
#                 logger.warning(f"Failed to load any feature data from {processed_data_dir}. Check predict_*.npy files.")
#         else:
#              logger.info("No specific features expected by model config. Skipping feature loading.")

#     # --- Predict RMSF ---
#     # Returns dict keyed by fasta_id, and a map from fasta_id to instance_key
#     predictions, id_to_instance_key_map = predict_rmsf_at_temperature(
#         model=model,
#         sequences=sequences,
#         target_temperature=target_temperature,
#         temp_scaler=temp_scaler,
#         batch_size=config.get('batch_size', 8),
#         device=device,
#         feature_data=feature_data, # Pass loaded features
#         feature_norm_params=feature_norm_params, # Pass norm params
#         model_config=model_config_from_ckpt, # Pass model config
#         use_amp=(device.type == 'cuda')
#     )

#     # --- Save & Plot Results ---
#     if predictions:
#          output_csv_path = os.path.join(output_dir_temp, f'predictions_{temp_str}.csv')
#          # Pass the map to save_predictions
#          save_predictions(
#              predictions=predictions,
#              id_to_instance_key_map=id_to_instance_key_map,
#              output_path=output_csv_path,
#              target_temperature=target_temperature
#          )

#          if config.get('plot_predictions', True):
#              plots_dir = os.path.join(output_dir_temp, 'plots')
#              os.makedirs(plots_dir, exist_ok=True)
#              smoothing = config.get('smoothing_window', 1)
#              logger.info(f"Generating plots (smoothing={smoothing})...")
#              plot_count = 0
#              # Iterate using original fasta IDs
#              for fasta_id, pred_array in tqdm(predictions.items(), desc="Plotting", leave=False):
#                   if fasta_id in sequences and pred_array.size > 0 :
#                       try:
#                           # Add temperature to plot title
#                           plot_title = f"{fasta_id} @ {target_temperature:.0f}K"
#                           # Sanitize fasta_id for filename
#                           safe_fasta_id = "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in fasta_id)
#                           plot_filename = f'{safe_fasta_id}_{temp_str}.png'

#                           plot_rmsf(
#                               sequence=sequences[fasta_id],
#                               predictions=pred_array,
#                               title=plot_title,
#                               output_path=os.path.join(plots_dir, plot_filename),
#                               window_size=smoothing
#                           )
#                           plot_count += 1
#                       except Exception as e:
#                           logger.error(f"Failed to generate plot for {fasta_id}: {e}")
#                   elif pred_array.size == 0:
#                        logger.debug(f"Skipping plot for {fasta_id} - no prediction data.")
#                   # else: logger.warning(f"Cannot plot for {fasta_id}: Original sequence not found.") # Should not happen if keys match
#              logger.info(f"Generated {plot_count} plots.")
#     else:
#          logger.warning("Prediction resulted in no output.")

#     # --- Finalize ---
#     predict_end_time = time.time()
#     logger.info(f"--- Prediction Run Finished (T={target_temperature:.1f}K) ---")
#     logger.info(f"Total prediction time: {predict_end_time - predict_start_time:.2f} seconds.")
#     logger.info(f"Results saved in: {output_dir_temp}")
#     # Remove the file handler specific to this run
#     logger.removeHandler(file_handler)
#     file_handler.close()




# Replace the existing function with this one:
def predict(config: Dict[str, Any]):
    """Main prediction function."""
    predict_start_time = time.time()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    if device.type == 'cuda': log_gpu_memory()

    # --- Get Required Config ---
    model_checkpoint = config.get('model_checkpoint')
    fasta_path = config.get('fasta_path')
    output_dir = config.get('output_dir', 'predictions')
    target_temperature_override = config.get('temperature') # May be None if using npy
    temperature_npy_path = config.get('temperature_npy') # New optional arg
    mc_samples = config.get('mc_samples', 0) # New uncertainty arg

    # --- Input Validation ---
    if not model_checkpoint or not os.path.exists(model_checkpoint):
        logger.critical(f"Model checkpoint not found or not specified: {model_checkpoint}")
        return
    if not fasta_path or not os.path.exists(fasta_path):
        logger.critical(f"Input FASTA file not found or not specified: {fasta_path}")
        return
    if target_temperature_override is None and (temperature_npy_path is None or not os.path.exists(temperature_npy_path)):
         logger.critical("Must provide either --temperature OR a valid path via --temperature_npy.")
         return
    if target_temperature_override is not None and temperature_npy_path is not None:
         logger.warning("Both --temperature and --temperature_npy provided. Prioritizing --temperature_npy.")
         target_temperature_override = None # Disable override

    # --- Output Dir & Logging ---
    run_name = f"prediction_run_{time.strftime('%Y%m%d_%H%M%S')}"
    if target_temperature_override is not None:
         try: run_name = f"prediction_{target_temperature_override:.0f}K"
         except TypeError: pass
    elif temperature_npy_path is not None:
         run_name = f"prediction_from_npy_{os.path.splitext(os.path.basename(temperature_npy_path))[0]}"
    if mc_samples > 1: run_name += f"_mc{mc_samples}" # Add MC suffix

    output_dir_run = os.path.join(output_dir, run_name)
    os.makedirs(output_dir_run, exist_ok=True)

    log_path = os.path.join(output_dir_run, 'prediction.log')
    for handler in logger.handlers[:]:
        if isinstance(handler, logging.FileHandler) and handler.baseFilename == log_path:
            logger.removeHandler(handler); handler.close()
    file_handler = logging.FileHandler(log_path, mode='w')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s [%(filename)s:%(lineno)d] - %(message)s'))
    logger.addHandler(file_handler)
    logger.info(f"--- Starting Prediction Run ---")
    logger.info(f"Using FASTA: {fasta_path}")
    if temperature_npy_path: logger.info(f"Using Temperatures from: {temperature_npy_path}")
    if target_temperature_override: logger.info(f"Using Override Temperature: {target_temperature_override:.1f}K")
    if mc_samples > 1: logger.info(f"Using MC Dropout Samples: {mc_samples}")
    logger.info(f"Prediction Config (Args): {json.dumps(config, indent=2)}")
    logger.info(f"Saving results to: {output_dir_run}")

    # --- Load Model ---
    model, model_config_from_ckpt = load_model_for_prediction(model_checkpoint, device)
    if model is None: logger.error("Failed to load model."); logger.removeHandler(file_handler); file_handler.close(); return
    if device.type == 'cuda': log_gpu_memory()

    # --- Load Temperature Scaler (MUST use scaler from original training) ---
    checkpoint_dir = os.path.dirname(model_checkpoint)
    potential_param_dirs = [checkpoint_dir]
    config_model_dir = model_config_from_ckpt.get('output',{}).get('model_dir')
    if config_model_dir and os.path.abspath(config_model_dir) != os.path.abspath(checkpoint_dir):
         potential_param_dirs.append(config_model_dir)
    temp_scaler = None
    scaling_filename = model_config_from_ckpt.get('data', {}).get('temp_scaling_filename', 'temp_scaling_params.json')
    logger.info(f"Attempting to load temperature scaler params '{scaling_filename}' used during training...")
    for p_dir in potential_param_dirs:
        temp_scaling_path = os.path.join(p_dir, scaling_filename)
        if os.path.exists(temp_scaling_path):
            logger.info(f"Found temperature scaling file: {temp_scaling_path}")
            try: temp_scaler = get_temperature_scaler(temp_scaling_path); break
            except Exception as e: logger.error(f"Failed to load temp scaler from {temp_scaling_path}: {e}. Aborting."); logger.removeHandler(file_handler); file_handler.close(); return
    if temp_scaler is None: logger.error(f"Temp scaling file '{scaling_filename}' not found in {potential_param_dirs}. Aborting."); logger.removeHandler(file_handler); file_handler.close(); return

    # --- Load Feature Normalization Parameters (MUST use params from original training) ---
    feature_norm_params = None
    if model_config_from_ckpt.get('model', {}).get('architecture', {}).get('use_enhanced_features', True):
        norm_params_filename = model_config_from_ckpt.get('data', {}).get('features', {}).get('normalization_params_file', 'feature_normalization.json')
        logger.info(f"Attempting to load feature norm params '{norm_params_filename}' used during training...")
        found_norm_params = False
        for p_dir in potential_param_dirs:
            norm_params_path = os.path.join(p_dir, norm_params_filename)
            if os.path.exists(norm_params_path):
                logger.info(f"Found feature normalization file: {norm_params_path}")
                try:
                    feature_norm_params = load_feature_norm_params(norm_params_path)
                    if feature_norm_params: logger.info(f"Loaded normalization params for {len(feature_norm_params)} features"); found_norm_params = True; break
                    else: logger.warning(f"Loaded feature normalization file {norm_params_path}, but it was empty.")
                except Exception as e: logger.warning(f"Error loading feature normalization parameters from {norm_params_path}: {e}. Continuing without.")
        if not found_norm_params: logger.warning(f"Feature norm file '{norm_params_filename}' not found or empty in {potential_param_dirs}. Features will not be normalized.")

    # --- Load Sequences ---
    try:
        sequences = load_sequences_from_fasta(fasta_path)
        if not sequences: raise ValueError("No sequences found in FASTA file.")
        logger.info(f"Loaded {len(sequences)} sequences from {fasta_path}")
    except Exception as e: logger.critical(f"Error loading sequences from {fasta_path}: {e}", exc_info=True); logger.removeHandler(file_handler); file_handler.close(); return

    # --- Load Temperatures from NPY if path provided ---
    temperatures_from_npy_dict = None
    if temperature_npy_path:
        try:
            temperatures_from_npy_dict = load_numpy_dict(temperature_npy_path)
            logger.info(f"Loaded {len(temperatures_from_npy_dict)} temperatures from {temperature_npy_path}")
        except Exception as e: logger.error(f"Failed to load temperatures from {temperature_npy_path}: {e}. Aborting."); logger.removeHandler(file_handler); file_handler.close(); return

    # --- Filter Sequences by Max Length (Optional) ---
    max_length = config.get('max_length')
    if max_length is not None and max_length > 0:
        original_count = len(sequences)
        sequences = {sid: seq for sid, seq in sequences.items() if len(seq) <= max_length}
        filtered_count = len(sequences)
        if filtered_count < original_count: logger.info(f"Filtered out {original_count - filtered_count} sequences longer than {max_length}.")
        if not sequences: logger.critical(f"No sequences remaining after filtering by max_length={max_length}. Aborting."); logger.removeHandler(file_handler); file_handler.close(); return

    # --- Load Structural Features if model uses them ---
    feature_data = None
    if model_config_from_ckpt.get('model', {}).get('architecture', {}).get('use_enhanced_features', True):
        logger.info("Model uses enhanced features. Attempting to load feature data...")
        expected_features = []
        feature_config = model_config_from_ckpt.get('data', {}).get('features', {})
        if feature_config.get('use_position_info', True): expected_features.append('normalized_resid')
        if feature_config.get('use_structure_info', True): expected_features.extend(['core_exterior_encoded', 'secondary_structure_encoded'])
        if feature_config.get('use_accessibility', True): expected_features.append('relative_accessibility')
        if feature_config.get('use_backbone_angles', True): expected_features.extend(['phi_norm', 'psi_norm'])
        if feature_config.get('use_protein_size', True): expected_features.append('protein_size')
        if feature_config.get('use_voxel_rmsf', True): expected_features.append('voxel_rmsf')
        if feature_config.get('use_bfactor', True): expected_features.append('bfactor_norm')

        # Determine feature file prefix based on fasta filename
        fasta_filename = os.path.basename(fasta_path)
        match = re.match(r"^(train|val|test|predict|holdout)_sequences\.fasta$", fasta_filename)
        feature_file_prefix = match.group(1) if match else "predict"
        feature_data_dir = os.path.dirname(fasta_path)
        logger.info(f"Inferred feature prefix '{feature_file_prefix}' from FASTA.")
        logger.info(f"Looking for feature files ({feature_file_prefix}_*.npy) in: {feature_data_dir}")

        # Construct potential lookup keys
        potential_lookup_keys = []
        for fasta_id in sequences.keys():
             if temperatures_from_npy_dict is not None:
                  potential_lookup_keys.append(fasta_id) # FASTA ID must be instance key
             elif target_temperature_override is not None:
                   potential_lookup_keys.append(create_instance_key(fasta_id, target_temperature_override) if INSTANCE_KEY_SEPARATOR not in fasta_id else fasta_id)

        if expected_features:
            feature_data = load_feature_data(expected_features, feature_data_dir, potential_lookup_keys, file_prefix=feature_file_prefix)
            if feature_data:
                 loaded_feature_count = sum(len(f_dict) > 0 for f_dict in feature_data.values())
                 logger.info(f"Loaded feature data for {loaded_feature_count} potential instance keys.")
                 if loaded_feature_count == 0: logger.warning(f"No feature data loaded using prefix '{feature_file_prefix}'."); feature_data = None
            else: logger.warning(f"Failed to load any feature data from {feature_data_dir} with prefix '{feature_file_prefix}'.")
        else: logger.info("No specific features expected. Skipping feature loading.")

    # --- Predict RMSF ---
    # Returns dicts keyed by fasta_id: mean_preds, uncertainties, and map from fasta_id to instance_key
    predictions, uncertainties, id_to_instance_key_map = predict_rmsf_at_temperature(
        model=model,
        sequences=sequences,
        target_temperature_override=target_temperature_override,
        temperatures_from_npy=temperatures_from_npy_dict,
        temp_scaler=temp_scaler,
        batch_size=config.get('batch_size', 8),
        device=device,
        feature_data=feature_data,
        feature_norm_params=feature_norm_params,
        model_config=model_config_from_ckpt,
        use_amp=(device.type == 'cuda'),
        mc_dropout_samples=mc_samples # Pass MC samples arg
    )

    # --- Save & Plot Results ---
    if predictions:
         output_csv_path = os.path.join(output_dir_run, f'predictions_{run_name}.csv')
         # Pass uncertainties to save_predictions
         save_predictions(
             predictions=predictions,
             uncertainties=uncertainties, # Pass uncertainty dict
             id_to_instance_key_map=id_to_instance_key_map,
             output_path=output_csv_path
             # No need to pass single temp anymore
         )

         if config.get('plot_predictions', True):
             plots_dir = os.path.join(output_dir_run, 'plots')
             os.makedirs(plots_dir, exist_ok=True)
             smoothing = config.get('smoothing_window', 1)
             logger.info(f"Generating plots (smoothing={smoothing})...")
             plot_count = 0
             for fasta_id, pred_array in tqdm(predictions.items(), desc="Plotting", leave=False):
                  if fasta_id in sequences and pred_array.size > 0 :
                      try:
                          instance_key = id_to_instance_key_map.get(fasta_id)
                          if instance_key and temperatures_from_npy_dict:
                               plot_temp = temperatures_from_npy_dict.get(instance_key, target_temperature_override if target_temperature_override else 0)
                          elif target_temperature_override is not None: plot_temp = target_temperature_override
                          else: plot_temp = 0

                          plot_title = f"{fasta_id} @ {plot_temp:.0f}K"
                          safe_fasta_id = "".join(c if c.isalnum() or c in ('-', '_', '@', '.') else '_' for c in fasta_id)
                          plot_filename = f'{safe_fasta_id}_T{plot_temp:.0f}.png'

                          plot_rmsf(
                              sequence=sequences[fasta_id],
                              predictions=pred_array,
                              title=plot_title,
                              output_path=os.path.join(plots_dir, plot_filename),
                              window_size=smoothing
                              # TODO: Optionally modify plot_rmsf to show uncertainty bands?
                          )
                          plot_count += 1
                      except Exception as e: logger.error(f"Failed to generate plot for {fasta_id}: {e}")
                  elif pred_array.size == 0: logger.debug(f"Skipping plot for {fasta_id} - no prediction data.")
             logger.info(f"Generated {plot_count} plots.")
    else: logger.warning("Prediction resulted in no output.")

    # --- Finalize ---
    predict_end_time = time.time()
    logger.info(f"--- Prediction Run Finished ---")
    logger.info(f"Total prediction time: {predict_end_time - predict_start_time:.2f} seconds.")
    logger.info(f"Results saved in: {output_dir_run}")
    logger.removeHandler(file_handler)
    file_handler.close()
    
    

# --- Also need to update the command-line parser in predict.py ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predict RMSF using a trained enhanced model, optionally using per-instance temperatures and MC Dropout uncertainty.')
    parser.add_argument('--model_checkpoint', type=str, required=True, help='Path to the trained model checkpoint (.pt file)')
    parser.add_argument('--fasta_path', type=str, required=True, help='Path to the input FASTA file (headers must be instance_keys if using --temperature_npy)')
    # Make temperature optional, add npy path
    parser.add_argument('--temperature', type=float, default=None, help='(Optional) Target temperature (Kelvin) to use for ALL sequences if --temperature_npy is not provided.')
    parser.add_argument('--temperature_npy', type=str, default=None, help='(Optional) Path to .npy file mapping instance_keys (from FASTA) to RAW temperatures. Overrides --temperature.')
    parser.add_argument('--mc_samples', type=int, default=0, help='Number of Monte Carlo Dropout samples for uncertainty estimation (e.g., 10-50). Default 0 disables MC Dropout.')
    # --- End changes ---
    parser.add_argument('--output_dir', type=str, default='predictions', help='Base directory to save prediction results')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for prediction')
    parser.add_argument('--max_length', type=int, default=None, help='Optional: Max sequence length filter')
    parser.add_argument('--plot_predictions', action=argparse.BooleanOptionalAction, default=True, help='Generate plots')
    parser.add_argument('--smoothing_window', type=int, default=1, help='Smoothing window for plots (1=none)')

    # --- Placeholder for direct script execution (for testing) ---
    # Remove or adapt this if predict() is only called via main.py
    args = parser.parse_args()
    config_dict = vars(args)
    predict(config_dict)

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description='Predict RMSF using a trained enhanced model')
#     parser.add_argument('--model_checkpoint', type=str, required=True, help='Path to the trained model checkpoint (.pt file)')
#     parser.add_argument('--fasta_path', type=str, required=True, help='Path to the input FASTA file')
#     parser.add_argument('--temperature', type=float, required=True, help='Target temperature (in Kelvin) for prediction')
#     parser.add_argument('--output_dir', type=str, default='predictions', help='Base directory to save prediction results')
#     parser.add_argument('--batch_size', type=int, default=8, help='Batch size for prediction')
#     parser.add_argument('--max_length', type=int, default=None, help='Optional: Max sequence length filter')
#     parser.add_argument('--plot_predictions', action=argparse.BooleanOptionalAction, default=True, help='Generate plots')
#     parser.add_argument('--smoothing_window', type=int, default=1, help='Smoothing window for plots (1=none)')

#     args = parser.parse_args()
#     config_dict = vars(args)
#     predict(config_dict)
---------------------------------------------------------
===== FILE: ./train.py =====
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts
import argparse
import yaml
from tqdm import tqdm
import numpy as np
import random
import matplotlib.pyplot as plt
import logging
import time
import json
from typing import Dict, Any, Optional, Callable

# Import the new model and dataset
from model import EnhancedTemperatureAwareESMModel, create_model_from_config
from dataset import create_enhanced_dataloader

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s [%(filename)s:%(lineno)d] - %(message)s')
logger = logging.getLogger(__name__)

def set_seed(seed):
    """Set seed for reproducibility."""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    # Optional: Force deterministic algorithms
    # torch.backends.cudnn.deterministic = True
    # torch.backends.cudnn.benchmark = False
    logger.info(f"Set random seed to {seed}")

def log_gpu_memory(detail=False):
    """Log GPU memory usage."""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        logger.debug(f"GPU Memory: Allocated={allocated:.1f} MB, Reserved={reserved:.1f} MB")
        if detail:
            logger.info(torch.cuda.memory_summary())

def get_temperature_scaler(params_path: str) -> Callable[[float], float]:
    """Loads scaling parameters and returns a scaling function."""
    try:
        with open(params_path, 'r') as f:
            params = json.load(f)
        temp_min = params['temp_min']
        temp_max = params['temp_max']
        logger.info(f"Loaded temperature scaling parameters from {params_path}: Min={temp_min}, Max={temp_max}")

        # Handle case where min and max are the same (avoid division by zero)
        temp_range = temp_max - temp_min
        if abs(temp_range) < 1e-6:
            logger.warning("Temperature min and max are identical in scaling parameters. Scaling will return 0.5.")
            # Return a function that outputs a constant (e.g., 0.5 for midpoint)
            return lambda t: 0.5
        else:
            # Standard Min-Max scaling function
            # Adding epsilon to range for safety, though checked above
            return lambda t: (float(t) - temp_min) / (temp_range + 1e-8)

    except FileNotFoundError:
        logger.error(f"Temperature scaling file not found: {params_path}")
        raise
    except KeyError as e:
        logger.error(f"Missing key {e} in temperature scaling file: {params_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading or parsing temperature scaling file {params_path}: {e}")
        raise

def train_epoch(model, dataloader, optimizer, device, temp_scaler, accumulation_steps=1, max_gradient_norm=1.0):
    """Train the model for one epoch."""
    model.train()
    total_loss = 0.0
    total_corr = 0.0
    num_samples_processed = 0
    num_residues_processed = 0

    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None
    autocast_device_type = device.type

    optimizer.zero_grad(set_to_none=True)

    batch_iterator = tqdm(dataloader, desc="Training", leave=False)
    for i, batch in enumerate(batch_iterator):
        sequences = batch['sequences']
        # Get raw temperatures from batch
        raw_temperatures = batch['temperatures'] # This is a tensor [batch_size]
        # Targets are a list of tensors [seq_len]
        target_rmsf_values = batch['rmsf_values']
        # Get features if available
        features = batch.get('features', None)

        current_batch_size = len(sequences)
        if current_batch_size == 0: continue

        try:
            # Scale temperatures before passing to model
            # Apply scaler (which operates on floats) and convert back to tensor
            # Requires moving tensor to CPU for scaling function, then back to device.
            scaled_temps_list = [temp_scaler(t.item()) for t in raw_temperatures]
            scaled_temps_tensor = torch.tensor(scaled_temps_list, device=device, dtype=torch.float32)

            # Forward pass with AMP
            with torch.amp.autocast(device_type=autocast_device_type, enabled=(scaler is not None)):
                # Pass scaled temperatures and features to model
                outputs = model(
                    sequences=sequences,
                    temperatures=scaled_temps_tensor,
                    target_rmsf_values=target_rmsf_values,
                    features=features
                )
                loss = outputs['loss']

            # Basic loss check
            if loss is None or torch.isnan(loss) or torch.isinf(loss):
                logger.warning(f"Batch {i}: Invalid loss ({loss}). Skipping gradient update.")
                # Clear gradients manually if we skip optimizer step
                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):
                     optimizer.zero_grad(set_to_none=True)
                continue

            loss_value = loss.item() # Store for logging before scaling

            # Normalize loss for accumulation BEFORE backward pass
            loss = loss / accumulation_steps

            # Backward pass
            if scaler is not None:
                scaler.scale(loss).backward()
            else:
                loss.backward()

            # Gradient Accumulation & Optimizer Step
            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):
                if scaler is not None:
                    if max_gradient_norm > 0:
                        scaler.unscale_(optimizer) # Unscale before clipping
                        torch.nn.utils.clip_grad_norm_(
                            (p for p in model.parameters() if p.requires_grad),
                            max_gradient_norm
                        )
                    scaler.step(optimizer)
                    scaler.update()
                else: # No AMP
                    if max_gradient_norm > 0:
                        torch.nn.utils.clip_grad_norm_(
                            (p for p in model.parameters() if p.requires_grad),
                            max_gradient_norm
                        )
                    optimizer.step()

                # IMPORTANT: Zero gradients AFTER optimizer step and gradient clipping
                optimizer.zero_grad(set_to_none=True)

            # Update cumulative metrics using the original (unscaled) loss value
            # Weight loss/corr by number of residues for a more stable average if lengths vary significantly
            batch_residues = sum(len(p) for p in outputs['predictions'] if p.numel() > 0)
            if batch_residues > 0:
                total_loss += loss_value * batch_residues # Use original loss here
                correlation = outputs['metrics'].get('pearson_correlation', 0.0)
                # Ensure correlation is float and not nan
                if isinstance(correlation, torch.Tensor): correlation = correlation.item()
                if not np.isnan(correlation):
                    total_corr += correlation * batch_residues
                num_samples_processed += current_batch_size
                num_residues_processed += batch_residues

            # Update progress bar (using residue-weighted averages)
            avg_loss = total_loss / num_residues_processed if num_residues_processed > 0 else 0.0
            avg_corr = total_corr / num_residues_processed if num_residues_processed > 0 else 0.0
            batch_iterator.set_postfix(loss=f"{avg_loss:.4f}", corr=f"{avg_corr:.4f}", lr=f"{optimizer.param_groups[0]['lr']:.2e}")

        except Exception as e:
             logger.error(f"Error during training batch {i}: {e}", exc_info=True)
             logger.warning("Skipping batch due to error. Attempting to clear gradients.")
             optimizer.zero_grad(set_to_none=True) # Attempt to reset state
             if device.type == 'cuda': torch.cuda.empty_cache()
             continue

        # Optional: Periodic memory logging
        # if i > 0 and i % 100 == 0 and device.type == 'cuda': log_gpu_memory()

    # Calculate final epoch averages (residue-weighted)
    final_avg_loss = total_loss / num_residues_processed if num_residues_processed > 0 else 0.0
    final_avg_corr = total_corr / num_residues_processed if num_residues_processed > 0 else 0.0

    logger.info(f"Processed {num_samples_processed} samples ({num_residues_processed} residues) in training epoch.")
    return final_avg_loss, final_avg_corr

def validate(model, dataloader, device, temp_scaler):
    """Validate the model."""
    model.eval()
    total_loss = 0.0
    total_corr = 0.0
    num_samples_processed = 0
    num_residues_processed = 0
    domain_correlations = {} # Per-domain/sample correlation

    autocast_device_type = device.type

    batch_iterator = tqdm(dataloader, desc="Validation", leave=False)
    with torch.no_grad():
        for batch in batch_iterator:
            sequences = batch['sequences']
            domain_ids = batch['domain_ids']
            # Get raw temperatures from batch
            raw_temperatures = batch['temperatures']
            target_rmsf_values = batch['rmsf_values']
            # Get features if available
            features = batch.get('features', None)

            current_batch_size = len(sequences)
            if current_batch_size == 0: continue

            try:
                # Scale temperatures
                scaled_temps_list = [temp_scaler(t.item()) for t in raw_temperatures]
                scaled_temps_tensor = torch.tensor(scaled_temps_list, device=device, dtype=torch.float32)

                # Forward pass with AMP
                with torch.amp.autocast(device_type=autocast_device_type, enabled=(device.type == 'cuda')):
                     # Pass scaled temperatures and features to model
                     outputs = model(
                         sequences=sequences,
                         temperatures=scaled_temps_tensor,
                         target_rmsf_values=target_rmsf_values,
                         features=features
                     )
                     loss = outputs['loss'] # Batch average loss

                if loss is None or torch.isnan(loss) or torch.isinf(loss):
                    logger.warning(f"Validation: Invalid loss ({loss}) for batch starting with {domain_ids[0]}. Skipping.")
                    continue

                loss_value = loss.item()

                # Calculate batch residues for weighting metrics
                predictions_list = outputs['predictions'] # List of tensors
                batch_residues = sum(len(p) for p in predictions_list if p.numel() > 0)

                if batch_residues > 0:
                    total_loss += loss_value * batch_residues
                    # Use batch correlation if available (already averaged over samples in batch)
                    batch_avg_corr = outputs['metrics'].get('pearson_correlation', 0.0)
                    if isinstance(batch_avg_corr, torch.Tensor): batch_avg_corr = batch_avg_corr.item()
                    if not np.isnan(batch_avg_corr):
                         total_corr += batch_avg_corr * batch_residues

                    num_samples_processed += current_batch_size
                    num_residues_processed += batch_residues

                    # Store per-domain/sample correlation (calculated within model forward now)
                    # Calculate per-sample metrics for detailed reporting
                    for i, domain_id in enumerate(domain_ids):
                         if i < len(predictions_list) and predictions_list[i].numel() > 1:
                              pred_tensor = predictions_list[i]
                              true_tensor = target_rmsf_values[i].to(device)
                              min_len = min(len(pred_tensor), len(true_tensor))
                              if min_len > 1:
                                   corr_val = model.safe_pearson_correlation(
                                        pred_tensor[:min_len], true_tensor[:min_len]
                                   ).item()
                                   domain_correlations[domain_id] = corr_val
                              else: domain_correlations[domain_id] = 0.0


                # Update progress bar (residue-weighted averages)
                avg_loss = total_loss / num_residues_processed if num_residues_processed > 0 else 0.0
                avg_corr = total_corr / num_residues_processed if num_residues_processed > 0 else 0.0
                batch_iterator.set_postfix(loss=f"{avg_loss:.4f}", corr=f"{avg_corr:.4f}")

            except Exception as e:
                logger.error(f"Error during validation batch starting with {domain_ids[0]}: {e}", exc_info=True)
                continue

    # Log detailed correlation statistics from this epoch
    if domain_correlations:
         correlations = np.array(list(domain_correlations.values()))
         correlations = correlations[~np.isnan(correlations)] # Clean NaNs
         if len(correlations) > 0:
              logger.info(f"Per-Sample Validation Correlation stats (n={len(correlations)}): "
                          f"Mean={np.mean(correlations):.4f}, Median={np.median(correlations):.4f}, "
                          f"Std={np.std(correlations):.4f}, Min={np.min(correlations):.4f}, Max={np.max(correlations):.4f}")
         else: logger.warning("No valid per-sample correlations calculated during validation.")
    else: logger.warning("No per-sample correlations were calculated during validation.")

    # Calculate final epoch averages (residue-weighted)
    final_avg_loss = total_loss / num_residues_processed if num_residues_processed > 0 else 0.0
    final_avg_corr = total_corr / num_residues_processed if num_residues_processed > 0 else 0.0

    logger.info(f"Processed {num_samples_processed} samples ({num_residues_processed} residues) in validation epoch.")
    return final_avg_loss, final_avg_corr

def save_model(model, optimizer, epoch, val_loss, val_corr, config, save_path):
    """Save model checkpoint."""
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'val_loss': val_loss,
        'val_corr': val_corr,
        'config': config # Include config used for this run
    }
    try:
        torch.save(checkpoint, save_path)
        logger.info(f"Model checkpoint saved to {save_path}")
    except Exception as e:
        logger.error(f"Error saving checkpoint to {save_path}: {e}")

def plot_metrics(train_losses, val_losses, train_corrs, val_corrs, save_dir, lr_values=None):
    """Plot training and validation metrics."""
    epochs = range(1, len(train_losses) + 1)
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)

    # Loss Plot
    axes[0].plot(epochs, train_losses, 'o-', color='royalblue', label='Train Loss', markersize=4)
    axes[0].plot(epochs, val_losses, 's-', color='orangered', label='Validation Loss', markersize=4)
    axes[0].set_ylabel('Loss (MSE)')
    axes[0].set_title('Training and Validation Loss')
    axes[0].legend()
    axes[0].grid(True, linestyle='--', alpha=0.6)
    if val_losses:
        best_epoch = np.argmin(val_losses) + 1
        axes[0].axvline(best_epoch, linestyle='--', color='gray', alpha=0.7, label=f'Best Val Loss Epoch ({best_epoch})')
        axes[0].legend() # Update legend

    # Correlation Plot
    axes[1].plot(epochs, train_corrs, 'o-', color='royalblue', label='Train Correlation', markersize=4)
    axes[1].plot(epochs, val_corrs, 's-', color='orangered', label='Validation Correlation', markersize=4)
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Pearson Correlation')
    axes[1].set_title('Training and Validation Correlation')
    axes[1].grid(True, linestyle='--', alpha=0.6)
    if val_corrs:
        best_epoch = np.argmax(val_corrs) + 1
        axes[1].axvline(best_epoch, linestyle='--', color='gray', alpha=0.7, label=f'Best Val Corr Epoch ({best_epoch})')

    # Optional Learning Rate Plot on Correlation Axis
    if lr_values:
        ax_lr = axes[1].twinx()
        ax_lr.plot(epochs, lr_values, 'd--', color='green', label='Learning Rate', markersize=3, alpha=0.6)
        ax_lr.set_ylabel('Learning Rate', color='green')
        ax_lr.tick_params(axis='y', labelcolor='green')
        if len(set(lr_values)) > 2: ax_lr.set_yscale('log')
        # Combine legends
        lines, labels = axes[1].get_legend_handles_labels()
        lines2, labels2 = ax_lr.get_legend_handles_labels()
        axes[1].legend(lines + lines2, labels + labels2, loc='lower left')
    else:
         axes[1].legend(loc='lower left')


    plt.tight_layout(pad=2.0)
    plot_path = os.path.join(save_dir, 'training_metrics.png')
    try:
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        logger.info(f"Metrics plot saved to {plot_path}")
    except Exception as e:
        logger.error(f"Error saving metrics plot to {plot_path}: {e}")
    plt.close(fig)

def create_optimizer(model, lr, weight_decay, adam_epsilon):
    """
    Create optimizer with weight decay applied only to weight matrices.
    
    Args:
        model: PyTorch model
        lr: Learning rate
        weight_decay: Weight decay factor
        adam_epsilon: Epsilon parameter for Adam
        
    Returns:
        Configured optimizer
    """
    # Separate parameters into those with and without weight decay
    decay_params = []
    no_decay_params = []
    
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
            
        # Skip normalization layer weights, biases, and layer norm
        if 'LayerNorm' in name or name.endswith('bias'):
            no_decay_params.append(param)
        else:
            decay_params.append(param)
    
    param_groups = [
        {'params': decay_params, 'weight_decay': weight_decay},
        {'params': no_decay_params, 'weight_decay': 0.0}
    ]
    
    optimizer = optim.AdamW(
        param_groups,
        lr=lr,
        eps=adam_epsilon
    )
    
    return optimizer

def create_scheduler(config, optimizer):
    """
    Create learning rate scheduler based on configuration.
    
    Args:
        config: Configuration dictionary
        optimizer: Optimizer to schedule
        
    Returns:
        Learning rate scheduler
    """
    scheduler_type = config.get('training', {}).get('scheduler', 'plateau')
    
    if scheduler_type.lower() == 'cosine':
        # Cosine annealing with warm restarts
        # T_0: Number of iterations for the first restart
        # T_mult: Factor by which T_i increases after a restart
        t_0 = config.get('training', {}).get('t_0', 10)
        t_mult = config.get('training', {}).get('t_mult', 2)
        eta_min = config.get('training', {}).get('min_lr', 1e-7)
        
        scheduler = CosineAnnealingWarmRestarts(
            optimizer,
            T_0=t_0,
            T_mult=t_mult,
            eta_min=eta_min
        )
        logger.info(f"Using CosineAnnealingWarmRestarts scheduler: T_0={t_0}, T_mult={t_mult}, min_lr={eta_min}")
        
        return scheduler, False  # scheduler needs to be called after each iteration
    else:
        # Default: ReduceLROnPlateau
        patience = config.get('training', {}).get('scheduler_patience', 5)
        factor = config.get('training', {}).get('scheduler_factor', 0.5)
        scheduler = ReduceLROnPlateau(
            optimizer, 
            mode='max',  # Maximize validation correlation
            factor=factor,
            patience=patience,
            verbose=True,
            threshold=0.001  # Minimum improvement to consider significant
        )
        logger.info(f"Using ReduceLROnPlateau scheduler: patience={patience}, factor={factor}")
        
        return scheduler, True  # scheduler called on validation metrics

def train(config: Dict[str, Any]):
    """Main training function."""
    start_time_train = time.time()

    # --- Setup ---
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    if device.type == 'cuda': log_gpu_memory()

    seed = config['training']['seed']
    set_seed(seed)

    # Directories
    model_save_dir = config['output']['model_dir']
    data_dir = config['data']['data_dir'] # Processed data lives here
    log_dir = os.path.join(model_save_dir, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(model_save_dir, exist_ok=True) # Ensure model dir exists

    # File logging
    log_path = os.path.join(log_dir, 'training.log')
    file_handler = logging.FileHandler(log_path, mode='a')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s [%(filename)s:%(lineno)d] - %(message)s'))
    logger.addHandler(file_handler)
    logger.info(f"--- Starting Enhanced Temperature-Aware Training Run (Seed: {seed}) ---")
    logger.info(f"Full Configuration: {json.dumps(config, indent=2)}")


    # --- Load Temperature Scaler ---
    temp_scaling_path = os.path.join(data_dir, config['data']['temp_scaling_filename'])
    try:
        temp_scaler = get_temperature_scaler(temp_scaling_path)
    except Exception:
         logger.error("Failed to load temperature scaler. Aborting training.")
         return

    # --- Data Loaders ---
    logger.info("Creating enhanced data loaders...")
    try:
        train_dataloader = create_enhanced_dataloader(
            data_dir, 'train', config['training']['batch_size'],
            shuffle=True, max_length=config['training'].get('max_length'),
            length_bucket_size=config['training'].get('length_bucket_size', 50),
            config=config
        )
        val_dataloader = create_enhanced_dataloader(
            data_dir, 'val', config['training']['batch_size'],
            shuffle=False, max_length=config['training'].get('max_length'),
            length_bucket_size=config['training'].get('length_bucket_size', 50),
            config=config
        )
        if not train_dataloader or not val_dataloader:
            raise RuntimeError("Failed to create one or both dataloaders.")
        logger.info(f"Train samples: {len(train_dataloader.dataset)}, Val samples: {len(val_dataloader.dataset)}")
    except Exception as e:
        logger.error(f"Error creating dataloaders: {e}", exc_info=True)
        return

    # --- Model ---
    logger.info("Creating enhanced model from config...")
    try:
        model = create_model_from_config(config)
        model = model.to(device)
        if device.type == 'cuda': log_gpu_memory()
    except Exception as e:
        logger.error(f"Error creating model: {e}", exc_info=True)
        return

    # --- Optimizer ---
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    if not trainable_params:
         logger.error("Model has no trainable parameters! Check model initialization and freezing logic.")
         return
    logger.info(f"Optimizing {len(trainable_params)} parameter tensors.")

    # Use improved optimizer with selective weight decay
    optimizer = create_optimizer(
        model,
        lr=float(config['training']['learning_rate']),
        weight_decay=float(config['training']['weight_decay']),
        adam_epsilon=float(config['training'].get('adam_epsilon', 1e-8))
    )
    logger.info(f"Optimizer created: AdamW with selective weight decay")

    # --- Scheduler ---
    scheduler, is_epoch_scheduler = create_scheduler(config, optimizer)
    
    # --- Training Loop ---
    logger.info("--- Starting Training Loop ---")
    best_val_corr = -float('inf')
    best_val_loss = float('inf')
    best_epoch = -1
    patience_counter = 0
    train_losses, val_losses, train_corrs, val_corrs, lr_values = [], [], [], [], []

    num_epochs = config['training']['num_epochs']
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        logger.info(f"--- Epoch {epoch+1}/{num_epochs} ---")

        # Train
        train_loss, train_corr = train_epoch(
            model, train_dataloader, optimizer, device, temp_scaler,
            config['training']['accumulation_steps'],
            config['training'].get('max_gradient_norm', 1.0)
        )
        train_losses.append(train_loss)
        train_corrs.append(train_corr)

        # Validate
        val_loss, val_corr = validate(model, val_dataloader, device, temp_scaler)
        val_losses.append(val_loss)
        val_corrs.append(val_corr)

        current_lr = optimizer.param_groups[0]['lr']
        lr_values.append(current_lr)

        epoch_duration = time.time() - epoch_start_time

        logger.info(f"Epoch {epoch+1} Summary (Duration: {epoch_duration:.1f}s):")
        logger.info(f"  Train Loss: {train_loss:.6f} | Train Corr: {train_corr:.6f}")
        logger.info(f"  Val Loss:   {val_loss:.6f} | Val Corr:   {val_corr:.6f} {'*' if val_corr > best_val_corr else ''}")
        logger.info(f"  Learning Rate: {current_lr:.3e}")

        # Update scheduler
        if is_epoch_scheduler:
            # For ReduceLROnPlateau - once per epoch, based on val_corr
            scheduler.step(val_corr)
        
        # Checkpointing & Early Stopping based on Validation Correlation
        is_best = val_corr > best_val_corr
        if is_best:
            improvement = val_corr - best_val_corr
            logger.info(f"  New best validation correlation! Improvement: +{improvement:.6f}")
            best_val_corr = val_corr
            best_val_loss = val_loss
            best_epoch = epoch + 1
            patience_counter = 0
            # Save best model
            save_model(model, optimizer, epoch, val_loss, val_corr, config, os.path.join(model_save_dir, 'best_model.pt'))
        else:
            patience_counter += 1
            logger.info(f"  Validation correlation did not improve. Patience: {patience_counter}/{config['training']['early_stopping_patience']}")

        # Save latest model state
        save_model(model, optimizer, epoch, val_loss, val_corr, config, os.path.join(model_save_dir, 'latest_model.pt'))

        # Save periodic checkpoint if enabled
        checkpoint_interval = config['training'].get('checkpoint_interval', 0)
        if checkpoint_interval > 0 and (epoch + 1) % checkpoint_interval == 0:
            save_model(model, optimizer, epoch, val_loss, val_corr, config, os.path.join(model_save_dir, f'checkpoint_epoch_{epoch+1}.pt'))

        # Update metrics plot
        plot_metrics(train_losses, val_losses, train_corrs, val_corrs, model_save_dir, lr_values)

        # Check for early stopping
        if patience_counter >= config['training']['early_stopping_patience']:
            logger.info(f"Early stopping triggered after {epoch+1} epochs due to lack of improvement in validation correlation.")
            break

        if device.type == 'cuda': torch.cuda.empty_cache()

    # --- End of Training ---
    total_training_time = time.time() - start_time_train
    logger.info("--- Training Finished ---")
    logger.info(f"Total training time: {total_training_time:.2f}s ({total_training_time/60:.1f} minutes)")
    logger.info(f"Best validation correlation: {best_val_corr:.6f} at epoch {best_epoch}")
    logger.info(f"Corresponding validation loss: {best_val_loss:.6f}")
    logger.info(f"Best model saved to: {os.path.join(model_save_dir, 'best_model.pt')}")
    logger.info(f"Training logs and plots saved in: {model_save_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train a Temperature-Aware ESM Regression model')
    parser.add_argument('--config', type=str, default='config.yaml', help='Path to the YAML configuration file')
    
    args = parser.parse_args()
    
    # Load configuration
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    train(config)
---------------------------------------------------------
==========================================================
End of Deep-flex Context Document
==========================================================
