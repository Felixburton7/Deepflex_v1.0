==========================================================
          EnsembleFlex: Temperature-Aware Protein Flexibility ML Pipeline
==========================================================
Context generated on: Wed 16 Apr 11:51:17 BST 2025

==========================================================
Input Data Format (Aggregated)
==========================================================
EnsembleFlex expects a SINGLE aggregated CSV file (e.g., 'aggregated_rmsf_data.csv') specified in the config.
This file contains data from multiple temperatures, with temperature as an input feature.

Expected Columns in the Aggregated CSV:
----------------------------------------------------------
| Column Name                 | Type    | Description                                                  |
|-----------------------------|---------|--------------------------------------------------------------|
| domain_id                   | string  | Protein domain identifier (e.g., '1a0aA00')                  |
| resid                       | int     | Residue ID (position in the original chain)                  |
| resname                     | string  | 3-letter amino acid code (e.g., 'ALA', 'LYS')                |
| protein_size                | int     | Total number of residues in the protein/domain               |
| normalized_resid            | float   | Residue position normalized to [0, 1] range                  |
| core_exterior               | string  | Original classification ('core' or 'surface')                |
| relative_accessibility    | float   | Solvent Accessible Surface Area (SASA), typically [0, 1]     |
| dssp                        | string  | Secondary structure char (DSSP: H, E, C, T, G, etc.)         |
| phi                         | float   | Backbone dihedral angle phi (degrees)                        |
| psi                         | float   | Backbone dihedral angle psi (degrees)                        |
| resname_encoded             | int     | Numerical encoding of 'resname'                              |
| core_exterior_encoded     | int     | Numerical encoding of 'core_exterior' (e.g., 0=core, 1=surf) |
| secondary_structure_encoded | int     | Numerical encoding of 'dssp' (e.g., 0=Helix, 1=Sheet, 2=Coil)|
| phi_norm                    | float   | Normalized phi angle (e.g., sin(rad(phi)), [-1, 1])          |
| psi_norm                    | float   | Normalized psi angle (e.g., sin(rad(psi)), [-1, 1])          |
| temperature                 | float   | **INPUT FEATURE:** Temperature (K) for this data point       |
| rmsf                        | float   | **TARGET:** RMSF value for this residue at 'temperature'     |
| esm_rmsf (OmniFlex only)    | float   | External prediction from ESM embeddings                      |
| voxel_rmsf (OmniFlex only)  | float   | External prediction from 3D voxel representation             |
| ... (*_offset_*)           | float/int | Optional window features based on neighboring residues     |
----------------------------------------------------------

==========================================================
Usage Examples (EnsembleFlex)
==========================================================
# Train the unified model (uses aggregated data specified in config)
ensembleflex train

# Train a specific model type (e.g., random_forest)
ensembleflex train --model random_forest

# Train a specific model type (e.g., random_forest)
ensembleflex run --model random_forest

# Evaluate the trained model (uses test split from aggregated data)
ensembleflex evaluate --model random_forest

# Generate predictions using the trained model REQUIRES temperature
ensembleflex predict --input new_proteins_features.csv --temperature 310.5


# Use OmniFlex mode (if model was trained with it)
ensembleflex train --mode omniflex

# Override a config parameter
ensembleflex train --param models.neural_network.training.epochs=50

==========================================================
Project Information
==========================================================
Project Root Directory: /home/s_felix/FINAL_PROJECT/packages/base_models/EnsembleFlex

Project Tree Structure:
---------------------------------------------------------
.
./ensemble_flex
./ensembleflex
./ensembleflex_
./ensemble_flex/data
./ensembleflex/data
./ensemble_flex/models
./ensembleflex/models
./ensembleflex_/models
./ensembleflex/temperature
./ensembleflex/utils
./scripts
./test
./test/test
./test/test/test_data

File Listing (Key Project Files):
---------------------------------------------------------
./AI_context.sh
./default_config.yaml
./ensemble_flex/cli.py
./ensembleflex/cli.py
./ensembleflex_/cli.py
./ensembleflex/config.py
./ensembleflex/data/__init__.py
./ensembleflex/data/loader.py
./ensemble_flex/data/processor.py
./ensembleflex/data/processor.py
./ensembleflex/__init__.py
./ensembleflex/models/base.py
./ensembleflex/models/__init__.py
./ensemble_flex/models/lightgbm_classifier.py
./ensembleflex/models/lightgbm_classifier.py
./ensembleflex_/models/lightgbm_classifier.py
./ensemble_flex/models/lightgbm.py
./ensembleflex/models/lightgbm.py
./ensembleflex_/models/lightgbm.py
./ensemble_flex/models/neural_network.py
./ensembleflex/models/neural_network.py
./ensembleflex_/models/neural_network.py
./ensemble_flex/models/random_forest.py
./ensembleflex/models/random_forest.py
./ensembleflex_/models/random_forest.py
./ensembleflex/pipeline.py
./ensembleflex/temperature/comparison.py
./ensembleflex/temperature/__init__.py
./ensembleflex/utils/helpers.py
./ensembleflex/utils/__init__.py
./ensembleflex/utils/metrics.py
./ensembleflex/utils/temperature_comparison_visualizer.py
./ensembleflex/utils/visualization.py
./get_rf_importance.py
./maxed_config.yaml
./pyproject.toml
./README.md
./scripts/aggregate_data.py
./setup.py
./test/generate_test_data.py
./test/__init__.py
./test/test_models.py
./test/test_pipeline.py

==========================================================
Default Configuration (default_config.yaml)
==========================================================
# # ensembleflex Configuration (Refactored from FlexSeq)
# Single, unified, temperature-aware model trained on aggregated data.

# Paths (Generalized - No temperature templating needed)
paths:
  data_dir: ./data               # Data directory
  output_dir: ./output/ensembleflex/  # Unified output directory for this model
  models_dir: ./models/ensembleflex/  # Unified models directory for this model

# Mode configuration (Remains the same)
mode:
  active: "omniflex"              # "standard" or "omniflex"
  omniflex:
    use_esm: true                 # Use ESM embeddings feature
    use_voxel: true               # Enable 3D voxel feature

# Temperature configuration (Simplified)
temperature:
  available: [320, 348, 379, 413, 450] # INFORMATIONAL ONLY - List of temps in original data
  comparison:                     # PURPOSE REDEFINED - Analyze single model's sensitivity to temp feature
    enabled: true                 # Analyze sensitivity to input temperature
    # Metrics can be calculated on test set binned by the 'temperature' column
    metrics: ["rmse", "r2", "pearson_correlation", "root_mean_square_absolute_error"]

# Dataset configuration (Updated for aggregated data)
dataset:
  # Data loading
  file_pattern: "aggregated_train_dataset.csv" # Points to single aggregated train file

  # Domain filtering (Remains the same)
  domains:
    include: []                   # Empty means include all domains
    exclude: []                   # Domains to exclude
    min_protein_size: 0           # Minimum protein size (based on unique residues per domain)
    max_protein_size: null        # Maximum protein size (null = no limit)

  # Feature configuration (Updated)
  features:
    # Required columns that must exist in the aggregated data
    required:
      - domain_id                 # Domain identifier
      - resid                     # Residue ID
      - resname                   # Residue name
      - rmsf                      # UPDATED - Unified target variable
      - temperature               # ADDED - Temperature feature
      - bfactor_norm

    # Input features with toggles - ADD 'temperature'
    use_features:
      protein_size: true          # Size of protein
      normalized_resid: true      # Position in sequence
      relative_accessibility: true # Solvent accessibility (Example: disabled)
      core_exterior_encoded: true # Core or exterior (Example: disabled)
      secondary_structure_encoded: true # Secondary structure (Example: disabled)
      phi_norm: true              # Normalized phi angle (Example: disabled)
      psi_norm: true              # Normalized psi angle (Example: disabled)
      resname_encoded: true       # Encoded residue name
      temperature: true           # ADDED & ENABLED - Include temperature as feature
      esm_rmsf: false              # ESM embeddings prediction (OmniFlex only)
      voxel_rmsf: true            # 3D voxel prediction (OmniFlex only)
      bfactor_norm: true

    # Enhanced feature engineering (Remains the same conceptually)
    window:
      enabled: false               # Use window-based features
      size: 0                    # Increased window size for better context

  # Target variable (Updated)
  target: rmsf                    # UPDATED - Unified target column name

  # Data splitting (Remains the same conceptually)
  split:
    test_size: 0.05                # Test set size
    validation_size: 0.1         # Validation set size
    stratify_by_domain: true      # Keep domains together (RECOMMENDED)
    random_state: 42              # Random seed

# Evaluation settings (Remains the same)
evaluation:
  comparison_set: "test"          # Which set to use: "validation" or "test"
  metrics:
    rmse: true                    # Root Mean Squared Error
    mae: true                     # Mean Absolute Error
    r2: true                      # R-squared
    pearson_correlation: true     # Pearson correlation
    spearman_correlation: true    # Spearman rank correlation
    root_mean_square_absolute_error: true  # Root Mean Square Absolute Error

# Model configurations 
models:
  # Shared settings
  common:
    cross_validation:
      enabled: true               # Enable cross-validation for better validation
      folds: 5                    # Number of folds if enabled
    save_best: true               # Save best model

  # Neural Network - enhanced architecture and training
  neural_network:
    enabled: true                 # Run this model
    architecture:
      hidden_layers: [256, 128, 64]  # Larger network
      activation: relu            # Activation function
      dropout: 0.3                # Increased dropout for better generalization
    training:
      optimizer: adam             # Optimizer
      learning_rate: 0.001        # Learning rate
      batch_size: 256              # Increased batch size
      epochs: 3                  # Increased max epochs
      early_stopping: true        # Use early stopping
      patience: 5                 # Increased patience
    hyperparameter_optimization:
      enabled: false              # Enable hyperparameter optimization
      method: "random"            # Better optimization method
      trials: 5                   # More trials
      parameters:                 # Enhanced parameter space
        hidden_layers:
          - [64, 32]
          - [128, 64]
          - [256, 128]
          - [512, 256, 128]
          - [256, 128, 64, 32]
          - [128, 128, 64]
        learning_rate: [0.01, 0.005, 0.001, 0.0005, 0.0001]
        batch_size: [32, 64, 128]
        dropout: [0.1, 0.2, 0.3, 0.4, 0.5]
        activation: ["relu", "leaky_relu", "tanh"]

  random_forest:
    enabled: true
    # Core parameters (These are less important when HPO is enabled,
    # as HPO finds the best ones, but good to have reasonable defaults)
    n_estimators: 150              # Default if HPO disabled
    max_depth: 25                  # Default if HPO disabled
    min_samples_split: 10
    min_samples_leaf: 5
    max_features: 'sqrt'
    bootstrap: true
    verbose: 1                     # Shows progress within each fit
    n_jobs: -12                     # USE ALL CORES - Standard practice

    # HPO configuration - Faster search
    randomized_search:
      enabled: false
      n_iter: 8                   
      cv: 2                        # Keep 2 folds for speed
      param_distributions:
        n_estimators: [50, 150, 250]   
        max_depth: [10, 15, 20]        
        min_samples_split: [10, 20]    # Focus on simpler trees
        min_samples_leaf: [5, 10]      # Focus on simpler trees
        max_features: ['sqrt', 0.4]    # Simpler search, includes 'sqrt'


  #LightGBM
  lightgbm:
    enabled: true                 # <<< ENABLE LIGHTGBM
    objective: 'regression_l1'    # MAE loss - often good for RMSF/distances
    metric: 'mae'                 # Evaluate using MAE during training
    n_estimators: 1500            # Start high, rely on early stopping
    learning_rate: 0.02           # Typical learning rate
    num_leaves: 35                # Default, good starting point
    max_depth: -1                 # No limit on depth
    reg_alpha: 0.05               # L1 regularization
    reg_lambda: 0.05              # L2 regularization
    colsample_bytree: 0.8         # Feature fraction per tree
    subsample: 0.8                # Data fraction per tree (row sampling)
    # boosting_type: 'gbdt'       # Default, others: 'dart', 'goss'
    n_jobs: -10                    # Use all cores
    random_state: 42              # Seed for reproducibility
    # Early stopping configuration for LightGBM
    early_stopping:
      enabled: true               # Enable early stopping based on validation set
      stopping_rounds: 50         # Number of rounds with no improvement to stop
      # verbose: false            # Log LightGBM's internal early stopping messages? (can be noisy)
    # Hyperparameter Optimization (Placeholder - Not implemented in model class yet)
    # hyperparameter_optimization:
    #   enabled: false
    #   method: "bayesian" # e.g., using Optuna
    #   trials: 50
    #   parameters:
    #     num_leaves: [15, 31, 63, 127]
    #     learning_rate: [0.01, 0.03, 0.05, 0.1]
    #     n_estimators: [500, 1000, 1500, 2000]
    #     reg_alpha: [0, 0.01, 0.1, 0.5]
    #     reg_lambda: [0, 0.01, 0.1, 0.5]
    #     colsample_bytree: [0.6, 0.7, 0.8, 0.9, 1.0]
    #     subsample: [0.6, 0.7, 0.8, 0.9, 1.0]

# Analysis and visualization (Purpose redefined for temp comparison)
analysis:
  feature_importance:
    enabled: true                 # Analyze feature importance (will include 'temperature')
    method: "permutation"         # Use permutation importance
    n_repeats: 20                 # Increased permutation repetitions
    use_validation_data: true     # Use validation data for importance calculation

  temperature_comparison:         # PURPOSE REDEFINED
    enabled: true                 # Analyze the single model's sensitivity to the input 'temperature' feature
    # Metrics applied to test set predictions, potentially binned by 'temperature' column values
    metrics: ["rmse", "r2", "pearson_correlation", "root_mean_square_absolute_error"]
    plots:                        # Plots should focus on error/prediction vs temperature feature
      histogram: true             # Generate histogram plots (overall RMSF, maybe binned errors)
      correlation: false          # Correlation between temps less relevant now
      performance: true           # Generate performance vs input temp plots
      error_vs_temp: true         # Explicitly plot error metrics vs input temperature
      prediction_vs_temp: true    # Explicitly plot predictions vs input temperature

# System settings (Remains the same)
system:
  n_jobs: -1                      # Use all available cores
  random_state: 42                # Global random seed
  log_level: INFO                 # Logging level
  gpu_enabled: auto               # Auto-detect GPU
==========================================================
EnsembleFlex Package Files
==========================================================
### Main Package Files ###
---------------------------------------------------------
===== FILE: pyproject.toml =====
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "ensembleflex"
version = "0.1.0"
description = "ML pipeline for protein flexibility prediction with multi-temperature analysis"
readme = "README.md"
authors = [
  { name = "Felix Burton", email = "felixburton2002@gmail.com" }
]
requires-python = ">=3.8"
classifiers = [
  "Programming Language :: Python :: 3",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Topic :: Scientific/Engineering :: Bio-Informatics",
  "Topic :: Scientific/Engineering :: Artificial Intelligence"
]
dependencies = [
  "numpy>=1.20.0",
  "pandas>=1.3.0",
  "scikit-learn>=1.0.0",
  "torch>=1.9.0",
  "pyyaml>=6.0",
  "click>=8.0.0",
  "matplotlib>=3.4.0",
  "seaborn>=0.11.0",
  "joblib>=1.0.0",
  "tqdm>=4.64.0",
  "optuna>=3.0.0",  # <-- ADDED COMMA HERE
  "lightgbm>=3.0.0"
]

[project.scripts]
ensembleflex = "ensembleflex.cli:cli"

[project.urls]
Homepage = "
"Bug Tracker" = "/issues"
===== FILE: setup.py =====
import os
from setuptools import setup, find_packages

# Read the content of README.md
this_directory = os.path.abspath(os.path.dirname(__file__))
with open(os.path.join(this_directory, 'README.md'), encoding='utf-8') as f:
    long_description = f.read()

setup(
    name="ensembleflex",
    version="0.1.0",
    description="ML pipeline for protein flexibility prediction with multi-temperature analysis",
    long_description=long_description,
    long_description_content_type="text/markdown",
    author="Felix Burton",
    author_email="felixburton2002@gmail.comcom",
    url="
    packages=find_packages(),
    include_package_data=True,
    install_requires=[
        "tqdm>=4.64.0",
        "numpy>=1.20.0",
        "pandas>=1.3.0",
        "scikit-learn>=1.0.0",
        "torch>=1.9.0",
        "pyyaml>=6.0",
        "click>=8.0.0",
        "matplotlib>=3.4.0",
        "seaborn>=0.11.0",
        "joblib>=1.0.0",
        "optuna>=3.0.0",
    ],
    entry_points={
        "console_scripts": [
            "ensembleflex=ensembleflex.cli:cli",
        ],
    },
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Topic :: Scientific/Engineering :: Bio-Informatics",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
    ],
    python_requires=">=3.8",
)
===== FILE: README.md =====
# FlexSeq: Protein Flexibility Prediction Pipeline üß¨üîç

<div align="center">

<img src="https://via.placeholder.com/150x150/4B0082/FFFFFF?text=FlexSeq" alt="FlexSeq Logo" width="150"/>

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen?style=for-the-badge)](CONTRIBUTING.md)
[![GitHub Repo](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](

**A comprehensive machine learning pipeline for predicting protein flexibility (RMSF) across multiple temperatures using sequence and structural features.**

[üìä Key Features](#key-features) ‚Ä¢
[üîß Installation](#installation) ‚Ä¢
[üöÄ Quick Start](#quick-start) ‚Ä¢
[üîÑ Pipeline Overview](#pipeline-overview) ‚Ä¢
[üì• Input Data](#input-data) ‚Ä¢
[üì§ Output Data](#output-data) ‚Ä¢
[ü§ñ Models](#models) ‚Ä¢
[üìà Analysis & Visualization](#analysis--visualization) ‚Ä¢
[‚öôÔ∏è Configuration](#configuration) ‚Ä¢
[üíª Command-Line Interface](#command-line-interface) ‚Ä¢
[üìö Documentation](#documentation) ‚Ä¢
[ü§ù Contributing](#contributing)

</div>

## üåü Overview

FlexSeq is a machine learning pipeline meticulously designed for predicting protein flexibility, quantified as Root Mean Square Fluctuation (RMSF), based on protein sequence and structural features. A core capability of FlexSeq is its robust support for analyzing and comparing flexibility across a range of user-defined temperatures (e.g., 320K, 348K, 379K, 413K, 450K, and an averaged dataset), enabling the study of temperature-dependent dynamic behavior.

The pipeline offers two distinct operational modes, configurable via the `mode.active` setting:

-   **üî¨ FlexSeq Mode**: The standard operational mode, utilizing a rich set of features derived directly from protein sequence and basic structural properties (e.g., protein size, residue position, solvent accessibility, secondary structure classification from DSSP, backbone dihedral angles œÜ/œà).
-   **üî≠ OmniFlex Mode**: An enhanced prediction mode that leverages the standard features *plus* pre-computed RMSF predictions derived from external, powerful models like ESM (Evolutionary Scale Modeling) embeddings (`esm_rmsf`) and potentially 3D voxel representations (`voxel_rmsf`), aiming for improved predictive accuracy.

FlexSeq employs a modular and configurable architecture, built upon Python libraries like Pandas, Scikit-learn, PyTorch, and Optuna. It features:
*   Configurable machine learning models (Random Forest and Neural Network).
*   Automated feature engineering, including sequence-window features and numerical encoding.
*   Comprehensive model evaluation using a suite of standard regression metrics.
*   Integrated hyperparameter optimization using Randomized Search (RF) or Optuna (NN).
*   Tools for systematic comparison of results across different temperatures.
*   Uncertainty estimation capabilities for model predictions.
*   A flexible and user-friendly Command-Line Interface (CLI) powered by Click.

## üìä Key Features

<table>
<thead>
  <tr bgcolor="#6236FF">
    <th width="200"><span style="color:white">Feature</span></th>
    <th><span style="color:white">Description</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>üå°Ô∏è **Multi-Temperature Analysis**</td>
    <td>Train models, evaluate performance, and compare RMSF predictions across a user-defined list of temperatures (e.g., `[320, 348, 379, 413, 450, "average"]`).</td>
  </tr>
  <tr>
    <td>ü§ñ **Multiple ML Models**</td>
    <td>Includes Random Forest (Scikit-learn) and Feed-Forward Neural Network (PyTorch) implementations. The architecture allows for easy addition of new models inheriting from `BaseModel`.</td>
  </tr>
  <tr>
    <td>‚öôÔ∏è **Feature Engineering**</td>
    <td>Automatic encoding of categorical features (residue name, core/exterior, secondary structure), normalization of angles (œÜ/œà), calculation of normalized residue position, and optional generation of window-based features using neighboring residue information.</td>
  </tr>
  <tr>
    <td>üî¨ **OmniFlex Mode**</td>
    <td>Optionally incorporates external predictions (`esm_rmsf`, `voxel_rmsf`) as input features for potentially enhanced performance. Enabled via configuration.</td>
  </tr>
  <tr>
    <td>‚ö†Ô∏è **Uncertainty Quantification**</td>
    <td>Models provide uncertainty estimates: standard deviation across trees for Random Forest, Monte Carlo Dropout sampling for Neural Network.</td>
  </tr>
  <tr>
    <td>üìè **Comprehensive Evaluation**</td>
    <td>Utilizes multiple metrics including RMSE, MAE, R¬≤, Pearson correlation, Spearman correlation, and Root Mean Square Absolute Error (RMSAE). Metrics are configurable.</td>
  </tr>
  <tr>
    <td>üìä **Analysis & Visualization**</td>
    <td>Generates detailed output CSV files for evaluation metrics, domain-level performance, residue-level errors (by AA type, position, structure), feature importance, and cross-temperature comparisons, suitable for external visualization tools. Also generates basic plots (e.g., feature importance).</td>
  </tr>
   <tr>
    <td>üß© **Domain Stratification**</td>
    <td>Supports data splitting (`train`/`validation`/`test`) that ensures all residues from a given protein domain are kept within the same split, preventing data leakage between sets.</td>
  </tr>
  <tr>
    <td>üéØ **Hyperparameter Optimization**</td>
    <td>Automated tuning for both models using Scikit-learn's `RandomizedSearchCV` for Random Forest and Optuna (supporting random search and Bayesian optimization) for Neural Network. Configuration allows defining search spaces and trials.</td>
  </tr>
  <tr>
    <td>üíª **Command-Line Interface**</td>
    <td>Provides the `flexseq` command with subcommands (`train`, `evaluate`, `predict`, `run`, `compare-temperatures`, etc.) for easy pipeline execution and control.</td>
  </tr>
   <tr>
    <td>‚öôÔ∏è **Configuration System**</td>
    <td>Highly flexible configuration via YAML (`default_config.yaml`), environment variables (prefixed `FLEXSEQ_`), and direct CLI parameter overrides (`--param`). Supports temperature templating in paths and column names.</td>
  </tr>
</tbody>
</table>

## üîÑ Pipeline Overview

The FlexSeq pipeline follows a structured workflow managed by the `Pipeline` class (`flexseq/pipeline.py`), driven by the configuration settings.

**Conceptual Workflow Diagram:**

```mermaid
graph TD
    A["Input: Temp-Specific CSV Data\n(e.g., temperature_320_train.csv)"] --> B(Load & Process Data);
    B --> C["Clean Data & Feature Engineering\n(Encoding, Normalization, Windowing)"];
    C --> D(Filter Domains);
    D --> E{"Split Data\n(Train/Val/Test Sets\nStratify by Domain?)"};

    subgraph "Model Training Pipeline"
        direction LR
        E -- Train Set --> F["Select Enabled Models\n(RF, NN)"];
        F --> G{Optimize Hyperparameters?};
        G -- Yes --> H["Optimize via CV\n(Optuna/RandomizedSearch)"];
        G -- No --> I["Train Model\n(model.fit)"];
        H --> I;
        I --> J["Save Trained Model\n(./models/...)"];
    end

    subgraph "Model Evaluation Pipeline"
        direction LR
        J --> K[Load Trained Model];
        E -- Evaluation Set (Test/Val) --> L[Prepare Eval Data];
        K --> M["Predict on Eval Set\n(model.predict/predict_with_std)"];
        L --> M;
        M --> N["Calculate Metrics\n(utils.metrics)"];
        N --> O["Save Metrics & Results\n(./output/outputs_T/...)"];
    end

    subgraph "Prediction Pipeline"
        direction LR
        P[Input: New Data CSV] --> Q(Load & Process New Data);
        J --> R[Load Trained Model];
        Q --> S["Predict on New Data\n(model.predict/predict_with_std)"];
        R --> S;
        S --> T[Save Predictions CSV];
    end

    subgraph "Analysis & Comparison"
        direction LR
        O -- Per-Temp Results --> U["Temperature Comparison\n(temperature.comparison)"];
        O -- Per-Temp Results --> V["Analysis & Vis Data Gen\n(Feature Importance, etc.)"];
        U --> W["Save Comparison Data\n(./output/outputs_comparison/)"]
        V --> X["Save Analysis CSVs & Plots\n(./output/outputs_T/...)"];
    end

    Z["Configuration File\n(YAML, Env Vars, CLI)"] -.-> B;
    Z -.-> C;
    Z -.-> D;
    Z -.-> E;
    Z -.-> F;
    Z -.-> G;
    Z -.-> L;
    Z -.-> N;
    Z -.-> U;
    Z -.-> V;

    style A fill:#FFDAB9,stroke:#FFA07A
    style P fill:#FFDAB9,stroke:#FFA07A
    style B fill:#ADD8E6,stroke:#87CEEB
    style C fill:#ADD8E6,stroke:#87CEEB
    style D fill:#ADD8E6,stroke:#87CEEB
    style E fill:#ADD8E6,stroke:#87CEEB
    style F fill:#90EE90,stroke:#3CB371
    style G fill:#FFFFE0,stroke:#F0E68C
    style H fill:#FFEC8B,stroke:#CDAD00
    style I fill:#90EE90,stroke:#3CB371
    style J fill:#C1FFC1,stroke:#00CD00
    style K fill:#FFFFE0,stroke:#F0E68C
    style L fill:#ADD8E6,stroke:#87CEEB
    style M fill:#FFB6C1,stroke:#FF69B4
    style N fill:#FFB6C1,stroke:#FF69B4
    style O fill:#DDA0DD,stroke:#BA55D3
    style Q fill:#ADD8E6,stroke:#87CEEB
    style R fill:#FFFFE0,stroke:#F0E68C
    style S fill:#FFB6C1,stroke:#FF69B4
    style T fill:#DDA0DD,stroke:#BA55D3
    style U fill:#E6E6FA,stroke:#9370DB
    style V fill:#E6E6FA,stroke:#9370DB
    style W fill:#D8BFD8,stroke:#9A32CD
    style X fill:#D8BFD8,stroke:#9A32CD
    style Z fill:#F5F5DC,stroke:#A0522D
```

### üß© Logical Flow of Operation (CLI Perspective)

```mermaid
flowchart TD
    start([üèÅ Start `flexseq <command>`]) --> config["üìù Load Configuration\n(YAML + Env Var + CLI Params)"];
    config --> op{‚öôÔ∏è Operation Type?};

    op -->|train| train_flow
    op -->|evaluate| eval_flow
    op -->|predict| predict_flow
    op -->|run| run_flow
    op -->|train-all-temps| train_all_flow
    op -->|compare-temperatures| compare_flow

    subgraph train_flow [Train Flow]
        direction LR
        tr_start(Train) --> tr_mode{Mode?};
        tr_mode -- FlexSeq --> tr_std_feats(Use Standard Features);
        tr_mode -- OmniFlex --> tr_adv_feats(Use Advanced Features);
        tr_std_feats --> tr_temp(Select Temperature);
        tr_adv_feats --> tr_temp;
        tr_temp --> tr_data(Load & Process Data);
        tr_data --> tr_split(Split Data);
        tr_split --> tr_models(Select Models);
        tr_models --> tr_hp_check{Optimize HParams?};
        tr_hp_check -- Yes --> tr_hp_opt(Hyperparameter Opt.);
        tr_hp_check -- No --> tr_train(Train Models);
        tr_hp_opt --> tr_train;
        tr_train --> tr_save(Save Models);
        tr_save --> tr_eval(Evaluate on Validation Set);
        tr_eval --> tr_end(End Train);
    end

    subgraph eval_flow [Evaluate Flow]
        direction LR
        ev_start(Evaluate) --> ev_mode{Mode?};
        ev_mode --> ev_temp(Select Temperature);
        ev_temp --> ev_load_data(Load & Process Data);
        ev_load_data --> ev_split(Split Data);
        ev_split -- Eval Set --> ev_load_models(Load Models);
        ev_load_models --> ev_predict(Generate Predictions);
        ev_predict --> ev_metrics(Calculate Metrics);
        ev_metrics --> ev_save(Save Results);
        ev_save --> ev_end(End Evaluate);
    end

    subgraph predict_flow [Predict Flow]
        direction LR
        pr_start(Predict) --> pr_mode{Mode?};
        pr_mode --> pr_temp(Select Temperature);
        pr_temp --> pr_input(Load & Process Input CSV);
        pr_input --> pr_load_model(Load Model);
        pr_load_model --> pr_predict(Generate Predictions);
        pr_predict --> pr_save(Save Output CSV);
        pr_save --> pr_end(End Predict);
    end

    subgraph run_flow [Run Flow]
        direction LR
        run_start(Run) --> run_train(Execute Train Flow);
        run_train --> run_eval(Execute Evaluate Flow);
        run_eval --> run_analyze(Analyze & Gen Viz Data);
        run_analyze --> run_end(End Run);
    end

     subgraph train_all_flow [Train All Temps Flow]
        direction LR
        tat_start(Train All) --> tat_loop{For each Temp in Config};
        tat_loop -- Loop --> tat_train(Execute Train Flow for Temp);
        tat_train -- Done --> tat_loop;
        tat_loop -- Finished --> tat_end(End Train All);
     end

     subgraph compare_flow [Compare Temps Flow]
        direction LR
        ct_start(Compare) --> ct_load(Load Results from All Temps);
        ct_load --> ct_analyze(Compare Metrics & Predictions);
        ct_analyze --> ct_save(Save Comparison Data);
        ct_save --> ct_end(End Compare);
     end

    tr_end --> finish([üèÅ Finish])
    ev_end --> finish
    pr_end --> finish
    run_end --> finish
    tat_end --> finish
    ct_end --> finish

    style start fill:#f9f9f9,stroke:#333,stroke-width:2px
    style finish fill:#f9f9f9,stroke:#333,stroke-width:2px
    style config fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    style op fill:#FFDAAB,stroke:#FF9933,stroke-width:2px
```

## üîß Installation

### Prerequisites
*   Python 3.8 or higher
*   pip (Python package installer)

### Install from Source (Recommended)
```bash
# 1. Clone the repository

cd flexseq

# 2. Install the package in editable mode
pip install -e .
```
*This installs the package such that changes to the source code are immediately reflected.*

### Dependencies
Core dependencies are managed by `setuptools` via `pyproject.toml` and `setup.py`. Key dependencies include:
`numpy`, `pandas`, `scikit-learn`, `torch`, `pyyaml`, `click`, `matplotlib`, `seaborn`, `joblib`, `tqdm`, `optuna`.

## üöÄ Quick Start

*(Run commands from the root directory: `/home/s_felix/flexseq`)*

### Basic Usage
```bash
# Train Random Forest model at 320K using default config
flexseq train --temperature 320 --model random_forest

# Evaluate the trained Random Forest model at 320K
flexseq evaluate --temperature 320 --model random_forest
# Check output in ./output/outputs_320/evaluation_results.csv

# Predict RMSF for new proteins at 320K using the best model
# (Assumes new_proteins.csv is in ./data and formatted correctly)
flexseq predict --input ./data/new_proteins.csv --temperature 320 --output ./output/new_proteins_pred_320.csv
```

### Advanced Usage
```bash
# Train Neural Network using OmniFlex mode at 348K
# (Requires esm_rmsf column in temperature_348_train.csv)
flexseq train --mode omniflex --temperature 348 --model neural_network

# Train enabled models on all available temperatures
flexseq train-all-temps

# Run the full pipeline (train, evaluate, analyze) for 379K
flexseq run --temperature 379

# Generate data comparing Random Forest results across all temperatures
flexseq compare-temperatures --model random_forest
# Check output in ./output/outputs_comparison/
```

## üì• Input Data

FlexSeq expects temperature-specific CSV files in the data directory (`./data` by default).

*   **File Naming:** Defined by `dataset.file_pattern` in config (e.g., `temperature_320_train.csv`).
*   **Required Columns for Training:** `domain_id`, `resid`, `resname`, `rmsf_{temperature}`.
*   **Optional/Recommended Columns:** `protein_size`, `normalized_resid`, `core_exterior`, `relative_accessibility`, `dssp`, `phi`, `psi`.
*   **OmniFlex Mode Columns:** `esm_rmsf` (required if `use_esm: true`), `voxel_rmsf` (required if `use_voxel: true`).

| Column                      | Description                                             | Type    | Example        | Notes                                       |
| :-------------------------- | :------------------------------------------------------ | :------ | :------------- | :------------------------------------------ |
| `domain_id`                 | Protein domain identifier                             | string  | `1a0aA00`      | Used for grouping and stratified splitting |
| `resid`                     | Residue ID (position in chain)                        | int     | `42`           |                                             |
| `resname`                   | 3-letter amino acid code                                | string  | `ALA`          |                                             |
| `rmsf_{temperature}`        | **Target:** RMSF value at specified temperature       | float   | `0.835`        | e.g., `rmsf_320` for T=320K                 |
| `protein_size`              | *Feature:* Total # residues in protein/domain         | int     | `153`          | Calculated if missing                       |
| `normalized_resid`          | *Feature:* Residue pos. normalized to 0-1             | float   | `0.274`        | Calculated if missing                       |
| `core_exterior`             | *Source:* Location ('interior' or 'surface')          | string  | `surface`      | Encoded to `core_exterior_encoded`        |
| `relative_accessibility`    | *Feature:* Solvent accessibility measure              | float   | `0.65`         | Typically 0-1                               |
| `dssp`                      | *Source:* Secondary structure (DSSP codes)            | string  | `H`, `E`, `C`  | Encoded to `secondary_structure_encoded`    |
| `phi`, `psi`                | *Source:* Backbone dihedral angles (degrees)          | float   | `-65.3`, `120.7` | Normalized to `phi_norm`, `psi_norm`        |
| `resname_encoded`           | *Feature:* Numerical encoding of `resname`            | int     | `1`            | Generated if `resname` present              |
| `core_exterior_encoded`     | *Feature:* Binary encoding (0=core, 1=surface)        | int     | `1`            | Generated if `core_exterior` present        |
| `secondary_structure_encoded`| *Feature:* Numerical encoding of `dssp` (0=H, 1=E, 2=Loop)| int| `0`            | Generated if `dssp` present                 |
| `phi_norm`, `psi_norm`      | *Feature:* Normalized angles [-1, 1]                  | float   | `-0.36`, `0.67` | Generated if `phi`/`psi` present           |
| `esm_rmsf`                  | *Feature (OmniFlex):* Prediction from ESM           | float   | `0.75`         | Required if `use_esm: true`               |
| `voxel_rmsf`                | *Feature (OmniFlex):* Prediction from Voxels        | float   | `0.81`         | Required if `use_voxel: true`             |

*The pipeline attempts data cleaning and feature generation (`flexseq/data/processor.py`). Missing optional source columns will prevent generation of derived features.*

## üì§ Output Data

Output files are saved to the configured `paths.output_dir` (default: `./output`), often within temperature-specific subdirectories (`outputs_{T}`).

| Output Type                 | Description                                             | Format | Default Path (`T`=temperature)                      |
| :-------------------------- | :------------------------------------------------------ | :----- | :-------------------------------------------------- |
| üíæ **Trained Models**       | Saved state of trained models                           | `.pkl` | `./models/models_{T}/{model_name}.pkl`              |
| üìä **Evaluation Metrics**   | Summary of performance metrics for each model           | CSV    | `./output/outputs_{T}/evaluation_results.csv`     |
| üìà **Detailed Results**     | Eval data + predictions + errors + uncertainty        | CSV    | `./output/outputs_{T}/all_results.csv`            |
| üß© **Domain Metrics**        | Performance metrics aggregated per domain               | CSV    | `./output/outputs_{T}/domain_metrics.csv`         |
| üîÆ **Predictions**          | Predictions on new input data                           | CSV    | `./output/{input_base}_predictions_{T}.csv`       |
| ‚≠ê **Feature Importance**   | Importance scores for each feature per model            | CSV, PNG| `./output/outputs_{T}/feature_importance/`        |
| üß¨ **Residue Analysis**     | Data for error analysis by AA, position, structure    | CSV, PNG| `./output/outputs_{T}/residue_analysis/`          |
| üå°Ô∏è **Temp Comparison**     | Combined results and metrics across temperatures        | CSV    | `./output/outputs_comparison/`                    |
| üìâ **Training History (NN)**| Epoch-wise loss/metrics for Neural Network              | CSV, PNG| `./output/outputs_{T}/neural_network_training_history.csv` |
| üìä **Visualization Data**   | Pre-formatted data for generating plots externally      | CSV    | `./output/outputs_{T}/visualization_data/`        |

*Note: Specific filenames for plots and analysis CSVs can be found in the `flexseq/utils/visualization.py` module.*

## ü§ñ Models

FlexSeq implements Random Forest and Neural Network models, configurable in the `models` section of the config YAML.

| Model             | Implementation             | Key Config Parameters (`models.{name}.*`)                                                                                                | Uncertainty Method               | Hyperparameter Optimization      |
| :---------------- | :------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------- | :----------------------------- |
| üå≤ **Random Forest** | `RandomForestModel`        | `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`, `bootstrap`, `randomized_search` (uses Scikit-learn's `RandomizedSearchCV`) | Variance across tree predictions | `RandomizedSearchCV` (built-in)|
| üß† **Neural Network**| `NeuralNetworkModel`       | `architecture` (`hidden_layers`, `activation`, `dropout`), `training` (`optimizer`, `learning_rate`, `batch_size`, `epochs`, `early_stopping`), `hyperparameter_optimization` (uses Optuna) | Monte Carlo Dropout sampling   | Optuna (Bayesian/Random/Grid)  |

*   See `flexseq/models/base.py` for the base class definition.
*   Model parameters and optimization settings are highly configurable (see `default_config.yaml`).

## üìà Analysis & Visualization

The pipeline focuses on generating structured CSV data to facilitate detailed analysis and external visualization, complementing the basic plots it generates directly.

**Generated Data/Plots Enable Analysis Of:**
*   **Overall Performance:** R¬≤, RMSE, MAE comparisons between models and across temperatures.
*   **Prediction Accuracy:** Scatter plots of Actual vs. Predicted RMSF, optionally with density contours or colored by residue properties.
*   **Error Analysis:** Distribution of errors (absolute, relative) grouped by amino acid type, secondary structure, sequence position, or surface exposure.
*   **Feature Contributions:** Ranked list/bar chart of feature importances (using permutation importance).
*   **Temperature Dependence:** Correlation of RMSF values between temperatures, trends of metrics vs. temperature, linear regression of RMSF change per residue.
*   **Model Behavior:** RMSF profiles along the sequence for specific domains, training/validation curves for Neural Networks.

*Refer to `flexseq/utils/visualization.py` for the functions generating plot data and basic plots.*

## ‚öôÔ∏è Configuration

Pipeline behavior is controlled via YAML configuration files.

*   **Default:** `default_config.yaml` (packaged with the library).
*   **User:** Specify a custom YAML via `flexseq <command> --config path/to/my_config.yaml`.
*   **Overrides:**
    *   **Environment Variables:** Prefix with `FLEXSEQ_`, use underscores for nesting (e.g., `FLEXSEQ_MODELS_RANDOM_FOREST_N_ESTIMATORS=500`).
    *   **CLI Parameters:** Use `--param key=value` (e.g., `--param dataset.split.test_size=0.25`). CLI overrides take highest precedence.

**Example Snippet (`default_config.yaml`):**
```yaml
# FlexSeq Configuration

paths:
  data_dir: ./data                # Data directory
  output_dir: ./output            # Output directory
  models_dir: ./models            # Saved models directory

mode:
  active: "omniflex"              # Using advanced mode ("flexseq" or "omniflex")
  omniflex:
    use_esm: true                 # Use ESM embeddings feature
    use_voxel: false              # Enable 3D voxel feature (if available)

temperature:
  current: 348                    # Current temperature to process
  available: [320, 348, 379, 413, 450, "average"] # Available datasets

dataset:
  file_pattern: "temperature_{temperature}_train.csv" # How to find data files
  features:
    use_features:                 # Features to use as model input
      protein_size: true
      normalized_resid: true
      relative_accessibility: true
      core_exterior_encoded: true
      secondary_structure_encoded: true
      phi_norm: true
      psi_norm: true
      resname_encoded: true
      esm_rmsf: true              # OmniFlex only (requires column in data)
      voxel_rmsf: false           # OmniFlex only (requires column in data)
    window:                       # Window feature settings
      enabled: true
      size: 5                     # Window = size*2 + 1 residues
  target: rmsf_{temperature}      # Target variable column name (templated)
  split:
    test_size: 0.2
    validation_size: 0.15
    stratify_by_domain: true      # Keep domains together during split

models:
  random_forest:
    enabled: true
    n_estimators: 500
    max_features: 0.7
    randomized_search:            # Hyperparameter optimization settings
      enabled: true
      n_iter: 50
      cv: 5
      param_distributions:        # Search space
        n_estimators: [100, 200, 300, 500, 800]
        # ... other parameters
  neural_network:
    enabled: true
    architecture:
      hidden_layers: [256, 128, 64]
    # ... other NN parameters and optimization settings
```
*Refer to the full `default_config.yaml` for all options.*

## üíª Command-Line Interface

The `flexseq` command provides structured access to pipeline functions.

| Command                 | Description                                              | Example                                              |
| :---------------------- | :------------------------------------------------------- | :--------------------------------------------------- |
| `train`                 | Train models for a specific temperature.                 | `flexseq train --temp 320 --model random_forest`     |
| `evaluate`              | Evaluate trained models.                                 | `flexseq evaluate --temp 320 --model random_forest`  |
| `predict`               | Generate predictions for new input data.                 | `flexseq predict --input new.csv --temp 320`         |
| `run`                   | Execute the full train, evaluate, analyze pipeline.      | `flexseq run --temp 348 --mode omniflex`             |
| `train-all-temps`       | Train models for all temperatures in `temperature.available`. | `flexseq train-all-temps`                            |
| `compare-temperatures`  | Generate data comparing results across temperatures.     | `flexseq compare-temperatures --model random_forest` |
| `preprocess`            | Only load, clean, and process data; save output.       | `flexseq preprocess --input raw.csv --out proc.csv`  |
| `list-models`           | List registered model names.                             | `flexseq list-models`                                |
| `list-temperatures`     | List temperatures defined in the configuration.          | `flexseq list-temperatures`                          |

**Common Options:** `--temperature` (`--temp`), `--model`, `--config`, `--param`, `--mode`, `--input`, `--output`. Use `flexseq <command> --help` for details.

## üìö Documentation

*(Placeholder links - adapt if full documentation exists)*
*   [Installation Guide](https://flexseq.readthedocs.io/en/latest/installation.html)
*   [User Guide](https://flexseq.readthedocs.io/en/latest/user_guide.html)
*   [API Reference](https://flexseq.readthedocs.io/en/latest/api.html)
*   [Examples](https://flexseq.readthedocs.io/en/latest/examples.html)

## üìù Citation

If you use FlexSeq in your research, please cite the repository:
```bibtex
@software{burton2023flexseq,
  author = {Burton, Felix},
  title = {FlexSeq: Protein Flexibility Prediction Pipeline},
  year = {2023},
  url = {
}
```

## ü§ù Contributing

Contributions are welcome! Please follow standard GitHub practices (fork, feature branch, pull request) or refer to the `CONTRIBUTING.md` file if present.

## üìÑ License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## üëè Acknowledgements

*   Developed by Felix Burton (Felix Burton).
*   Utilizes numerous open-source libraries including Scikit-learn, PyTorch, Pandas, NumPy, Matplotlib, Seaborn, Click, PyYAML, TQDM, Joblib, and Optuna.
```

### Core Module Files ###
---------------------------------------------------------
===== FILE: ensembleflex/config.py =====
# /home/s_felix/ensembleflex/ensembleflex/config.py

"""
Configuration handling for the ensembleflex ML pipeline.

Handles loading, validation, and management of configuration settings
for the unified temperature-aware model.
"""

import os
import logging
# from pathlib import Path # Not strictly needed anymore
from typing import Dict, Any, Optional, List, Union
import yaml
# import re # No longer needed for templating

logger = logging.getLogger(__name__)

def deep_merge(base_dict: Dict, overlay_dict: Dict) -> Dict:
    """
    Recursively merge two dictionaries, with values from overlay_dict taking precedence.

    Args:
        base_dict: Base dictionary to merge into
        overlay_dict: Dictionary with values that should override base_dict

    Returns:
        Dict containing merged configuration
    """
    result = base_dict.copy()

    for key, value in overlay_dict.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value

    return result

def get_env_var_config() -> Dict[str, Any]:
    """
    Get configuration from environment variables.
    Environment variables should be prefixed with ensembleflex_ and use
    underscore separators for nested keys.

    Examples:
        ensembleflex_PATHS_DATA_DIR=/path/to/data
        ensembleflex_MODELS_RANDOM_FOREST_N_ESTIMATORS=200

    Returns:
        Dict containing configuration from environment variables
    """
    config = {}
    prefix = "ensembleflex_" # Changed prefix

    for key, value in os.environ.items():
        if not key.startswith(prefix):
            continue

        # Remove prefix and convert to lowercase
        key = key[len(prefix):].lower()

        # Split into parts and create nested dict
        parts = key.split("_")
        current = config

        for part in parts[:-1]:
            if part not in current:
                current[part] = {}
            current = current[part]

        # Set value, converting to appropriate type
        value_part = parts[-1]

        # Try to convert to appropriate type
        if value.lower() == "true":
            value = True
        elif value.lower() == "false":
            value = False
        elif value.lower() == "null" or value.lower() == "none":
            value = None
        else:
            try:
                # Try int first, then float
                value = int(value)
            except ValueError:
                try:
                     value = float(value)
                except ValueError:
                     # Keep as string if not convertible
                     pass

        current[value_part] = value

    return config

def parse_param_overrides(params: List[str]) -> Dict[str, Any]:
    """
    Parse parameter overrides from CLI arguments.

    Args:
        params: List of parameter overrides in format "key=value"

    Returns:
        Dict containing parameter overrides
    """
    if not params:
        return {}

    override_dict = {}

    for param in params:
        if "=" not in param:
            logger.warning(f"Ignoring invalid parameter override: {param}")
            continue

        key, value_str = param.split("=", 1)
        value: Any = value_str # Keep type flexible initially

        # Convert value to appropriate type
        if value_str.lower() == "true":
            value = True
        elif value_str.lower() == "false":
            value = False
        elif value_str.lower() == "null" or value_str.lower() == "none":
            value = None
        else:
            try:
                # Try int first, then float
                value = int(value_str)
            except ValueError:
                try:
                    value = float(value_str)
                except ValueError:
                    # Keep as string if not convertible
                    value = value_str

        # Split key into parts and create nested dict
        parts = key.split(".")
        current = override_dict

        for part in parts[:-1]:
            # Ensure intermediate levels are dictionaries
            if part not in current or not isinstance(current[part], dict):
                current[part] = {}
            current = current[part]
            # Check if trying to assign value to a non-leaf node
            if not isinstance(current, dict):
                 logger.warning(f"Trying to override intermediate key '{part}' in '{key}' which is not a dictionary. Override might fail.")
                 break # Stop processing this override

        # Only assign if we reached the intended leaf node
        if isinstance(current, dict):
            current[parts[-1]] = value
        else:
            logger.warning(f"Could not apply override '{key}={value_str}' due to path conflict.")


    return override_dict

# REMOVED: template_config_for_temperature function

def load_config(
    config_path: Optional[str] = None,
    param_overrides: Optional[List[str]] = None,
    use_env_vars: bool = True,
    # temperature: Optional[Union[int, str]] = None # REMOVED temperature argument
) -> Dict[str, Any]:
    """
    Load configuration from default and user-provided sources.
    Handles merging of defaults, user file, environment variables,
    and command-line parameters.

    Args:
        config_path: Optional path to user config file.
        param_overrides: Optional list of parameter overrides from CLI.
        use_env_vars: Whether to use environment variables (ensembleflex_*).

    Returns:
        Dict containing the final merged configuration.

    Raises:
        FileNotFoundError: If default or specified user config file doesn't exist.
        ValueError: If configuration is invalid after merging.
    """
    # Determine default config path relative to this file's location
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # Project root
    default_path = os.path.join(base_dir, "default_config.yaml")

    # Load default config
    if not os.path.exists(default_path):
        raise FileNotFoundError(f"Default config not found at expected path: {default_path}")

    logger.debug(f"Loading default configuration from: {default_path}")
    with open(default_path, 'r') as f:
        config = yaml.safe_load(f)
    if config is None:
         config = {}
         logger.warning(f"Default config file '{default_path}' seems empty.")


    # Overlay user config if provided
    if config_path:
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"User config file not found at: {config_path}")
        logger.info(f"Loading user configuration from: {config_path}")
        with open(config_path, 'r') as f:
            user_config = yaml.safe_load(f)
            if user_config:
                config = deep_merge(config, user_config)
            else:
                 logger.warning(f"User config file '{config_path}' is empty.")


    # Apply environment variable overrides
    if use_env_vars:
        env_config = get_env_var_config()
        if env_config:
            logger.debug(f"Applying {len(env_config)} environment variable overrides.")
            config = deep_merge(config, env_config)
        else:
             logger.debug("No environment variable overrides found.")


    # Apply CLI parameter overrides
    if param_overrides:
        override_config = parse_param_overrides(param_overrides)
        if override_config:
            logger.debug(f"Applying {len(override_config)} CLI parameter overrides.")
            config = deep_merge(config, override_config)
        else:
            logger.debug("No valid CLI parameter overrides provided.")


    # REMOVED: Temperature override and templating logic

    # Handle OmniFlex mode settings (Ensure features are correctly toggled based on mode)
    # This needs to happen *after* all overrides are applied.
    try:
        if config.get("mode", {}).get("active", "standard").lower() == "omniflex":
            logger.info("OmniFlex mode active. Ensuring relevant features are enabled.")
            omniflex_cfg = config.get("mode", {}).get("omniflex", {})
            use_features = config.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})

            if omniflex_cfg.get("use_esm", False):
                if 'esm_rmsf' not in use_features: logger.warning("Enabling 'esm_rmsf' feature for OmniFlex mode.")
                use_features["esm_rmsf"] = True
            else:
                 if use_features.get("esm_rmsf", False): logger.info("Disabling 'esm_rmsf' feature as OmniFlex use_esm is false.")
                 use_features["esm_rmsf"] = False # Explicitly disable if OmniFlex doesn't want it


            if omniflex_cfg.get("use_voxel", False):
                 if 'voxel_rmsf' not in use_features: logger.warning("Enabling 'voxel_rmsf' feature for OmniFlex mode.")
                 use_features["voxel_rmsf"] = True
            else:
                 if use_features.get("voxel_rmsf", False): logger.info("Disabling 'voxel_rmsf' feature as OmniFlex use_voxel is false.")
                 use_features["voxel_rmsf"] = False # Explicitly disable

        elif config.get("mode", {}).get("active", "standard").lower() == "standard":
             logger.info("Standard mode active. Ensuring OmniFlex features are disabled.")
             use_features = config.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
             if use_features.get("esm_rmsf", False): logger.info("Disabling 'esm_rmsf' feature for Standard mode.")
             use_features["esm_rmsf"] = False
             if use_features.get("voxel_rmsf", False): logger.info("Disabling 'voxel_rmsf' feature for Standard mode.")
             use_features["voxel_rmsf"] = False

    except Exception as e:
         logger.error(f"Error applying OmniFlex mode settings: {e}", exc_info=True)


    # Validate final config
    validate_config(config)

    # Set system-wide logging level
    try:
        log_level_str = config.get("system", {}).get("log_level", "INFO").upper()
        numeric_level = getattr(logging, log_level_str, None)
        if isinstance(numeric_level, int):
            # Set level for the root logger and potentially specific package loggers
            logging.getLogger().setLevel(numeric_level)
            logging.getLogger("ensembleflex").setLevel(numeric_level) # Set for our package too
            logger.info(f"Logging level set to {log_level_str}")
        else:
             logger.warning(f"Invalid log level '{log_level_str}' in config. Using default.")

    except Exception as e:
         logger.warning(f"Could not set logging level from config: {e}")


    return config

def validate_config(config: Dict[str, Any]) -> None:
    """
    Perform basic validation of the merged ensembleflex configuration.

    Args:
        config: Configuration dictionary to validate

    Raises:
        ValueError: If configuration is invalid
    """
    logger.debug("Validating final configuration...")
    # Check required top-level sections
    required_sections = ["paths", "dataset", "models", "evaluation", "system", "temperature", "mode"]
    for section in required_sections:
        if section not in config:
            raise ValueError(f"Missing required config section: '{section}'")

    # Validate dataset section
    dataset_cfg = config.get("dataset", {})
    if "target" not in dataset_cfg:
        raise ValueError("Missing required 'dataset.target' configuration")
    if dataset_cfg["target"] != "rmsf":
        logger.warning(f"Expected 'dataset.target' to be 'rmsf', but found '{dataset_cfg['target']}'.")
    if "file_pattern" not in dataset_cfg:
        raise ValueError("Missing required 'dataset.file_pattern' configuration")
    if "{temperature}" in dataset_cfg["file_pattern"]:
        logger.warning(f"Found '{{temperature}}' in 'dataset.file_pattern'. This is likely incorrect for ensembleflex aggregated data.")

    # Validate features section
    features_cfg = dataset_cfg.get("features", {})
    if "required" not in features_cfg:
        raise ValueError("Missing required 'dataset.features.required' configuration")
    required_list = features_cfg["required"]
    if "rmsf" not in required_list:
        logger.warning("Target 'rmsf' not listed in 'dataset.features.required'.")
    if "temperature" not in required_list:
        logger.warning("Input feature 'temperature' not listed in 'dataset.features.required'.")

    if "use_features" not in features_cfg:
        raise ValueError("Missing required 'dataset.features.use_features' configuration")
    if "temperature" not in features_cfg["use_features"]:
        logger.warning("Feature toggle 'dataset.features.use_features.temperature' is missing. It might not be included as input.")

    # REMOVED: Temperature section validation for current/available consistency

    # Check paths
    paths_cfg = config.get("paths", {})
    if "output_dir" not in paths_cfg or "{temperature}" in paths_cfg["output_dir"]:
         logger.warning(f"Path 'paths.output_dir' might be misconfigured: {paths_cfg.get('output_dir')}")
    if "models_dir" not in paths_cfg or "{temperature}" in paths_cfg["models_dir"]:
         logger.warning(f"Path 'paths.models_dir' might be misconfigured: {paths_cfg.get('models_dir')}")


    # Check that at least one model is enabled
    any_model_enabled = False
    for model_name, model_config in config.get("models", {}).items():
        if model_name != "common" and isinstance(model_config, dict) and model_config.get("enabled", False):
            any_model_enabled = True
            break

    if not any_model_enabled:
        logger.warning("No models are enabled in configuration ('models.*.enabled: true'). No training will occur.")

    # Add more specific checks as needed (e.g., data types, valid choices)
    logger.debug("Configuration validation passed.")


def get_enabled_models(config: Dict[str, Any]) -> List[str]:
    """
    Get list of enabled model names from config.

    Args:
        config: Configuration dictionary

    Returns:
        List of enabled model names
    """
    enabled_models = []

    for model_name, model_config in config.get("models", {}).items():
        # Ensure model_config is a dictionary and has 'enabled' key
        if model_name != "common" and isinstance(model_config, dict) and model_config.get("enabled", False):
            enabled_models.append(model_name)

    return enabled_models

def get_model_config(config: Dict[str, Any], model_name: str) -> Dict[str, Any]:
    """
    Get configuration for a specific model, with common settings applied.

    Args:
        config: Full configuration dictionary
        model_name: Name of the model

    Returns:
        Model-specific configuration with common settings merged in

    Raises:
        ValueError: If model_name is not found in configuration
    """
    models_config = config.get("models", {})

    if model_name not in models_config:
        raise ValueError(f"Model '{model_name}' not found in configuration")

    model_config = models_config[model_name]
    if not isinstance(model_config, dict):
         logger.warning(f"Configuration for model '{model_name}' is not a dictionary. Using empty config.")
         model_config = {}

    common_config = models_config.get("common", {})
    if not isinstance(common_config, dict):
         logger.warning("Section 'models.common' is not a dictionary. Using empty common config.")
         common_config = {}


    # Merge common config with model-specific config
    merged_config = deep_merge(common_config, model_config)

    return merged_config

def get_available_temperatures(config: Dict[str, Any]) -> List[Union[int, str]]:
    """
    Get list of available temperatures from config (informational only).

    Args:
        config: Configuration dictionary

    Returns:
        List of available temperatures defined in the config
    """
    return config.get("temperature", {}).get("available", [])

# --- ADAPTED Path Functions ---
# These now ignore any temperature argument and return the static paths

def get_output_dir(config: Dict[str, Any]) -> str:
    """
    Get the unified output directory path.

    Args:
        config: Configuration dictionary

    Returns:
        Path to the output directory.
    """
    return config.get("paths", {}).get("output_dir", "./output/ensembleflex/") # Provide default

def get_models_dir(config: Dict[str, Any]) -> str:
    """
    Get the unified models directory path.

    Args:
        config: Configuration dictionary

    Returns:
        Path to the models directory.
    """
    return config.get("paths", {}).get("models_dir", "./models/ensembleflex/") # Provide default


===== FILE: ensembleflex/pipeline.py =====
# /home/s_felix/ensembleflex/ensembleflex/pipeline.py

"""
Main pipeline orchestration for the ensembleflex ML workflow.

Handles training, evaluation, prediction, and analysis for the
single, unified, temperature-aware model using aggregated data.
"""

import os
import logging
from typing import Dict, List, Tuple, Optional, Any, Union

import pandas as pd
import numpy as np
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch
import time
import inspect
# import joblib # Keep if models use joblib
from ensembleflex.models.neural_network import NeuralNetworkModel
from ensembleflex.models.random_forest import RandomForestModel
from ensembleflex.models.lightgbm import LightGBMModel
from ensembleflex.models.lightgbm_classifier import LightGBMClassifier



# Updated imports for ensembleflex structure
from ensembleflex.config import (
    load_config,
    get_enabled_models,
    get_model_config,
    get_output_dir,  # UPDATED
    get_models_dir   # UPDATED
)
from ensembleflex.utils.helpers import progress_bar, ProgressCallback, ensure_dir
from ensembleflex.models import get_model_class, BaseModel # Import BaseModel for type hint
from ensembleflex.data.processor import (
    load_file,
    load_and_process_data,
    split_data,
    prepare_data_for_model,
    process_features
)

from ensembleflex.utils.metrics import evaluate_predictions
# Import visualization functions that will be added/adapted
from ensembleflex.utils.visualization import (
    plot_feature_importance,
    plot_scatter_with_density_contours,
    # plot_residue_level_rmsf, # Keep if useful for single domain viz
    plot_amino_acid_error_analysis,
    plot_error_analysis_by_property,
    plot_prediction_vs_temperature, # NEW
    plot_error_vs_temperature,      # NEW
    plot_training_validation_curves # Keep for NN
)

logger = logging.getLogger(__name__)

class Pipeline:
    """
    Main pipeline orchestration for ensembleflex.
    Handles the full ML workflow for the unified temperature-aware model.
    """
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the pipeline with configuration.

        Args:
            config: Configuration dictionary.
        """
        self.config = config
        self.models: Dict[str, BaseModel] = {} # Stores the single trained model instance {model_name: model_object}
        self.output_dir = get_output_dir(config)
        self.models_dir = get_models_dir(config)
        self.prepare_directories() # Use updated paths

        mode = config.get("mode", {}).get("active", "standard")
        logger.info(f"Pipeline initialized in {mode.upper()} mode.")
        logger.info(f"Output directory: {self.output_dir}")
        logger.info(f"Models directory: {self.models_dir}")



    def prepare_directories(self) -> None:
        """Create necessary output directories using unified paths."""
        paths = self.config["paths"]
        data_dir = paths.get("data_dir", "./data") # Keep for data loading consistency
        ensure_dir(data_dir)
        ensure_dir(self.output_dir)
        ensure_dir(self.models_dir)

        # Create subdirectories within the unified output directory
        analysis_subdirs = [
            "feature_importance",
            "residue_analysis",
            "domain_analysis", # Keep if domain-level metrics are still generated
            "comparisons", # Keep for plots like scatter comparisons
            "training_performance", # For NN history
            "temperature_analysis" # NEW subdir for temp-specific plots
        ]
        for subdir in analysis_subdirs:
            ensure_dir(os.path.join(self.output_dir, subdir))


    # def load_data(self, data_path: Optional[str] = None) -> pd.DataFrame:
    #     """
    #     Loads and processes the single aggregated input data file.

    #     Args:
    #         data_path: Optional explicit path to override config's file_pattern.

    #     Returns:
    #         Processed DataFrame.
    #     """
    #     # Determine input data path (CLI override or config default)
    #     effective_data_path = data_path
    #     if not effective_data_path:
    #         data_dir = self.config["paths"]["data_dir"]
    #         file_pattern = self.config["dataset"]["file_pattern"] # Should be the aggregated file name
    #         effective_data_path = os.path.join(data_dir, file_pattern)
    #         logger.info(f"Loading data from config path: {effective_data_path}")
    #     else:
    #          logger.info(f"Loading data from provided path: {effective_data_path}")

    #     if not os.path.exists(effective_data_path):
    #          raise FileNotFoundError(f"Data file not found: {effective_data_path}")

    #     # Call processor's main loading function
    #     return load_and_process_data(data_path=effective_data_path, config=self.config)
    def load_data(self, data_path: Optional[str] = None) -> pd.DataFrame:
            """
            Loads and processes the single aggregated input data file.

            Args:
                data_path: Optional explicit path to override config's file_pattern.

            Returns:
                Processed DataFrame.
            """
            # Determine input data path (CLI override or config default)
            effective_data_input: Union[str, None] = data_path # Start with optional path override
            if not effective_data_input:
                # If no path override, construct path from config
                data_dir = self.config["paths"]["data_dir"]
                file_pattern = self.config["dataset"]["file_pattern"] # Should be the aggregated file name
                effective_data_input = os.path.join(data_dir, file_pattern)
                logger.info(f"Loading data from config path: {effective_data_input}")
            else:
                logger.info(f"Loading data from provided path: {effective_data_input}")

            return load_and_process_data(effective_data_input, config=self.config)


    def train(
        self,
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None
    ) -> Dict[str, BaseModel]:
        """
        Train the single specified model on the aggregated data.

        Handles passing validation data for early stopping and feature names
        to the model's fit method if supported.

        Args:
            model_names: List containing the single model name to train.
                         If None, uses the first enabled model from config.
            data_path: Optional explicit path to the aggregated data file.

        Returns:
            Dictionary containing the trained model instance: {model_name: model_object},
            or an empty dictionary if training fails.
        """
        # 1. Determine the single model to train
        if not model_names:
            model_names = get_enabled_models(self.config)
            if not model_names:
                logger.error("No models enabled in configuration. Cannot train.")
                return {}
            model_name_to_train = model_names[0]
            logger.info(f"No model specified, training first enabled model: {model_name_to_train}")
        elif len(model_names) > 1:
             model_name_to_train = model_names[0]
             logger.warning(f"Multiple models specified ({model_names}). Training only the first: {model_name_to_train}")
        else:
             model_name_to_train = model_names[0]

        # 2. Load and preprocess the full aggregated data
        logger.info("Loading and processing aggregated data...")
        try:
            with ProgressCallback(total=1, desc="Loading data") as pbar:
                df = self.load_data(data_path=data_path)
                pbar.update()
            if df.empty:
                logger.error("Loaded data is empty. Cannot train.")
                return {}
        except FileNotFoundError as e:
             logger.error(f"Data loading failed: {e}")
             return {}
        except Exception as e:
             logger.error(f"Data loading/processing failed: {e}", exc_info=True)
             return {}


        # 3. Split data into train/validation/test sets once
        logger.info("Splitting data into train/validation/test sets...")
        try:
            with ProgressCallback(total=1, desc="Splitting data") as pbar:
                train_df, val_df, test_df = split_data(df, self.config)
                pbar.update()
            if train_df.empty:
                 logger.error("Training split is empty after splitting data. Cannot train.")
                 return {}
            # val_df can be empty if validation_size is 0 or very small
            if val_df.empty:
                 logger.warning("Validation split is empty. Early stopping based on validation data will be disabled.")

        except Exception as e:
             logger.error(f"Data splitting failed: {e}", exc_info=True)
             return {}


        # 4. Prepare training and validation data (features X and target y)
        logger.info("Preparing features for training and validation...")
        try:
            X_train, y_train, feature_names = prepare_data_for_model(train_df, self.config, include_target=True)
            X_val, y_val = None, None
            if not val_df.empty:
                X_val, y_val, _ = prepare_data_for_model(val_df, self.config, include_target=True)
            else:
                 logger.info("Skipping validation data preparation as validation set is empty.")


            # Sanity check features
            if X_train.size == 0 or y_train.size == 0:
                 logger.error("Training data (X_train or y_train) is empty after preparation. Cannot train.")
                 return {}
            if self.config['dataset']['features']['use_features'].get('temperature', False):
                 if 'temperature' not in feature_names:
                      logger.error("Config enables temperature feature, but it's missing from prepared training data!")
                      # Optionally raise error: raise ValueError("Temp feature missing")
                 else: logger.info(f"Training with {len(feature_names)} features, including 'temperature'.")
            else: logger.info(f"Training with {len(feature_names)} features (temp excluded).")

        except Exception as e:
             logger.error(f"Feature preparation failed: {e}", exc_info=True)
             return {}


        # 5. --- Train the Single Model ---
        trained_models: Dict[str, BaseModel] = {}
        logger.info(f"--- Training Model: {model_name_to_train} ---")

        try:
            model_class = get_model_class(model_name_to_train)
            model_config = get_model_config(self.config, model_name_to_train) # Merges common and specific
            if not model_config.get("enabled", False):
                logger.error(f"Model {model_name_to_train} is not enabled in the final config. Cannot train."); return {}

            # Instantiate the model
            model = model_class(**model_config)

            # --- HPO Check (Informational) ---
            if (model_name_to_train == "neural_network" and model_config.get("hyperparameter_optimization", {}).get("enabled", False)) or \
               (model_name_to_train == "random_forest" and model_config.get("randomized_search", {}).get("enabled", False)):
                logger.warning("HPO enabled. Ensure it's handled within model's .fit() or done prior.")

            # --- Fit the model ---
            start_time = time.time()
            logger.info("Fitting model...")

            # Prepare keyword arguments for model.fit dynamically
            fit_kwargs = {}
            fit_signature = inspect.signature(model.fit)

            # Pass validation data if fit accepts it and data exists
            if 'X_val' in fit_signature.parameters and 'y_val' in fit_signature.parameters:
                 if X_val is not None and y_val is not None:
                      fit_kwargs['X_val'] = X_val
                      fit_kwargs['y_val'] = y_val
                      logger.debug(f"Passing validation data to {model_name_to_train}.fit.")
                 else:
                      logger.debug(f"{model_name_to_train}.fit accepts validation data, but validation set was empty.")

            # Pass feature names if fit accepts it
            if 'feature_names' in fit_signature.parameters:
                 fit_kwargs['feature_names'] = feature_names
                 logger.debug(f"Passing feature_names to {model_name_to_train}.fit.")

            # Call fit with appropriate arguments
            model.fit(X_train, y_train, **fit_kwargs)

            train_time = time.time() - start_time
            logger.info(f"Finished training {model_name_to_train} in {train_time:.2f} seconds")

            # Store the successfully trained model
            trained_models[model_name_to_train] = model
            self.models[model_name_to_train] = model # Cache in pipeline instance

            # --- Post-Training Steps ---

            # Save Training History (NN specific usually)
            if hasattr(model, 'get_training_history') and callable(model.get_training_history):
                history = model.get_training_history()
                if history:
                    try:
                        history_df = pd.DataFrame(history)
                        perf_dir = os.path.join(self.output_dir, "training_performance")
                        ensure_dir(perf_dir)
                        history_path = os.path.join(perf_dir, f"{model_name_to_train}_training_history.csv")
                        history_df.to_csv(history_path, index=False)
                        logger.info(f"Saved training history to {history_path}")
                        # Plot curves
                        curve_plot_path = os.path.join(perf_dir, f"{model_name_to_train}_training_curves.png")
                        plot_training_validation_curves(history, history, model_name_to_train, curve_plot_path) # Pass history dict directly
                    except Exception as hist_e:
                        logger.warning(f"Could not save/plot training history: {hist_e}", exc_info=True)

            # Save the trained model object
            if model_config.get("save_best", True):
                with ProgressCallback(total=1, desc=f"Saving {model_name_to_train}") as pbar:
                    self.save_model(model, model_name_to_train)
                    pbar.update()

            # Evaluate on Validation Set (if validation set exists)
            if X_val is not None and y_val is not None:
                logger.info("Evaluating model performance on validation set...")
                with ProgressCallback(total=1, desc=f"Validating {model_name_to_train}") as pbar:
                    val_predictions = model.predict(X_val)
                    n_features_train = X_train.shape[1]
                    val_metrics = evaluate_predictions(y_val, val_predictions, self.config, X=X_val, n_features=n_features_train)
                    logger.info(f"Validation metrics for {model_name_to_train}: {val_metrics}")
                    # Save validation metrics
                    val_metrics_df = pd.DataFrame([val_metrics], index=[model_name_to_train])
                    val_metrics_df.index.name = "model"
                    val_metrics_path = os.path.join(self.output_dir, f"{model_name_to_train}_validation_metrics.csv")
                    try:
                         val_metrics_df.to_csv(val_metrics_path)
                         logger.info(f"Saved validation metrics to {val_metrics_path}")
                    except Exception as e:
                         logger.error(f"Failed to save validation metrics: {e}")
                    pbar.update()
            else:
                 logger.info("Skipping evaluation on validation set as it was empty.")


        except Exception as e:
            logger.error(f"Error during training process for {model_name_to_train}: {e}", exc_info=True)
            # Return empty dict if training failed catastrophically
            return {}

        return trained_models


    def save_model(self, model: BaseModel, model_name: str) -> None:
            """
            Save the single trained model to the unified models directory.

            Args:
                model: Trained model instance.
                model_name: Name of the model.
            """
            # Use the unified models directory stored in self.models_dir
            # Determine file extension - default to .pkl, use .pt if it's a torch Module
            if isinstance(model.model, torch.nn.Module): # Check the actual model object inside the wrapper
                model_filename = f"{model_name}.pt"
            else:
                model_filename = f"{model_name}.pkl"

            model_path = os.path.join(self.models_dir, model_filename)

            try:
                # The model's save method should handle the actual saving logic
                model.save(model_path) # The model's save method receives the full path including extension
                logger.info(f"Saved model '{model_name}' to: {model_path}")
            except AttributeError:
                logger.error(f"Model object for '{model_name}' does not have a 'save' method.")
            except Exception as e:
                logger.error(f"Error saving model '{model_name}' to {model_path}: {e}", exc_info=True)


    def load_model(self, model_name: str) -> BaseModel:
        """
        Load the single trained model from the unified models directory.

        Args:
            model_name: Name of the model to load.

        Returns:
            Loaded model instance.
        """
        # Use the unified models directory stored in self.models_dir
        # Try common extensions
        possible_extensions = ['.pkl', '.pt'] # Add more if needed
        model_path = None
        for ext in possible_extensions:
             path_try = os.path.join(self.models_dir, f"{model_name}{ext}")
             if os.path.exists(path_try):
                  model_path = path_try
                  break

        if model_path is None:
            raise FileNotFoundError(f"Model file for '{model_name}' not found in {self.models_dir} with extensions {possible_extensions}")

        logger.info(f"Loading model '{model_name}' from: {model_path}")
        try:
            model_class = get_model_class(model_name)
            # The model's load method should handle instantiation and state loading
            model = model_class.load(model_path)
            logger.info(f"Successfully loaded model '{model_name}'.")
            return model
        except AttributeError:
             logger.error(f"Model class for '{model_name}' does not have a 'load' classmethod.")
             raise
        except Exception as e:
            logger.error(f"Error loading model '{model_name}' from {model_path}: {e}", exc_info=True)
            raise


    def evaluate(
        self,
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None
    ) -> Dict[str, Dict[str, float]]:
        """
        Evaluate the single trained model on the test split of the aggregated data.
        Includes enhanced logging for feature mismatch debugging.

        Args:
            model_names: List containing the single model name to evaluate.
                         If None, uses the first enabled model from config.
            data_path: Optional explicit path to the aggregated data file.

        Returns:
            Dictionary containing evaluation metrics for the model: {model_name: metrics_dict}.
        """
        # 1. Determine Model to Evaluate
        if not model_names:
            model_names = get_enabled_models(self.config)
            if not model_names:
                logger.error("No models enabled in configuration. Cannot evaluate.")
                return {}
            model_name_to_eval = model_names[0]
            logger.info(f"No model specified, evaluating first enabled model: {model_name_to_eval}")
        elif len(model_names) > 1:
             model_name_to_eval = model_names[0]
             logger.warning(f"Multiple models specified ({model_names}). Evaluating only: '{model_name_to_eval}'")
        else:
             model_name_to_eval = model_names[0]

        # 2. Load Data and Get Evaluation Split
        logger.info("Loading aggregated data for evaluation split...")
        try:
            with ProgressCallback(total=1, desc="Loading data") as pbar:
                df = self.load_data(data_path=data_path)
                pbar.update()
            if df.empty: raise ValueError("Loaded data is empty.")

            with ProgressCallback(total=1, desc="Splitting data") as pbar:
                _, val_df, test_df = split_data(df, self.config) # Get both val and test splits
                pbar.update()

            comparison_set_name = self.config["evaluation"].get("comparison_set", "test").lower()
            if comparison_set_name == "validation":
                eval_df = val_df
                logger.info("Using validation set for evaluation.")
            else:
                if comparison_set_name != "test": logger.warning(f"Unknown comparison_set, using test.")
                eval_df = test_df
                logger.info("Using test set for evaluation.")

            if eval_df.empty: raise ValueError(f"{comparison_set_name} set is empty.")

        except (FileNotFoundError, ValueError, Exception) as e:
             logger.error(f"Failed to load or split data for evaluation: {e}", exc_info=True)
             return {}

        # 3. Prepare Evaluation Data (X, y, feature_names)
        logger.info(f"Preparing {comparison_set_name} features for evaluation...")
        try:
            with ProgressCallback(total=1, desc="Preparing features") as pbar:
                # This is where feature names for evaluation are generated
                X_eval, y_eval, feature_names_eval = prepare_data_for_model(eval_df, self.config, include_target=True)
                pbar.update()
            if X_eval.size == 0 or y_eval.size == 0:
                raise ValueError("Evaluation data (X or y) is empty after preparation.")
            logger.debug(f"Features prepared for EVALUATION ({len(feature_names_eval)}): {feature_names_eval}") # Log prepared features

        except Exception as e:
             logger.error(f"Feature preparation failed for evaluation: {e}", exc_info=True)
             return {}

        # 4. Evaluate the Single Model
        results = {}
        predictions = {}
        uncertainties = {}
        logger.info(f"--- Evaluating Model: {model_name_to_eval} ---")

        try:
            # Load model
            with ProgressCallback(total=1, desc=f"Loading {model_name_to_eval}", leave=False) as pbar:
                model = self.models.get(model_name_to_eval) or self.load_model(model_name_to_eval)
                self.models[model_name_to_eval] = model # Cache loaded model
                pbar.update()

            # --- START: Feature Consistency Check ---
            if hasattr(model, 'feature_names_') and model.feature_names_:
                expected_features = model.feature_names_
                logger.debug(f"Model '{model_name_to_eval}' expects features ({len(expected_features)}): {expected_features}")

                if feature_names_eval != expected_features:
                    logger.error(f"FATAL: Feature mismatch detected for model '{model_name_to_eval}'!")
                    logger.error(f"  Data has {len(feature_names_eval)} features: {feature_names_eval}")
                    logger.error(f"  Model expects {len(expected_features)} features: {expected_features}")

                    # Detailed comparison (optional, can be verbose)
                    set_eval = set(feature_names_eval)
                    set_expected = set(expected_features)
                    missing_in_eval = set_expected - set_eval
                    extra_in_eval = set_eval - set_expected
                    if missing_in_eval:
                        logger.error(f"  Features MISSING in evaluation data: {sorted(list(missing_in_eval))}")
                    if extra_in_eval:
                        logger.error(f"  Features EXTRA in evaluation data: {sorted(list(extra_in_eval))}")
                    if len(feature_names_eval) == len(expected_features) and feature_names_eval != expected_features:
                         logger.error("  Feature ORDER differs.")

                    # Option 1: Raise an error to stop execution
                    raise ValueError(f"Feature mismatch for model '{model_name_to_eval}'. Cannot proceed.")
                    # Option 2: Return empty results (current behavior if error is caught below)
                    # return {}
                else:
                     logger.info(f"Feature consistency check passed for model '{model_name_to_eval}'.")

            else:
                 logger.warning(f"Loaded model '{model_name_to_eval}' does not have 'feature_names_' attribute. Cannot perform consistency check.")
            # --- END: Feature Consistency Check ---


            # Generate predictions
            logger.info("Generating predictions on evaluation set...")
            with ProgressCallback(total=1, desc=f"Predicting", leave=False) as pbar:
                if hasattr(model, 'predict_with_std') and callable(model.predict_with_std):
                    preds, stds = model.predict_with_std(X_eval)
                    uncertainties[model_name_to_eval] = stds
                else:
                    preds = model.predict(X_eval)
                predictions[model_name_to_eval] = preds
                pbar.update()

            # Calculate metrics
            logger.info("Calculating evaluation metrics...")
            with ProgressCallback(total=1, desc="Computing metrics", leave=False) as pbar:
                n_features = X_eval.shape[1] # Use actual shape of X_eval
                metrics = evaluate_predictions(y_eval, preds, self.config, X=X_eval, n_features=n_features)
                pbar.update()

            results[model_name_to_eval] = metrics
            logger.info(f"Evaluation metrics for {model_name_to_eval}: {metrics}")

        except FileNotFoundError as e:
             logger.error(f"Could not load model '{model_name_to_eval}' for evaluation: {e}")
             return {}
        except ValueError as ve: # Catch the explicit ValueError from the feature check
             logger.error(f"Evaluation aborted due to error: {ve}")
             return {}
        except Exception as e:
            logger.error(f"Error during evaluation process for {model_name_to_eval}: {e}", exc_info=True)
            return {} # Return empty if evaluation fails

        # 5. Save evaluation results
        logger.info("Saving evaluation results...")
        with ProgressCallback(total=1, desc="Saving results") as pbar:
            # Pass original eval_df (contains feature names if derived from pandas)
            # Pass predictions dict and uncertainties dict
            self.save_evaluation_results(results, eval_df, predictions, uncertainties)
            pbar.update()

        return results


    def save_evaluation_results(
        self,
        results: Dict[str, Dict[str, float]], # {model_name: metrics_dict}
        eval_df: pd.DataFrame, # The original eval split df (test or val)
        predictions: Dict[str, np.ndarray], # {model_name: predictions_array}
        uncertainties: Dict[str, np.ndarray] # {model_name: uncertainty_array}
    ) -> None:
        """
        Save evaluation results (metrics and detailed predictions) to the unified output directory.

        Args:
            results: Dictionary of evaluation metrics for the evaluated model.
            eval_df: DataFrame with the evaluation data (features and actual target).
            predictions: Dictionary of predictions by model name.
            uncertainties: Dictionary of prediction uncertainties by model name.
        """
        # Save metrics summary to CSV
        results_path = os.path.join(self.output_dir, "evaluation_results.csv")
        try:
            results_summary_df = pd.DataFrame(results).T # Transpose to have models as rows
            results_summary_df.index.name = "model"
            results_summary_df.to_csv(results_path)
            logger.info(f"Saved evaluation metrics summary to: {results_path}")
        except Exception as e:
             logger.error(f"Failed to save evaluation metrics summary: {e}")


        # Save detailed results (original eval data + predictions + errors + uncertainty)
        if predictions:
            # Make a copy to avoid modifying the original eval_df slice
            # Ensure index alignment if eval_df was sliced/diced earlier
            all_results_df = eval_df.copy()
            target_col = self.config["dataset"]["target"] # Should be 'rmsf'

            # Add predictions and calculate errors for each model evaluated (usually just one)
            for model_name, preds in predictions.items():
                pred_col_name = f"{model_name}_predicted"
                error_col_name = f"{model_name}_error"
                abs_error_col_name = f"{model_name}_abs_error"

                # Assign predictions based on index
                all_results_df[pred_col_name] = pd.Series(preds, index=all_results_df.index)

                # Calculate errors (ensure target column exists)
                if target_col in all_results_df.columns:
                    all_results_df[error_col_name] = all_results_df[pred_col_name] - all_results_df[target_col]
                    all_results_df[abs_error_col_name] = np.abs(all_results_df[error_col_name])
                else:
                     logger.warning(f"Target column '{target_col}' not found in eval_df. Cannot calculate errors.")
                     all_results_df[error_col_name] = np.nan
                     all_results_df[abs_error_col_name] = np.nan


                # Add uncertainties if available
                if uncertainties and model_name in uncertainties:
                    uncertainty_col_name = f"{model_name}_uncertainty"
                    all_results_df[uncertainty_col_name] = pd.Series(uncertainties[model_name], index=all_results_df.index)

            # Save the combined DataFrame
            all_results_path = os.path.join(self.output_dir, "all_results.csv")
            try:
                all_results_df.to_csv(all_results_path, index=False)
                logger.info(f"Saved detailed evaluation results to: {all_results_path}")

                # Optionally save domain-level metrics based on this detailed file
                domain_metrics_path = os.path.join(self.output_dir, "domain_analysis", "domain_metrics.csv")
                self.save_domain_metrics(all_results_df, target_col, list(predictions.keys()), domain_metrics_path)

            except Exception as e:
                 logger.error(f"Failed to save detailed evaluation results: {e}")

        else:
             logger.warning("No predictions provided to save_evaluation_results. Skipping detailed results saving.")


    def save_domain_metrics(
        self,
        results_df: pd.DataFrame,
        target_col: str,
        model_names: List[str],
        output_path: str
    ) -> None:
        """
        Calculate and save domain-level metrics from detailed results.

        Args:
            results_df: DataFrame with all results (including predictions/errors).
            target_col: Target column name ('rmsf').
            model_names: List of model names present in results_df.
            output_path: Full path to save the domain metrics CSV.
        """
        if 'domain_id' not in results_df.columns:
             logger.warning("Cannot calculate domain metrics: 'domain_id' column missing.")
             return

        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

        domain_metrics = []
        logger.info("Calculating domain-level metrics...")

        for domain_id, domain_df in progress_bar(results_df.groupby("domain_id", observed=False), desc="Domain Metrics"):
            if domain_df.empty: continue
            domain_result = {"domain_id": domain_id}

            # Get unique residue count for this domain
            num_unique_residues = domain_df['resid'].nunique()
            domain_result["num_unique_residues"] = num_unique_residues
            domain_result["num_rows"] = len(domain_df) # Rows = residues * temps

            # Add avg temp if temp column exists
            if 'temperature' in domain_df.columns:
                 domain_result["avg_temperature"] = domain_df['temperature'].mean()

            # Calculate metrics for each model
            for model_name in model_names:
                pred_col = f"{model_name}_predicted"
                error_col = f"{model_name}_error"
                abs_error_col = f"{model_name}_abs_error"

                if pred_col not in domain_df.columns or target_col not in domain_df.columns:
                    logger.debug(f"Skipping metrics for model {model_name} in domain {domain_id} (missing columns).")
                    # Add NaN placeholders for consistent columns
                    domain_result[f"{model_name}_rmse"] = np.nan
                    domain_result[f"{model_name}_mae"] = np.nan
                    domain_result[f"{model_name}_r2"] = np.nan
                    if error_col in domain_df.columns:
                        domain_result[f"{model_name}_mean_error"] = np.nan
                        domain_result[f"{model_name}_std_error"] = np.nan
                    continue

                # Drop rows where either actual or prediction is NaN for metric calculation
                valid_rows = domain_df[[target_col, pred_col]].dropna()
                if valid_rows.empty:
                     logger.debug(f"Skipping metrics for model {model_name} in domain {domain_id} (no valid rows).")
                     rmse, mae, r2 = np.nan, np.nan, np.nan
                else:
                    actual = valid_rows[target_col].values
                    predicted = valid_rows[pred_col].values
                    try:
                         rmse = np.sqrt(mean_squared_error(actual, predicted))
                         mae = mean_absolute_error(actual, predicted)
                         # R2 requires variance in actual values
                         r2 = r2_score(actual, predicted) if np.var(actual) > 1e-9 else np.nan
                    except ValueError:
                         rmse, mae, r2 = np.nan, np.nan, np.nan


                # Store metrics
                domain_result[f"{model_name}_rmse"] = rmse
                domain_result[f"{model_name}_mae"] = mae
                domain_result[f"{model_name}_r2"] = r2

                # Basic error stats if error columns exist
                if error_col in domain_df.columns:
                     # Calculate mean/std only on non-NaN errors
                     valid_errors = domain_df[error_col].dropna()
                     domain_result[f"{model_name}_mean_error"] = valid_errors.mean() if not valid_errors.empty else np.nan
                     domain_result[f"{model_name}_std_error"] = valid_errors.std() if len(valid_errors) > 1 else 0.0


            domain_metrics.append(domain_result)

        # Save domain metrics to CSV
        if domain_metrics:
            domain_metrics_df = pd.DataFrame(domain_metrics)
            try:
                ensure_dir(os.path.dirname(output_path))
                domain_metrics_df.to_csv(output_path, index=False)
                logger.info(f"Saved domain-level metrics to: {output_path}")
            except Exception as e:
                 logger.error(f"Failed to save domain metrics: {e}")
        else:
             logger.warning("No domain metrics were calculated.")

    def predict(
        self,
        data: Union[str, pd.DataFrame],
        temperature: float,  # REQUIRED prediction temperature
        model_name: Optional[str] = None,
        with_uncertainty: bool = False,
        batch_size: int = 10000,  # Added batch processing
        check_temp_range: bool = True  # Added temperature validation
    ) -> Tuple[pd.DataFrame, Optional[Dict[str, float]]]:
        """
        Enhanced version: Generate predictions for new data at a specific prediction temperature.
        
        Improvements:
        - Batch processing for large datasets
        - Temperature range validation
        - Better error handling
        - Progress tracking for large datasets

        Args:
            data: DataFrame or path to CSV file with protein data features.
                May optionally contain 'rmsf' and 'temperature' columns.
            temperature: The target temperature (K) for which to predict.
            model_name: Specific model to use (if None, finds best/first enabled).
            with_uncertainty: Whether to include uncertainty estimates.
            batch_size: Number of rows to process at once for large datasets.
            check_temp_range: Whether to validate if temperature is in training range.

        Returns:
            Tuple containing:
            - DataFrame with identifiers, prediction temp, prediction, original temp (if present),
            true target (if present), errors (if present), uncertainty (if requested).
            - Dictionary of evaluation metrics calculated ONLY on the subset where
            original input temperature == prediction temperature (if possible), else None.
        """
        logger.info(f"Starting prediction for prediction_temperature: {temperature} K")
        target_col_name = self.config["dataset"]["target"]  # 'rmsf'
        original_temp_col_name = 'temperature'  # Standard name for original temp feature
        pred_col_name = f"{target_col_name}_predicted"
        unc_col_name = f"{target_col_name}_uncertainty"
        error_col_name = f"{target_col_name}_error"
        abs_error_col_name = f"{target_col_name}_abs_error"

        # --- 1. Load and Process Input Data ---
        input_df_contains_target = False
        input_df_contains_orig_temp = False
        input_df_raw: Optional[pd.DataFrame] = None

        if isinstance(data, str):
            logger.debug(f"Loading prediction input data from path: {data}")
            try:
                input_df_raw = load_file(data)  # Load raw first
                if target_col_name in input_df_raw.columns: input_df_contains_target = True
                if original_temp_col_name in input_df_raw.columns: input_df_contains_orig_temp = True
                logger.info(f"Input file contains target: {input_df_contains_target}, original temperature: {input_df_contains_orig_temp}.")
                
                # Check dataset size for potential batching
                large_dataset = len(input_df_raw) > batch_size
                if large_dataset:
                    logger.info(f"Large dataset detected: {len(input_df_raw)} rows. Will process in batches of {batch_size}.")
                
                df_processed = load_and_process_data(input_df_raw, self.config)
            except Exception as e:
                logger.error(f"Error loading/processing input file {data}: {e}", exc_info=True)
                raise ValueError(f"Failed to load/process input file {data}") from e
        elif isinstance(data, pd.DataFrame):
            logger.debug("Processing provided DataFrame for prediction...")
            if target_col_name in data.columns: input_df_contains_target = True
            if original_temp_col_name in data.columns: input_df_contains_orig_temp = True
            logger.info(f"Input DataFrame contains target: {input_df_contains_target}, original temperature: {input_df_contains_orig_temp}.")
            input_df_raw = data.copy()
            df_processed = process_features(input_df_raw.copy(), self.config)
        else:
            raise TypeError("Input 'data' must be a file path (str) or a pandas DataFrame.")

        if not isinstance(input_df_raw, pd.DataFrame):
            logger.error("Failed to obtain a valid DataFrame from input.")
            raise ValueError("Could not process input data into a DataFrame.")

        # --- 2. Determine and Load Model ---
        if model_name is None:
            try:  # Find best model logic...
                results_path = os.path.join(self.output_dir, "evaluation_results.csv")
                if os.path.exists(results_path):
                    results_df = pd.read_csv(results_path, index_col="model")
                    if not results_df.empty:
                        if "r2" in results_df.columns and results_df["r2"].notna().any(): model_name = results_df["r2"].idxmax()
                        elif "rmse" in results_df.columns and results_df["rmse"].notna().any(): model_name = results_df["rmse"].idxmin()
                        else: model_name = results_df.index[0]
                        logger.info(f"Using best model: '{model_name}'")
                    else: raise ValueError("Eval file empty.")
                else: raise FileNotFoundError
            except (FileNotFoundError, IndexError, ValueError, Exception) as e:
                logger.warning(f"Could not determine best model ({e}). Falling back.")
                enabled_models = get_enabled_models(self.config)
                if not enabled_models: raise RuntimeError("No model specified/enabled.")
                model_name = enabled_models[0]
                logger.info(f"Using first enabled: '{model_name}'")

        logger.info(f"Loading model '{model_name}' for prediction.")
        model = self.models.get(model_name) or self.load_model(model_name)
        self.models[model_name] = model

        # --- 3. Temperature Range Validation (New) ---
        if check_temp_range and input_df_contains_orig_temp:
            # Get temperature range from training data
            available_temps = input_df_raw[original_temp_col_name].dropna().unique()
            if len(available_temps) > 0:
                min_temp, max_temp = np.min(available_temps), np.max(available_temps)
                # Add a small buffer (5% of range)
                temp_range = max_temp - min_temp
                buffer = temp_range * 0.05 if temp_range > 0 else 10.0
                
                if temperature < min_temp - buffer or temperature > max_temp + buffer:
                    logger.warning(f"Requested temperature {temperature}K is outside training range "
                                f"({min_temp:.1f}K - {max_temp:.1f}K ¬± {buffer:.1f}K buffer). "
                                f"Predictions may be less reliable.")

        # --- 4. Prepare Feature Matrix & Augment Temperature ---
        logger.debug("Preparing feature matrix for prediction...")
        X_input, _, feature_names = prepare_data_for_model(
            df_processed, self.config, include_target=False
        )
        temp_feature_enabled = self.config['dataset']['features']['use_features'].get('temperature', False)
        
        if temp_feature_enabled:
            try:
                temp_feature_index = feature_names.index(original_temp_col_name)  # Use standard temp col name
                logger.debug(f"Found temperature feature at index {temp_feature_index}")
            except ValueError:
                logger.error(f"'{original_temp_col_name}' required but missing from prepared features!")
                raise ValueError(f"Feature mismatch: '{original_temp_col_name}' missing.")
        else: 
            logger.warning("Predicting, but model not trained with temperature feature.")
            temp_feature_index = None
            
        # --- 5. Generate Predictions (with batching) ---
        logger.info(f"Generating predictions with model '{model_name}'...")
        
        # Determine if batching is needed
        large_dataset = len(X_input) > batch_size
        
        predictions_list = []
        uncertainties_list = []
        
        if large_dataset:
            # Process in batches
            num_batches = int(np.ceil(len(X_input) / batch_size))
            for i in range(num_batches):
                start_idx = i * batch_size
                end_idx = min((i + 1) * batch_size, len(X_input))
                logger.info(f"Processing batch {i+1}/{num_batches} (rows {start_idx}-{end_idx})")
                
                # Get batch and set temperature
                X_batch = X_input[start_idx:end_idx].copy()
                
                # Set temperature for this batch
                if temp_feature_enabled and temp_feature_index is not None:
                    X_batch[:, temp_feature_index] = temperature
                    
                # Generate predictions for batch
                try:
                    if with_uncertainty and hasattr(model, 'predict_with_std') and callable(model.predict_with_std):
                        batch_preds, batch_uncs = model.predict_with_std(X_batch)
                        predictions_list.append(batch_preds)
                        uncertainties_list.append(batch_uncs)
                    else:
                        if with_uncertainty: 
                            logger.warning(f"Uncertainty requested, but not supported.")
                        batch_preds = model.predict(X_batch)
                        predictions_list.append(batch_preds)
                except Exception as e:
                    logger.error(f"Batch prediction failed: {e}", exc_info=True)
                    # Continue with next batch instead of failing completely
                    predictions_list.append(np.full(end_idx - start_idx, np.nan))
                    if with_uncertainty:
                        uncertainties_list.append(np.full(end_idx - start_idx, np.nan))
        else:
            # Process entire dataset at once
            X_augmented = X_input.copy()
            if temp_feature_enabled and temp_feature_index is not None:
                X_augmented[:, temp_feature_index] = temperature
                
            try:
                if with_uncertainty and hasattr(model, 'predict_with_std') and callable(model.predict_with_std):
                    full_preds, full_uncs = model.predict_with_std(X_augmented)
                    predictions_list.append(full_preds)
                    uncertainties_list.append(full_uncs)
                else:
                    if with_uncertainty: 
                        logger.warning(f"Uncertainty requested, but not supported.")
                    full_preds = model.predict(X_augmented)
                    predictions_list.append(full_preds)
            except Exception as e:
                logger.error(f"Prediction generation failed: {e}", exc_info=True)
                raise

        # Combine predictions from batches
        if predictions_list:
            predictions_array = np.concatenate(predictions_list) if len(predictions_list) > 1 else predictions_list[0]
            uncertainties_array = None
            if uncertainties_list:
                uncertainties_array = np.concatenate(uncertainties_list) if len(uncertainties_list) > 1 else uncertainties_list[0]
        else:
            raise RuntimeError("No predictions were generated")

        # --- 6. Construct Result DataFrame ---
        logger.debug("Constructing result DataFrame...")
        id_cols = ['domain_id', 'resid', 'resname']
        cols_to_copy = id_cols[:]
        if input_df_contains_orig_temp: cols_to_copy.append(original_temp_col_name)
        if input_df_contains_target: cols_to_copy.append(target_col_name)

        result_df_base = pd.DataFrame(index=range(len(predictions_array)))
        if all(c in input_df_raw.columns for c in id_cols):
            temp_ids_df = input_df_raw[id_cols].copy().reset_index(drop=True)
            if len(temp_ids_df) == len(predictions_array):
                result_df_base = temp_ids_df
            else: logger.warning("ID length mismatch.")
        else: logger.warning("Input missing IDs.")

        # Add optional columns if they exist in raw input and length matches
        for col in cols_to_copy:
            if col not in id_cols and col in input_df_raw.columns and len(input_df_raw) == len(predictions_array):
                result_df_base[col] = input_df_raw[col].values
            elif col not in id_cols and col not in result_df_base.columns: # Add as NaN if missing entirely
                result_df_base[col] = np.nan

        # Add prediction temperature and predictions
        result_df_base['prediction_temperature'] = temperature
        result_df_base[pred_col_name] = predictions_array
        if uncertainties_array is not None: result_df_base[unc_col_name] = uncertainties_array

        # Calculate errors *if* target was present
        if input_df_contains_target:
            result_df_base[error_col_name] = result_df_base[pred_col_name] - result_df_base[target_col_name]
            result_df_base[abs_error_col_name] = np.abs(result_df_base[error_col_name])

        # --- Calculate Metrics ONLY on Matching Temperatures ---
        metrics = None
        if input_df_contains_target and input_df_contains_orig_temp:
            logger.info(f"Calculating metrics comparing predictions @{temperature}K vs true values @{temperature}K...")
            # Filter the results to rows where original temp == prediction temp
            # Use np.isclose for robust float comparison
            matching_temp_df = result_df_base[np.isclose(result_df_base[original_temp_col_name], temperature)]

            if not matching_temp_df.empty:
                # Drop NaNs from target and prediction within this subset
                eval_subset = matching_temp_df[[target_col_name, pred_col_name]].dropna()
                if not eval_subset.empty and len(eval_subset) > 1:
                    y_true_eval = eval_subset[target_col_name].values
                    y_pred_eval = eval_subset[pred_col_name].values
                    # Need to prepare X subset for metrics requiring features (e.g., Adj R2)
                    # This requires re-preparing features for the matching_temp_df indices
                    # For simplicity now, we won't pass X to evaluate_predictions here
                    # If Adj R2 is critical, this needs refinement
                    n_feat_for_metrics = len(feature_names) # Number of features model was trained on
                    metrics = evaluate_predictions(y_true_eval, y_pred_eval, self.config, n_features=n_feat_for_metrics)
                    logger.info(f"Prediction Metrics (for T={temperature}K only): {metrics}")
                else:
                    logger.warning(f"Not enough valid data points found where original temperature == {temperature}K for metric calculation.")
            else:
                logger.warning(f"No rows found in input where original temperature == {temperature}K. Cannot calculate specific metrics.")
        # --- End Metrics Calculation ---

        # Reorder columns for clarity
        final_cols_order = ['domain_id', 'resid', 'resname']
        if original_temp_col_name in result_df_base.columns: final_cols_order.append(original_temp_col_name)
        if target_col_name in result_df_base.columns: final_cols_order.append(target_col_name)
        final_cols_order.append('prediction_temperature')
        final_cols_order.append(pred_col_name)
        if unc_col_name in result_df_base.columns: final_cols_order.append(unc_col_name)
        if error_col_name in result_df_base.columns: final_cols_order.append(error_col_name)
        if abs_error_col_name in result_df_base.columns: final_cols_order.append(abs_error_col_name)
        final_cols_order = [col for col in final_cols_order if col in result_df_base.columns]
        result_df_final = result_df_base[final_cols_order]

        logger.info(f"Prediction completed successfully. Generated {len(result_df_final)} predictions.")
        return result_df_final, metrics

    def analyze(
        self,
        model_name: Optional[str] = None,
        results_df: Optional[pd.DataFrame] = None
    ) -> None:
        """
        Perform analysis of the single trained model's results.
        Focuses on feature importance (including temperature) and error analysis
        as a function of the input temperature feature.

        Args:
            model_name: Name of the model to analyze (if None, finds first enabled).
            results_df: Optional DataFrame containing detailed evaluation results
                        (e.g., from `all_results.csv`). If None, it will be loaded.
        """
        logger.info("--- Starting Model Analysis ---")

        # 1. Determine Model Name & Load Model/Results
        # (This part remains the same as the previous corrected version)
        model: Optional[BaseModel] = None
        try:
            if not model_name:
                enabled_models = get_enabled_models(self.config)
                if not enabled_models: raise ValueError("No models enabled.")
                model_name = enabled_models[0]; logger.info(f"Analyzing first enabled: {model_name}")
            else: logger.info(f"Analyzing model: {model_name}")

            model = self.models.get(model_name) or self.load_model(model_name)
            self.models[model_name] = model # Cache

            if results_df is None:
                results_path = os.path.join(self.output_dir, "all_results.csv")
                if not os.path.exists(results_path): raise FileNotFoundError(f"Results missing: {results_path}")
                logger.info(f"Loading results: {results_path}")
                results_df = pd.read_csv(results_path)

            target_col=self.config['dataset']['target']; pred_col=f"{model_name}_predicted"
            if not all(c in results_df.columns for c in ['temperature',target_col,pred_col]):
                raise ValueError(f"Results missing essential columns.")
            abs_error_col = f"{model_name}_abs_error"
            if abs_error_col not in results_df.columns:
                results_df.loc[:, abs_error_col] = (results_df[pred_col] - results_df[target_col]).abs()
        except (FileNotFoundError, ValueError, Exception) as e:
            logger.error(f"Analysis setup failed: {e}", exc_info=True); return

        # --- 2. Feature Importance Analysis ---
        analysis_cfg = self.config.get("analysis", {})
        importance_cfg = analysis_cfg.get("feature_importance", {})
        if importance_cfg.get("enabled", False):
            logger.info("Calculating feature importance...")
            X_eval, y_eval, feature_names = None, None, None
            importance_dict = None # Initialize importance_dict
            try:
                # Prepare data
                use_val_data = importance_cfg.get("use_validation_data", True)
                eval_set_name = "validation" if use_val_data else "test"
                split_set_index = 1 if use_val_data else 2
                logger.debug(f"Preparing {eval_set_name} data for importance...")
                full_df = self.load_data(); splits = split_data(full_df, self.config)
                eval_df_for_importance = splits[split_set_index]

                if not eval_df_for_importance.empty:
                    # *** Get feature names during data preparation ***
                    X_eval, y_eval, feature_names = prepare_data_for_model(eval_df_for_importance, self.config)
                else: logger.warning(f"{eval_set_name} set empty.")

                if X_eval is not None and y_eval is not None and feature_names:
                    importance_method_cfg = importance_cfg.get("method", "permutation")
                    n_repeats_cfg = importance_cfg.get("n_repeats", 10)
                    importance_values = None

                    # Call get_feature_importance correctly
                    logger.debug(f"Calling importance (method='{importance_method_cfg}') for {type(model).__name__}")
                    if not hasattr(model, 'get_feature_importance'):
                         logger.warning(f"Model missing get_feature_importance.")
                    else:
                         sig = inspect.signature(model.get_feature_importance)
                         params = sig.parameters; call_kwargs = {}
                         # Pass arguments accepted by the specific model's method
                         if 'X_val' in params: call_kwargs['X_val'] = X_eval
                         if 'y_val' in params: call_kwargs['y_val'] = y_eval
                         if 'method' in params: call_kwargs['method'] = importance_method_cfg
                         if 'n_repeats' in params: call_kwargs['n_repeats'] = n_repeats_cfg
                         try: importance_values = model.get_feature_importance(**call_kwargs)
                         except Exception as fe_e: logger.error(f"Call failed: {fe_e}", exc_info=True)

                    # *** Process results AND map to feature names ***
                    if importance_values is not None:
                        # If it's an array, map it using the feature_names list
                        if isinstance(importance_values, np.ndarray):
                             if len(importance_values) == len(feature_names):
                                 importance_dict = dict(zip(feature_names, importance_values.astype(float)))
                                 logger.debug("Mapped importance array to feature names.")
                             else:
                                 logger.warning(f"Importance array length ({len(importance_values)}) != feature names length ({len(feature_names)}). Cannot map names.")
                        elif isinstance(importance_values, dict):
                             importance_dict = importance_values # Assume already mapped
                             logger.debug("Importance already returned as dictionary.")
                        else: logger.warning("Importance format unexpected.")

                        # Plotting/Saving uses the dictionary with correct names
                        if importance_dict:
                            importance_dir = os.path.join(self.output_dir, "feature_importance"); ensure_dir(importance_dir)
                            plot_path = os.path.join(importance_dir, f"{model_name}_feature_importance.png")
                            csv_path = os.path.join(importance_dir, f"{model_name}_feature_importance.csv")
                            # plot_feature_importance now receives the dict with correct names
                            plot_feature_importance(importance_dict, plot_path, csv_path)
                            if 'temperature' in importance_dict: logger.info(f"Importance of 'temperature': {importance_dict['temperature']:.4f}")
                        else: logger.warning(f"Failed to create importance dictionary for {model_name}.")
                    else: logger.warning(f"Could not calculate feature importance for {model_name}.")
                else: logger.warning("Data preparation failed for importance.")
            except Exception as e: logger.error(f"Error during feature importance step: {e}", exc_info=True)
        else: logger.info("Feature importance analysis skipped (config).")

        # --- 3. Error Analysis vs. Temperature ---
        # (This part remains the same as previous correction)
        logger.info("Analyzing performance vs. input temperature feature...")
        temp_analysis_dir = os.path.join(self.output_dir, "temperature_analysis"); ensure_dir(temp_analysis_dir)
        pred_vs_temp_path = os.path.join(temp_analysis_dir, f"{model_name}_prediction_vs_temp.png")
        error_vs_temp_plot_path = os.path.join(temp_analysis_dir, f"{model_name}_error_vs_temp.png")
        error_vs_temp_csv_path = os.path.join(temp_analysis_dir, f"{model_name}_error_vs_temp_binned.csv")
        try: plot_prediction_vs_temperature(results_df, model_name, pred_vs_temp_path)
        except Exception as e: logger.error(f"Pred vs Temp plot failed: {e}", exc_info=True)
        try: plot_error_vs_temperature(results_df, model_name, self.config, error_vs_temp_plot_path, error_vs_temp_csv_path)
        except Exception as e: logger.error(f"Error vs Temp plot failed: {e}", exc_info=True)

        # --- 4. Standard Residue/Property Analysis ---
        # (This part remains the same)
        logger.info("Performing standard residue/property error analysis...")
        residue_analysis_dir = os.path.join(self.output_dir, "residue_analysis"); ensure_dir(residue_analysis_dir)
        aa_error_csv_path = os.path.join(residue_analysis_dir, f"{model_name}_amino_acid_errors.csv")
        aa_error_plot_path = os.path.join(residue_analysis_dir, f"{model_name}_amino_acid_errors.png")
        prop_analysis_base_path = os.path.join(residue_analysis_dir, f"{model_name}_error_by")
        try: plot_amino_acid_error_analysis(results_df, model_name, target_col, aa_error_csv_path, aa_error_plot_path)
        except Exception as e: logger.error(f"AA error analysis failed: {e}", exc_info=True)
        try: plot_error_analysis_by_property(results_df, model_name, target_col, prop_analysis_base_path)
        except Exception as e: logger.error(f"Prop error analysis failed: {e}", exc_info=True)

        # --- 5. Scatter Plot ---
        # (This part remains the same)
        scatter_dir = os.path.join(self.output_dir, "comparisons"); ensure_dir(scatter_dir)
        scatter_plot_path = os.path.join(scatter_dir, f"{model_name}_actual_vs_predicted_scatter.png")
        scatter_csv_path = os.path.join(scatter_dir, f"{model_name}_actual_vs_predicted_data.csv")
        try: plot_scatter_with_density_contours(results_df, model_name, target_col, scatter_plot_path, scatter_csv_path)
        except Exception as e: logger.error(f"Scatter plot failed: {e}", exc_info=True)

        logger.info("--- Model Analysis Complete ---")


    def run_pipeline(
        self,
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None,
        skip_analysis: bool = False # Flag name matches CLI
    ) -> Dict[str, Dict[str, float]]:
        """
        Run the complete pipeline: train, evaluate, and analyze the single model.

        Args:
            model_names: List containing the single model name to use.
            data_path: Optional explicit path to the aggregated data file.
            skip_analysis: Whether to skip the analysis step.

        Returns:
            Dictionary of evaluation metrics for the trained model, or empty if failed.
        """
        final_results = {}
        model_to_run = None
        train_successful = False
        eval_successful = False

        try:
            # --- Train ---
            logger.info("--- Starting Training Phase ---")
            trained_models_dict = self.train(model_names, data_path)
            if trained_models_dict:
                model_to_run = list(trained_models_dict.keys())[0]
                train_successful = True
                logger.info(f"--- Training Phase Complete for {model_to_run} ---")
            else:
                logger.error("Training phase failed or returned no models. Aborting pipeline.")
                return {}

            # --- Evaluate ---
            logger.info("--- Starting Evaluation Phase ---")
            eval_results = self.evaluate(model_names=[model_to_run], data_path=data_path)
            if eval_results and model_to_run in eval_results:
                final_results = eval_results
                eval_successful = True
                logger.info(f"--- Evaluation Phase Complete for {model_to_run} ---")
            else:
                logger.error(f"Evaluation phase failed to produce results for model {model_to_run}.")
                # Decide if pipeline should continue to analysis despite failed eval
                # For now, we will stop analysis if eval fails

            # --- Analyze ---
            if not skip_analysis and eval_successful:
                logger.info("--- Starting Analysis Phase ---")
                # Load the detailed results saved by evaluate step
                results_df_path = os.path.join(self.output_dir, "all_results.csv")
                results_df_for_analysis = None
                if os.path.exists(results_df_path):
                    try:
                        results_df_for_analysis = pd.read_csv(results_df_path)
                        logger.debug("Loaded detailed results file for analysis.")
                    except Exception as e:
                        logger.error(f"Failed to load detailed results file '{results_df_path}' for analysis: {e}")

                if results_df_for_analysis is not None:
                    self.analyze(model_name=model_to_run, results_df=results_df_for_analysis)
                    logger.info("--- Analysis Phase Complete ---")
                else:
                    logger.error("Skipping analysis because detailed results file could not be loaded.")
            elif skip_analysis:
                logger.info("--- Analysis Phase Skipped ---")
            else: # Analysis not skipped, but eval failed
                 logger.warning("--- Analysis Phase Skipped due to evaluation failure ---")


        except Exception as pipeline_error:
             logger.error(f"Pipeline execution failed: {pipeline_error}", exc_info=True)
             return final_results # Return whatever eval results were obtained before the crash

        logger.info("--- Pipeline Run Finished ---")
        return final_results
===== FILE: ensembleflex/cli.py =====
# /home/s_felix/ensembleflex/ensembleflex/cli.py

"""
Command-line interface for the EnsembleFlex ML pipeline.

Handles training, evaluation, prediction, and analysis for the
single, unified, temperature-aware model.
"""

import os
import sys
import logging
from typing import List, Optional, Tuple, Dict, Any # Removed Union

import click
import pandas as pd
import numpy as np
from ensembleflex.utils.helpers import ensure_dir



# Updated imports for ensembleflex structure
from ensembleflex.config import (
    load_config,
    get_enabled_models,
    # get_model_config, # Potentially needed by specific commands if not using Pipeline methods directly
    # get_available_temperatures, # No longer needed for driving runs
    get_output_dir,  # UPDATED
    get_models_dir,  # UPDATED
    # get_comparison_output_dir # Can use get_output_dir + "/comparison" if needed
)
from ensembleflex.pipeline import Pipeline
from ensembleflex.models import get_available_models # Keep this for listing

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def parse_model_list(model_arg: Optional[str]) -> List[str]:
    """
    Parse comma-separated list of models.

    Args:
        model_arg: Comma-separated model names or None

    Returns:
        List of model names
    """
    if not model_arg:
        return []

    return [m.strip() for m in model_arg.split(",")]

@click.group()
@click.version_option(package_name="ensembleflex", version="0.1.0") # Updated package name
def cli():
    """
    ensembleflex: ML pipeline for temperature-aware protein flexibility prediction.

    Trains a single, unified model on aggregated data across temperatures.
    Requires temperature input for predictions.
    """
    pass

# --- Modified `train` Command ---
@cli.command()
@click.option("--model",
              help="Model to train (e.g., random_forest, neural_network). Defaults to enabled models in config.")
@click.option("--config",
              type=click.Path(exists=True, dir_okay=False),
              help="Path to custom YAML config file.")
@click.option("--param",
              multiple=True,
              help="Override config parameter (e.g. models.random_forest.n_estimators=200)")
# --input option might be less relevant if config specifies the aggregated file, but keep for flexibility
@click.option("--input",
              type=click.Path(exists=True, dir_okay=False),
              help="Override input data file (CSV). Defaults to config dataset.file_pattern.")
# REMOVED: --temperature option
@click.option("--mode",
              type=click.Choice(["standard", "omniflex"], case_sensitive=False),
              help="Override operation mode (standard or omniflex).")
def train(
    model, config, param, input, mode
):
    """
    Train the unified flexibility prediction model on aggregated data.
    Uses settings from default_config.yaml, overridden by --config, env vars, and --param.

    Examples:
        ensembleflex train
        ensembleflex train --model random_forest
        ensembleflex train --mode standard
        ensembleflex train --input path/to/my_aggregated_data.csv
        ensembleflex train --param models.neural_network.training.epochs=50
    """
    # Load configuration (No temperature override needed here)
    cfg = load_config(config_path=config, param_overrides=param)

    # Set mode if specified via CLI
    if mode:
        cfg["mode"]["active"] = mode.lower()
        logger.info(f"Mode overridden by CLI: {cfg['mode']['active']}")
        # Re-apply mode logic after potential override
        try:
            if cfg["mode"]["active"] == "omniflex":
                omniflex_cfg = cfg.get("mode", {}).get("omniflex", {})
                use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                use_features["esm_rmsf"] = omniflex_cfg.get("use_esm", False)
                use_features["voxel_rmsf"] = omniflex_cfg.get("use_voxel", False)
            else: # Standard mode
                 use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                 use_features["esm_rmsf"] = False
                 use_features["voxel_rmsf"] = False
        except Exception as e:
            logger.error(f"Error applying mode override settings: {e}", exc_info=True)


    # Determine which models to train (usually just one based on config)
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)

    if not model_list:
        click.echo(click.style("Error: No models specified via --model or enabled in config.", fg="red"))
        sys.exit(1)
    if len(model_list) > 1:
         click.echo(click.style(f"Warning: Multiple models enabled/specified ({model_list}). ensembleflex trains one unified model. Using first: '{model_list[0]}'", fg="yellow"))
         model_list = [model_list[0]]


    # Get unified output/models directories from config
    output_dir = get_output_dir(cfg)
    models_dir = get_models_dir(cfg)

    # Create directories if they don't exist
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(models_dir, exist_ok=True)
    click.echo(f"Using Output Directory: {os.path.abspath(output_dir)}")
    click.echo(f"Using Models Directory: {os.path.abspath(models_dir)}")

    # Determine input data path
    data_path = input # Use CLI input if provided
    if not data_path:
        # Construct path from config if CLI input not given
        data_dir = cfg["paths"]["data_dir"]
        file_pattern = cfg["dataset"]["file_pattern"]
        data_path = os.path.join(data_dir, file_pattern)
        click.echo(f"Using Input Data from config: {os.path.abspath(data_path)}")
    else:
        click.echo(f"Using Input Data from CLI: {os.path.abspath(data_path)}")


    # Create pipeline and train the single model
    try:
        pipeline = Pipeline(cfg) # Config now uses static paths
        click.echo(f"Training model: {model_list[0]}...")
        trained_models = pipeline.train(model_names=model_list, data_path=data_path) # Pass data_path explicitly

        if trained_models:
            click.echo(click.style(f"Successfully trained model: {list(trained_models.keys())[0]}", fg="green"))
        else:
             click.echo(click.style("Training finished, but no models were returned by the pipeline.", fg="yellow"))

    except FileNotFoundError as e:
         click.echo(click.style(f"Error: Input data file not found. {e}", fg="red"))
         sys.exit(1)
    except Exception as e:
        logger.error(f"Error during training: {e}", exc_info=True)
        click.echo(click.style(f"An error occurred during training: {e}", fg="red"))
        sys.exit(1)


# --- Modified `evaluate` Command ---
@cli.command()
@click.option("--model",
              help="Model to evaluate (e.g., random_forest). Defaults to first enabled model.")
@click.option("--config",
              type=click.Path(exists=True, dir_okay=False),
              help="Path to custom YAML config file.")
@click.option("--param",
              multiple=True,
              help="Override config parameter.")
@click.option("--input",
              type=click.Path(exists=True, dir_okay=False),
              help="Override input data file (CSV) for evaluation. Defaults to config.")
# REMOVED: --temperature option
@click.option("--mode",
              type=click.Choice(["standard", "omniflex"], case_sensitive=False),
              help="Override operation mode (standard or omniflex).")
def evaluate(model, config, param, input, mode):
    """
    Evaluate the trained unified model on the test split of aggregated data.
    """
     # Load configuration
    cfg = load_config(config_path=config, param_overrides=param)

    # Set mode if specified via CLI
    if mode:
        cfg["mode"]["active"] = mode.lower()
        logger.info(f"Mode overridden by CLI: {cfg['mode']['active']}")
        # Re-apply mode logic (copied from train)
        try:
            if cfg["mode"]["active"] == "omniflex":
                omniflex_cfg = cfg.get("mode", {}).get("omniflex", {})
                use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                use_features["esm_rmsf"] = omniflex_cfg.get("use_esm", False)
                use_features["voxel_rmsf"] = omniflex_cfg.get("use_voxel", False)
            else: # Standard mode
                 use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                 use_features["esm_rmsf"] = False
                 use_features["voxel_rmsf"] = False
        except Exception as e:
            logger.error(f"Error applying mode override settings: {e}", exc_info=True)

    # Determine which model to evaluate
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)
        if model_list:
             model_to_eval = model_list[0]
             logger.info(f"No model specified via --model, using first enabled model: {model_to_eval}")
        else:
             click.echo(click.style("Error: No model specified or enabled in config to evaluate.", fg="red"))
             sys.exit(1)
    else:
         model_to_eval = model_list[0]
         if len(model_list) > 1:
              click.echo(click.style(f"Warning: Multiple models specified ({model_list}). Evaluating only the first: '{model_to_eval}'", fg="yellow"))

    # Get unified output/models directories
    output_dir = get_output_dir(cfg)
    models_dir = get_models_dir(cfg)
    click.echo(f"Using Output Directory: {os.path.abspath(output_dir)}")
    click.echo(f"Using Models Directory: {os.path.abspath(models_dir)}")

    # Determine input data path
    data_path = input # Use CLI input if provided
    if not data_path:
        data_dir = cfg["paths"]["data_dir"]
        file_pattern = cfg["dataset"]["file_pattern"]
        data_path = os.path.join(data_dir, file_pattern)
        click.echo(f"Using Input Data from config: {os.path.abspath(data_path)}")
    else:
        click.echo(f"Using Input Data from CLI: {os.path.abspath(data_path)}")

    # Create pipeline and evaluate the model
    try:
        pipeline = Pipeline(cfg)
        click.echo(f"Evaluating model: {model_to_eval}...")
        # Evaluate expects a list, even if it's just one model
        results = pipeline.evaluate(model_names=[model_to_eval], data_path=data_path)

        if results and model_to_eval in results:
            click.echo(click.style("\nEvaluation Results:", bold=True))
            metrics = results[model_to_eval]
            for metric, value in metrics.items():
                click.echo(f"  {metric}: {value:.4f}")
            results_file = os.path.join(output_dir, "evaluation_results.csv")
            click.echo(f"\nDetailed results saved in: {output_dir}")
            click.echo(f"Metrics summary saved to: {results_file}")
        else:
            click.echo(click.style(f"Evaluation completed, but no metrics found for model '{model_to_eval}'.", fg="yellow"))


    except FileNotFoundError as e:
         click.echo(click.style(f"Error: Required file not found. {e}", fg="red"))
         sys.exit(1)
    except Exception as e:
        logger.error(f"Error during evaluation: {e}", exc_info=True)
        click.echo(click.style(f"An error occurred during evaluation: {e}", fg="red"))
        sys.exit(1)

# Place this corrected command function in ensembleflex/cli.py

@cli.command()
@click.option("--model",
              help="Model to use for prediction (e.g., random_forest). Defaults to best/first enabled model.")
@click.option("--config",
              type=click.Path(exists=True, dir_okay=False),
              help="Path to custom YAML config file.")
@click.option("--param",
              multiple=True,
              help="Override config parameter.")
@click.option("--input",
              type=click.Path(exists=True, dir_okay=False),
              required=True,
              help="Input data file (CSV) with features for prediction. May optionally contain true 'rmsf' target column.")
@click.option("--output",
              type=click.Path(dir_okay=False),
              help="Output file path for predictions (CSV). Defaults to <input_base>_predictions_<temp>K.csv in output dir.")
@click.option("--metrics-output",
              type=click.Path(dir_okay=False),
              help="Output file path for prediction metrics (CSV), if target is present in input. Defaults to <output>_metrics.csv.")
@click.option("--temperature", "--temp",
              type=float,
              required=True,
              help="REQUIRED: Target temperature (K) for which to generate predictions.")
@click.option("--mode",
              type=click.Choice(["standard", "omniflex"], case_sensitive=False),
              help="Override operation mode. Should match the mode the model was trained in.")
@click.option("--uncertainty",
              is_flag=True,
              help="Include uncertainty estimates if the model supports it.")
def predict(model, config, param, input, output, metrics_output, temperature, mode, uncertainty):
    """
    Generate RMSF predictions for new data at a SPECIFIED TEMPERATURE.

    If the input file contains the true 'rmsf' target column, evaluation
    metrics (PCC, R2, etc.) will be calculated and saved.
    """
    # --- Config loading and mode handling ---
    cfg = load_config(config_path=config, param_overrides=param)
    if mode:
        cfg["mode"]["active"] = mode.lower()
        logger.info(f"Mode overridden by CLI: {cfg['mode']['active']}")
        try: # Re-apply mode logic
            use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
            if cfg["mode"]["active"] == "omniflex":
                omniflex_cfg = cfg.get("mode", {}).get("omniflex", {})
                use_features["esm_rmsf"] = omniflex_cfg.get("use_esm", False)
                use_features["voxel_rmsf"] = omniflex_cfg.get("use_voxel", False)
            else:
                 use_features["esm_rmsf"] = False
                 use_features["voxel_rmsf"] = False
        except Exception as e: logger.error(f"Error applying mode override: {e}", exc_info=True)

    # --- Get paths ---
    output_cfg_dir = get_output_dir(cfg) # Base output dir from config
    models_dir = get_models_dir(cfg)
    logger.info(f"Using Models Directory: {os.path.abspath(models_dir)}")

    try:
        # Initialize pipeline
        pipeline = Pipeline(cfg)

        click.echo(f"Generating predictions for temperature: {temperature} K")
        click.echo(f"Using input data: {os.path.abspath(input)}")

        # --- Call predict method ---
        # It returns predictions DataFrame and metrics dictionary (or None)
        predictions_df, calculated_metrics = pipeline.predict(
            data=input,
            model_name=model, # Pass CLI arg for model name
            temperature=temperature,
            with_uncertainty=uncertainty
        )

        # --- Determine output paths ---
        if not output:
            base = os.path.splitext(os.path.basename(input))[0]
            # Use K for Kelvin in filename for clarity
            output_filename = f"{base}_predictions_{int(temperature)}K.csv"
            output = os.path.join(output_cfg_dir, output_filename)
        click.echo(f"Prediction output path: {os.path.abspath(output)}")

        if calculated_metrics and not metrics_output:
             metrics_output = os.path.splitext(output)[0] + "_metrics.csv"
        elif calculated_metrics:
             click.echo(f"Metrics output path: {os.path.abspath(metrics_output)}")
        elif metrics_output:
             click.echo(click.style("Warning: --metrics-output specified, but no metrics calculated.", fg="yellow"))

        # --- Save Predictions ---
        output_save_dir = os.path.dirname(os.path.abspath(output))
        ensure_dir(output_save_dir) # Uses the imported function
        predictions_df.to_csv(output, index=False)
        click.echo(click.style(f"Saved predictions to {output}", fg="green"))

        # --- Save Metrics (if calculated) ---
        if calculated_metrics and metrics_output:
             metrics_save_dir = os.path.dirname(os.path.abspath(metrics_output))
             ensure_dir(metrics_save_dir)
             metrics_df_to_save = pd.DataFrame([calculated_metrics])

             # --- CORRECTED MODEL NAME FALLBACK ---
             # Determine the model name used (either from CLI or determined by pipeline)
             model_name_used = model
             if not model_name_used: # If --model wasn't specified
                # Get the name from the loaded model in the pipeline cache
                if pipeline.models:
                     model_name_used = list(pipeline.models.keys())[0] # Convert keys to list
                else: # Fallback if model wasn't cached (shouldn't happen if predict worked)
                     model_name_used = get_enabled_models(cfg)[0] if get_enabled_models(cfg) else 'unknown'

             metrics_df_to_save.insert(0, 'model', model_name_used)
             # --- END CORRECTION ---

             metrics_df_to_save.insert(1, 'predicted_temperature', temperature)
             metrics_df_to_save.to_csv(metrics_output, index=False)
             click.echo(click.style(f"Saved prediction metrics to {metrics_output}", fg="green"))
             click.echo(click.style("Prediction Metrics:", bold=True))
             for key in ['r2', 'pearson_correlation', 'rmse', 'mae']:
                  if key in calculated_metrics:
                       click.echo(f"  {key}: {calculated_metrics[key]:.4f}")

    except FileNotFoundError as e:
         click.echo(click.style(f"Error: Required file not found. {e}", fg="red"))
         sys.exit(1)
    except Exception as e:
        logger.error(f"Error generating predictions: {e}", exc_info=True)
        click.echo(click.style(f"An error occurred during prediction: {e}", fg="red"))
        sys.exit(1)


# --- REMOVED `train-all-temps` Command ---
# (Delete the entire function and its decorator)


# --- Redesigned `compare-temperatures` Command ---
@cli.command()
@click.option("--model",
              help="Model to use for comparison (e.g., random_forest). Defaults to best/first enabled model.")
@click.option("--config",
              type=click.Path(exists=True, dir_okay=False),
              help="Path to custom YAML config file.")
@click.option("--param",
              multiple=True,
              help="Override config parameter.")
@click.option("--input",
              type=click.Path(exists=True, dir_okay=False),
              required=True,
              help="Input data file (CSV) containing features (e.g., the test split).")
@click.option("--temp-list",
              help="Comma-separated list of temperatures to predict (e.g., '320,350,400').")
@click.option("--temp-range",
              help="Temperature range MIN,MAX,STEP (e.g., '300,450,10' for 300K to 450K in 10K steps).")
@click.option("--output-dir",
              required=True,
              type=click.Path(file_okay=False),
              help="Directory to save the comparison results (plots and CSVs).")
@click.option("--mode",
              type=click.Choice(["standard", "omniflex"], case_sensitive=False),
              help="Override operation mode. Should match the mode the model was trained in.")
def compare_temperatures(model, config, param, input, temp_list, temp_range, output_dir, mode):
    """
    Compare the single trained model's predictions across a range of temperatures.

    Uses the trained model specified (or default) and the feature data from
    --input. Predicts RMSF for each temperature in the specified list or range.
    Saves aggregated predictions and analysis plots to --output-dir.
    """
    if not temp_list and not temp_range:
        click.echo(click.style("Error: Must provide either --temp-list or --temp-range.", fg="red"))
        sys.exit(1)
    if temp_list and temp_range:
        click.echo(click.style("Error: Cannot use both --temp-list and --temp-range.", fg="red"))
        sys.exit(1)

    # Parse temperatures
    temperatures_to_predict = []
    if temp_list:
        try:
            temperatures_to_predict = [float(t.strip()) for t in temp_list.split(',')]
        except ValueError:
            click.echo(click.style("Error: Invalid format in --temp-list. Use comma-separated numbers.", fg="red"))
            sys.exit(1)
    elif temp_range:
        try:
            min_t, max_t, step_t = map(float, temp_range.split(','))
            if step_t <= 0: raise ValueError("Step must be positive")
            # Use numpy.arange for float steps, include endpoint carefully
            temperatures_to_predict = np.arange(min_t, max_t + step_t / 2, step_t).tolist() # Add small buffer to include max_t
        except (ValueError, IndexError):
            click.echo(click.style("Error: Invalid format in --temp-range. Use MIN,MAX,STEP.", fg="red"))
            sys.exit(1)

    if not temperatures_to_predict:
         click.echo(click.style("Error: No valid temperatures specified for comparison.", fg="red"))
         sys.exit(1)

    click.echo(f"Comparing model predictions across {len(temperatures_to_predict)} temperatures: {temperatures_to_predict}")

    # Load configuration
    cfg = load_config(config_path=config, param_overrides=param)

    # Set mode if specified via CLI
    if mode:
        cfg["mode"]["active"] = mode.lower()
        logger.info(f"Mode overridden by CLI: {cfg['mode']['active']}")
        # Re-apply mode logic (copied from train)
        try:
            if cfg["mode"]["active"] == "omniflex":
                omniflex_cfg = cfg.get("mode", {}).get("omniflex", {})
                use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                use_features["esm_rmsf"] = omniflex_cfg.get("use_esm", False)
                use_features["voxel_rmsf"] = omniflex_cfg.get("use_voxel", False)
            else: # Standard mode
                 use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                 use_features["esm_rmsf"] = False
                 use_features["voxel_rmsf"] = False
        except Exception as e:
            logger.error(f"Error applying mode override settings: {e}", exc_info=True)


    # Get models directory for loading
    models_dir = get_models_dir(cfg)
    click.echo(f"Using Models Directory: {os.path.abspath(models_dir)}")
    os.makedirs(output_dir, exist_ok=True)
    click.echo(f"Saving comparison results to: {os.path.abspath(output_dir)}")


    try:
        # Initialize pipeline (needed for predict method structure)
        pipeline = Pipeline(cfg)

        # Load the input data ONCE (features only)
        click.echo(f"Loading input data for features: {input}")
        from ensembleflex.data.processor import load_and_process_data, prepare_data_for_model
        # Load the raw data, process features, but don't necessarily need target
        input_df_processed = load_and_process_data(data_path=input, config=cfg)
        # Prepare just the features, target isn't strictly needed for prediction input prep
        X_input, _, feature_names = prepare_data_for_model(input_df_processed, cfg, include_target=False)
        input_ids_df = input_df_processed[['domain_id', 'resid', 'resname']].reset_index(drop=True) # Keep IDs for merging

        # Load the single model ONCE
        # Determine model name (use specified or find best/first enabled)
        if not model:
            # Logic to find best model (e.g., from evaluation results) could go here
            # Fallback to first enabled model
            enabled_models = get_enabled_models(cfg)
            if not enabled_models:
                 click.echo(click.style("Error: No model specified and no models enabled in config.", fg="red"))
                 sys.exit(1)
            model_name_to_load = enabled_models[0]
            click.echo(f"No model specified, using first enabled model: {model_name_to_load}")
        else:
            model_name_to_load = model

        click.echo(f"Loading model: {model_name_to_load}...")
        loaded_model = pipeline.load_model(model_name_to_load) # Assumes load_model works correctly

        # Store predictions per temperature
        all_predictions = []

        # Loop through temperatures and predict
        for temp in temperatures_to_predict:
            click.echo(f"  Predicting for T = {temp:.1f} K...")

            # --- Manual Feature Augmentation for Temperature ---
            # Find index of 'temperature' feature if it was used during training
            temp_feature_index = -1
            if cfg['dataset']['features']['use_features'].get('temperature', False):
                try:
                    temp_feature_index = feature_names.index('temperature')
                except ValueError:
                     click.echo(click.style("Error: Model trained with temperature feature, but 'temperature' not in prepared feature names.", fg="red"))
                     sys.exit(1)
                 # Create augmented feature matrix X_augmented
                X_augmented = X_input.copy()
                X_augmented[:, temp_feature_index] = temp # Set the temperature column
            else:
                # If model wasn't trained with temperature, we shouldn't be here.
                # Log warning or error, potentially skip?
                logger.warning(f"Running compare-temperatures, but model {model_name_to_load} was not trained with temperature as a feature. Predictions might be constant.")
                X_augmented = X_input # Use original features

            # Predict using the loaded model and augmented features
            temp_preds = loaded_model.predict(X_augmented)

            # Store results with identifiers and temperature
            temp_df = input_ids_df.copy()
            temp_df['temperature'] = temp
            temp_df['predicted_rmsf'] = temp_preds
            all_predictions.append(temp_df)

        # Combine all predictions
        if not all_predictions:
             click.echo(click.style("Error: No predictions were generated.", fg="red"))
             sys.exit(1)

        combined_results_df = pd.concat(all_predictions, ignore_index=True)

        # Save combined predictions
        output_csv_path = os.path.join(output_dir, f"{model_name_to_load}_predictions_vs_temp.csv")
        combined_results_df.to_csv(output_csv_path, index=False)
        click.echo(f"Saved combined predictions to: {output_csv_path}")

        # --- Perform Analysis and Visualization ---
        click.echo("Generating analysis plots...")

        # 1. Plot Predicted RMSF vs. Input Temperature for a few residues/average
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            from ensembleflex.utils.visualization import save_plot # Assume this exists

            # Select a few domains/residues for plotting
            ids_to_plot = combined_results_df[['domain_id', 'resid']].drop_duplicates().sample(min(5, len(combined_results_df)), random_state=cfg['system']['random_state'])

            plt.figure(figsize=(10, 6))
            for _, row in ids_to_plot.iterrows():
                domain, resid = row['domain_id'], row['resid']
                subset = combined_results_df[(combined_results_df['domain_id'] == domain) & (combined_results_df['resid'] == resid)]
                plt.plot(subset['temperature'], subset['predicted_rmsf'], marker='o', linestyle='-', label=f"{domain}-{resid}")

            plt.xlabel("Input Temperature (K)")
            plt.ylabel("Predicted RMSF")
            plt.title(f"Predicted RMSF vs. Temperature ({model_name_to_load})")
            plt.legend(title="Domain-ResID", bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.grid(True, linestyle='--', alpha=0.6)
            plot_path = os.path.join(output_dir, f"{model_name_to_load}_pred_vs_temp_samples.png")
            save_plot(plt, plot_path)
            click.echo(f"Saved sample prediction vs. temperature plot to: {plot_path}")

            # 2. Plot average predicted RMSF vs Temperature
            avg_rmsf_vs_temp = combined_results_df.groupby('temperature')['predicted_rmsf'].mean().reset_index()
            plt.figure(figsize=(10, 6))
            plt.plot(avg_rmsf_vs_temp['temperature'], avg_rmsf_vs_temp['predicted_rmsf'], marker='o', linestyle='-')
            plt.xlabel("Input Temperature (K)")
            plt.ylabel("Average Predicted RMSF")
            plt.title(f"Average Predicted RMSF vs. Temperature ({model_name_to_load})")
            plt.grid(True, linestyle='--', alpha=0.6)
            plot_path = os.path.join(output_dir, f"{model_name_to_load}_avg_pred_vs_temp.png")
            save_plot(plt, plot_path)
            click.echo(f"Saved average prediction vs. temperature plot to: {plot_path}")


        except ImportError:
            click.echo(click.style("Warning: Matplotlib/Seaborn not found. Skipping plot generation.", fg="yellow"))
        except Exception as e:
            logger.error(f"Error during plot generation: {e}", exc_info=True)
            click.echo(click.style(f"Warning: Failed to generate plots: {e}", fg="yellow"))

        click.echo(click.style("Temperature comparison finished.", fg="green"))


    except FileNotFoundError as e:
         click.echo(click.style(f"Error: Required file not found. {e}", fg="red"))
         sys.exit(1)
    except Exception as e:
        logger.error(f"Error during temperature comparison: {e}", exc_info=True)
        click.echo(click.style(f"An error occurred during temperature comparison: {e}", fg="red"))
        sys.exit(1)


# --- `preprocess` Command (Largely unchanged, but remove temp templating) ---
@cli.command()
@click.option("--input",
              type=click.Path(exists=True, dir_okay=False),
              required=True,
              help="Input data file (CSV) to preprocess.")
@click.option("--config",
              type=click.Path(exists=True, dir_okay=False),
              help="Path to custom YAML config file.")
@click.option("--param",
              multiple=True,
              help="Override config parameter.")
@click.option("--output",
              type=click.Path(dir_okay=False),
              help="Output file path for processed data (CSV). Defaults to input_processed.csv.")
# REMOVED: --temperature option
@click.option("--mode",
              type=click.Choice(["standard", "omniflex"], case_sensitive=False),
              help="Operation mode (influences feature processing).")
def preprocess(input, config, param, output, mode):
    """
    Preprocess data only (load, clean, engineer features) without training.
    Uses the aggregated data structure.
    """
    from ensembleflex.data.processor import load_and_process_data # Import here

    # Load configuration
    cfg = load_config(config_path=config, param_overrides=param)

    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode.lower()
        logger.info(f"Mode overridden by CLI: {cfg['mode']['active']}")
        # Re-apply mode logic (copied from train)
        try:
            if cfg["mode"]["active"] == "omniflex":
                omniflex_cfg = cfg.get("mode", {}).get("omniflex", {})
                use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                use_features["esm_rmsf"] = omniflex_cfg.get("use_esm", False)
                use_features["voxel_rmsf"] = omniflex_cfg.get("use_voxel", False)
            else: # Standard mode
                 use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                 use_features["esm_rmsf"] = False
                 use_features["voxel_rmsf"] = False
        except Exception as e:
            logger.error(f"Error applying mode override settings: {e}", exc_info=True)


    try:
        # Process data using the main function
        click.echo(f"Preprocessing data from: {os.path.abspath(input)}")
        # We pass the input path directly to load_and_process_data
        processed_df = load_and_process_data(data_path=input, config=cfg)

        # Determine output path
        if not output:
            base = os.path.splitext(os.path.basename(input))[0]
            output_filename = f"{base}_processed.csv"
             # Save relative to input file's directory or current dir? Let's use current dir.
            output = os.path.join(".", output_filename) # Save in current directory
            click.echo(f"Output path not specified, saving to: {output}")


        # Ensure output directory exists
        output_save_dir = os.path.dirname(os.path.abspath(output))
        os.makedirs(output_save_dir, exist_ok=True)

        # Save processed data
        processed_df.to_csv(output, index=False)
        click.echo(click.style(f"Saved processed data to {output}", fg="green"))

    except FileNotFoundError as e:
         click.echo(click.style(f"Error: Input file not found. {e}", fg="red"))
         sys.exit(1)
    except Exception as e:
        logger.error(f"Error preprocessing data: {e}", exc_info=True)
        click.echo(click.style(f"An error occurred during preprocessing: {e}", fg="red"))
        sys.exit(1)

# --- Modified `run` Command ---
@cli.command()
@click.option("--model",
              help="Model to run (train, evaluate, analyze). Defaults to first enabled model.")
@click.option("--config",
              type=click.Path(exists=True, dir_okay=False),
              help="Path to custom YAML config file.")
@click.option("--param",
              multiple=True,
              help="Override config parameter.")
@click.option("--input",
              type=click.Path(exists=True, dir_okay=False),
              help="Override input data file (CSV). Defaults to config.")
# REMOVED: --temperature option
@click.option("--mode",
              type=click.Choice(["standard", "omniflex"], case_sensitive=False),
              help="Override operation mode.")
@click.option("--skip-analysis", # Changed from skip-visualization for clarity
              is_flag=True,
              help="Skip the analysis step after training and evaluation.")
def run(model, config, param, input, mode, skip_analysis):
    """
    Run the complete pipeline (train, evaluate, analyze) for the unified model.
    """
     # Load configuration
    cfg = load_config(config_path=config, param_overrides=param)

    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode.lower()
        logger.info(f"Mode overridden by CLI: {cfg['mode']['active']}")
        # Re-apply mode logic (copied from train)
        try:
            if cfg["mode"]["active"] == "omniflex":
                omniflex_cfg = cfg.get("mode", {}).get("omniflex", {})
                use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                use_features["esm_rmsf"] = omniflex_cfg.get("use_esm", False)
                use_features["voxel_rmsf"] = omniflex_cfg.get("use_voxel", False)
            else: # Standard mode
                 use_features = cfg.setdefault("dataset", {}).setdefault("features", {}).setdefault("use_features", {})
                 use_features["esm_rmsf"] = False
                 use_features["voxel_rmsf"] = False
        except Exception as e:
            logger.error(f"Error applying mode override settings: {e}", exc_info=True)


    # Determine which models to use (usually just one)
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)

    if not model_list:
        click.echo(click.style("Error: No models specified or enabled in config.", fg="red"))
        sys.exit(1)
    if len(model_list) > 1:
         click.echo(click.style(f"Warning: Multiple models enabled/specified ({model_list}). ensembleflex runs one unified model. Using first: '{model_list[0]}'", fg="yellow"))
         model_list = [model_list[0]]

    # Get unified output/models directories
    output_dir = get_output_dir(cfg)
    models_dir = get_models_dir(cfg)

    # Create directories
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(models_dir, exist_ok=True)
    click.echo(f"Using Output Directory: {os.path.abspath(output_dir)}")
    click.echo(f"Using Models Directory: {os.path.abspath(models_dir)}")

    # Determine input data path
    data_path = input # Use CLI input if provided
    if not data_path:
        data_dir = cfg["paths"]["data_dir"]
        file_pattern = cfg["dataset"]["file_pattern"]
        data_path = os.path.join(data_dir, file_pattern)
        click.echo(f"Using Input Data from config: {os.path.abspath(data_path)}")
    else:
        click.echo(f"Using Input Data from CLI: {os.path.abspath(data_path)}")

    # Create pipeline and run
    try:
        pipeline = Pipeline(cfg)
        click.echo(f"Running full pipeline for model: {model_list[0]}...")

        # Pass skip_analysis flag correctly
        results = pipeline.run_pipeline(
            model_names=model_list,
            data_path=data_path,
            skip_analysis=skip_analysis # Use the flag directly
        )

        click.echo(click.style("\nPipeline completed successfully!", fg="green"))
        click.echo(f"Results saved to: {output_dir}")

        if results and model_list[0] in results:
            click.echo(click.style("\nFinal Evaluation Metrics:", bold=True))
            metrics = results[model_list[0]]
            for metric, value in metrics.items():
                click.echo(f"  {metric}: {value:.4f}")
        else:
            click.echo(click.style("Evaluation completed, but no final metrics were returned.", fg="yellow"))


    except FileNotFoundError as e:
         click.echo(click.style(f"Error: Required file not found. {e}", fg="red"))
         sys.exit(1)
    except Exception as e:
        logger.error(f"Error running pipeline: {e}", exc_info=True)
        click.echo(click.style(f"An error occurred running the pipeline: {e}", fg="red"))
        sys.exit(1)

# --- `list-models` Command (Unchanged) ---
@cli.command()
def list_models():
    """
    List available models registered in the application.

    Examples:
        ensembleflex list-models
    """
    models = get_available_models() # Function imported from ensembleflex.models

    click.echo("Available models:")
    if models:
        for model in sorted(models):
            click.echo(f"  - {model}")
    else:
        click.echo("  No models found or registered.")

# --- REMOVED `list-temperatures` Command ---
# (Its purpose is informational and less relevant now)


if __name__ == "__main__":
    cli()
### Model Files ###
---------------------------------------------------------
===== FILE: ensembleflex/models/base.py =====
"""
Base model class for ensembleflex ML pipeline.

This module defines the BaseModel abstract class that all
protein flexibility prediction models must implement.
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union, Tuple

import numpy as np
import pandas as pd

class BaseModel(ABC):
    """
    Base class for all ensembleflex ML models.
    All models must implement these methods.
    """
    
    @abstractmethod
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]) -> 'BaseModel':
        """
        Train the model on input data.
        
        Args:
            X: Feature matrix
            y: Target values
            
        Returns:
            Self, for method chaining
        """
        pass
    
    @abstractmethod
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Generate predictions for input data.
        
        Args:
            X: Feature matrix
            
        Returns:
            Array of predictions
        """
        pass
    
    @abstractmethod
    def save(self, path: str) -> None:
        """
        Save model to disk.
        
        Args:
            path: Path to save location
        """
        pass
    
    @classmethod
    @abstractmethod
    def load(cls, path: str) -> 'BaseModel':
        """
        Load model from disk.
        
        Args:
            path: Path to saved model
            
        Returns:
            Loaded model instance
        """
        pass
    
    def predict_with_std(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate predictions with uncertainty estimates.
        
        Args:
            X: Feature matrix
            
        Returns:
            Tuple of (predictions, std_deviation)
        """
        # Default implementation returns predictions with zeros for std dev
        # Models that support uncertainty should override this
        predictions = self.predict(X)
        std_deviation = np.zeros_like(predictions)
        return predictions, std_deviation
    
    def get_feature_importance(self) -> Optional[Dict[str, float]]:
        """
        Get feature importance values if available.
        
        Returns:
            Dictionary mapping feature names to importance values,
            or None if feature importance is not available
        """
        return None
    
    def get_params(self) -> Dict[str, Any]:
        """
        Get model parameters.
        
        Returns:
            Dictionary of model parameters
        """
        return {
            key: value for key, value in self.__dict__.items()
            if not key.startswith('_') and key != 'model'
        }
    
    def get_model_name(self) -> str:
        """
        Get model name.
        
        Returns:
            String representing model name
        """
        return self.__class__.__name__
    
    def hyperparameter_optimize(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        method: str = "bayesian",
        n_trials: int = 20,
        cv: int = 3
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid or distributions
            method: Optimization method ("grid", "random", or "bayesian")
            n_trials: Number of trials for random or bayesian methods
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
            
        Raises:
            NotImplementedError: If the model doesn't support hyperparameter optimization
        """
        raise NotImplementedError("This model doesn't support hyperparameter optimization")
    
    def get_training_history(self) -> Optional[Dict[str, List[float]]]:
        """
        Get training history if available.
        
        Returns:
            Dictionary with training metrics by epoch, or None if not available
        """
        return None
===== FILE: ensembleflex/models/random_forest.py =====



"""
Optimized Random Forest model for the EnsembleFlex ML pipeline.

This module provides a high-performance RandomForestModel for protein flexibility prediction,
focused on simplicity and performance with large datasets.
"""

import os
import logging
from typing import Dict, Any, Optional, Union, List, Tuple
import time
import gc

import numpy as np
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
import inspect

from ensembleflex.models import register_model
from ensembleflex.models.base import BaseModel
from ensembleflex.utils.helpers import ProgressCallback, ensure_dir

logger = logging.getLogger(__name__)

@register_model("random_forest")
class RandomForestModel(BaseModel):
    """
    Efficient Random Forest implementation for protein flexibility prediction.
    
    Optimized for large datasets with many features (like window-based encoding)
    while maintaining simplicity and compatibility with the ensembleflex pipeline.
    """
    
    def __init__(
        self,
        n_estimators: int = 200,
        max_depth: Optional[int] = 50,
        min_samples_split: int = 2,
        min_samples_leaf: int = 1,
        max_features: Union[str, float, int] = "sqrt",
        bootstrap: bool = True,
        random_state: int = 42,
        **kwargs
    ):
        """
        Initialize the Random Forest model with configured parameters.
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.bootstrap = bootstrap
        self.random_state = random_state
        
        # Handle n_jobs parameter (from kwargs or system settings)
        n_jobs = kwargs.pop('n_jobs', None)
        if n_jobs is None or n_jobs == -16:
            # Use 90% of cores for large datasets
            import multiprocessing
            total_cores = multiprocessing.cpu_count()
            self.n_jobs = max(1, int(total_cores * 0.3))
            logger.info(f"Using {self.n_jobs} cores for training (system has {total_cores})")
        else:
            self.n_jobs = n_jobs
        
        # Filter kwargs for valid RandomForestRegressor parameters
        rf_params = set(inspect.signature(RandomForestRegressor).parameters.keys())
        self.model_params = {k: v for k, v in kwargs.items() if k in rf_params}
        
        # Store hyperparameter search config separately
        self.randomized_search_config = kwargs.get('randomized_search', {})
        
        # Initialize model state
        self.model = None
        self.feature_names_ = None
        self.best_params_ = None
        self._feature_importances_cache = None

    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series], 
            feature_names: Optional[List[str]] = None,
            X_val: Optional[Union[np.ndarray, pd.DataFrame]] = None,
            y_val: Optional[Union[np.ndarray, pd.Series]] = None) -> 'RandomForestModel':
        """
        Train the Random Forest model with optimized settings.
        
        Args:
            X: Feature matrix
            y: Target RMSF values
            feature_names: Optional list of feature names
            X_val: Optional validation features
            y_val: Optional validation targets
        """
        # Store feature names
        if isinstance(X, pd.DataFrame):
            self.feature_names_ = X.columns.tolist()
        elif feature_names is not None:
            self.feature_names_ = feature_names
        else:
            self.feature_names_ = [f"feature_{i}" for i in range(X.shape[1])]

        # Convert to numpy arrays for better performance
        if isinstance(X, pd.DataFrame): 
            X = X.values
        if isinstance(y, pd.Series): 
            y = y.values
        
        # Ensure y is 1D
        y = np.ravel(y)
        
        # Log dataset properties
        logger.info(f"Training RandomForest on dataset with {X.shape[0]} samples and {X.shape[1]} features")
        
        # Check if using hyperparameter optimization
        use_randomized_search = self.randomized_search_config.get('enabled', False)
        
        # Measure total training time
        fit_start_time = time.time()
        
        try:
            if use_randomized_search:
                self._fit_with_hpo(X, y)
            else:
                self._fit_standard(X, y)
            
            # Log training time
            total_fit_time = time.time() - fit_start_time
            logger.info(f"RandomForest training completed in {total_fit_time:.2f} seconds")
            
            # Force garbage collection to free memory
            gc.collect()
            
            return self
        except Exception as e:
            logger.error(f"Error during training: {e}", exc_info=True)
            raise
            
    def _fit_standard(self, X, y):
        """Standard efficient training without hyperparameter optimization."""
        logger.info(f"Training Random Forest with {self.n_estimators} trees and {self.n_jobs} parallel jobs")
        
        # Create parameter dictionary for RandomForestRegressor
        rf_params = {
            'n_estimators': self.n_estimators,
            'max_depth': self.max_depth,
            'min_samples_split': self.min_samples_split,
            'min_samples_leaf': self.min_samples_leaf,
            'max_features': self.max_features,
            'bootstrap': self.bootstrap,
            'random_state': self.random_state,
            'n_jobs': self.n_jobs,
            'verbose': 1,  # Show progress
            **self.model_params
        }
        
        # Create and train the model
        with ProgressCallback(total=1, desc="Training Random Forest") as pbar:
            self.model = RandomForestRegressor(**rf_params)
            self.model.fit(X, y)
            pbar.update()
        
        logger.info(f"Random Forest training complete with {self.n_estimators} trees")
    
    def _fit_with_hpo(self, X, y):
        """Train with hyperparameter optimization using RandomizedSearchCV."""
        logger.info("Running hyperparameter optimization with RandomizedSearchCV")
        
        # Get HPO settings from config
        search_params = self.randomized_search_config
        n_iter = search_params.get('n_iter', 10)
        cv = search_params.get('cv', 3)
        verbose = search_params.get('verbose', 1)
        param_distributions = search_params.get('param_distributions', {})
        
        if not isinstance(param_distributions, dict) or not param_distributions:
            logger.error("HPO enabled, but 'param_distributions' is missing or invalid")
            raise ValueError("Invalid 'param_distributions' for RandomizedSearchCV")
        
        # Configure the base estimator with minimum parallelization
        # since parallelization happens at the CV level
        base_rf = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=1,  # Single job here, parallelism is at CV level
            **{k: v for k, v in self.model_params.items() 
               if k not in param_distributions}
        )
        
        logger.info(f"Setting up RandomizedSearchCV with n_iter={n_iter}, cv={cv}, n_jobs={self.n_jobs}")
        
        # Set up search with efficient parameters
        search = RandomizedSearchCV(
            estimator=base_rf,
            param_distributions=param_distributions,
            n_iter=n_iter,
            cv=cv,
            scoring='neg_mean_squared_error',
            n_jobs=self.n_jobs,
            random_state=self.random_state,
            verbose=verbose,
            # pre_dispatch='2*n_jobs',  # Control worker dispatching
            return_train_score=False,
            error_score='raise'
        )
        
        try:
            # Fit the search
            logger.info("Starting RandomizedSearchCV fitting...")
            search.fit(X, y)
            
            # Store best model and parameters
            self.model = search.best_estimator_
            self.best_params_ = search.best_params_
            
            # Log results
            logger.info(f"Best score: {search.best_score_:.4f}")
            logger.info(f"Best parameters: {self.best_params_}")
            
            # Set optimal n_jobs for the final model
            if hasattr(self.model, 'n_jobs') and self.model.n_jobs == 1:
                self.model.n_jobs = self.n_jobs
            
        except Exception as e:
            logger.error(f"Error during HPO: {e}", exc_info=True)
            
            # Fallback to standard training
            logger.warning("Falling back to standard training after HPO failure")
            self._fit_standard(X, y)
    
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """Generate predictions using the trained model."""
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Convert to numpy if needed
        if isinstance(X, pd.DataFrame):
            X = X.values
        
        return self.model.predict(X)
    
    def predict_with_std(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """Generate predictions with uncertainty estimates using tree variance."""
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Convert to numpy if needed
        if isinstance(X, pd.DataFrame):
            X = X.values
        
        try:
            # Verify model has estimators
            if not hasattr(self.model, 'estimators_') or not self.model.estimators_:
                logger.warning("Model has no estimators. Cannot calculate uncertainty.")
                mean_prediction = self.model.predict(X)
                return mean_prediction, np.zeros_like(mean_prediction)
            
            # Get predictions from each tree
            all_preds = np.array([tree.predict(X) for tree in self.model.estimators_])
            
            # Calculate mean and standard deviation across trees
            mean_prediction = np.mean(all_preds, axis=0)
            std_prediction = np.std(all_preds, axis=0)
            
            return mean_prediction, std_prediction
        except Exception as e:
            logger.error(f"Error during uncertainty prediction: {e}. Falling back to zeros.")
            mean_prediction = self.model.predict(X)
            return mean_prediction, np.zeros_like(mean_prediction)
    
    def get_feature_importance(self, 
                              X_val=None, 
                              y_val=None, 
                              method="permutation", 
                              n_repeats=10) -> Optional[Union[Dict[str, float], np.ndarray]]:
        """Calculate feature importance using permutation or impurity methods."""
        if self.model is None:
            return None
        
        # Use cached result if available (and no new validation data provided)
        if self._feature_importances_cache is not None and X_val is None:
            return self._feature_importances_cache
        
        # Impurity-based importance (fast, no validation data needed)
        if method == "impurity" or X_val is None or y_val is None:
            try:
                logger.debug("Using impurity-based feature importance")
                importances = self.model.feature_importances_
                
                # Map to feature names if available
                if self.feature_names_ and len(self.feature_names_) == len(importances):
                    result = dict(zip(self.feature_names_, importances))
                    self._feature_importances_cache = result
                    return result
                else:
                    return importances
                    
            except Exception as e:
                logger.warning(f"Could not get impurity-based importance: {e}")
                return None
        
        # Permutation importance with validation data
        elif method == "permutation" and X_val is not None and y_val is not None:
            try:
                from sklearn.inspection import permutation_importance
                
                logger.debug(f"Calculating permutation importance with {n_repeats} repeats")
                
                # Convert validation data to numpy if needed
                if isinstance(X_val, pd.DataFrame):
                    X_val = X_val.values
                if isinstance(y_val, pd.Series):
                    y_val = y_val.values
                
                # Sample data if it's large to avoid memory issues
                if len(X_val) > 10000:
                    indices = np.random.choice(len(X_val), size=10000, replace=False)
                    X_val_sample = X_val[indices]
                    y_val_sample = y_val[indices]
                else:
                    X_val_sample = X_val
                    y_val_sample = y_val
                
                # Calculate permutation importance
                r = permutation_importance(
                    self.model, X_val_sample, y_val_sample,
                    n_repeats=n_repeats,
                    random_state=self.random_state,
                    n_jobs=min(4, self.n_jobs)  # Limit jobs to prevent memory issues
                )
                
                importances = r.importances_mean
                
                # Map to feature names if available
                if self.feature_names_ and len(self.feature_names_) == len(importances):
                    result = dict(zip(self.feature_names_, importances))
                    self._feature_importances_cache = result
                    return result
                else:
                    return importances
                    
            except Exception as e:
                logger.warning(f"Permutation importance failed: {e}. Falling back to impurity.")
                return self.get_feature_importance(method="impurity")
        
        logger.warning(f"Unsupported feature importance method: {method}")
        return None
    
    def hyperparameter_optimize(self, 
                               X, 
                               y, 
                               param_grid, 
                               method="random", 
                               n_trials=20, 
                               cv=3):
        """Perform hyperparameter optimization by calling fit with RandomizedSearchCV."""
        logger.info(f"Setting up hyperparameter optimization with {n_trials} trials")
        
        # Only RandomizedSearchCV is supported
        if method != "random":
            logger.warning(f"Method '{method}' not supported, using 'random'")
        
        # Configure randomized search
        self.randomized_search_config = {
            'enabled': True,
            'n_iter': n_trials,
            'cv': cv,
            'param_distributions': param_grid,
            'verbose': 1
        }
        
        # Train with HPO
        self.fit(X, y)
        
        # Return best parameters
        return self.best_params_ if self.best_params_ else {}
    
    def save(self, path: str) -> None:
        """Save model to disk with all necessary metadata."""
        if self.model is None:
            raise RuntimeError("Cannot save untrained model")
        
        # Ensure directory exists
        ensure_dir(os.path.dirname(path))
        
        # Prepare state dictionary
        state = {
            'model': self.model,
            'feature_names': self.feature_names_,
            'best_params': self.best_params_,
            # Store parameters for reconstruction
            'init_params': {
                'n_estimators': self.n_estimators,
                'max_depth': self.max_depth,
                'min_samples_split': self.min_samples_split,
                'min_samples_leaf': self.min_samples_leaf,
                'max_features': self.max_features,
                'bootstrap': self.bootstrap,
                'random_state': self.random_state,
                'n_jobs': self.n_jobs,
                **self.model_params
            },
            'randomized_search_config': self.randomized_search_config
        }
        
        try:
            # Save with compression
            joblib.dump(state, path, compress=3)
            logger.info(f"Model saved to {path}")
        except Exception as e:
            logger.error(f"Failed to save model to {path}: {e}", exc_info=True)
            raise
    
    @classmethod
    def load(cls, path: str) -> 'RandomForestModel':
        """Load model from disk."""
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found: {path}")
        
        try:
            # Load state dictionary
            state = joblib.load(path)
            
            # Create new instance
            instance = cls()
            
            # Restore model and metadata
            instance.model = state['model']
            instance.feature_names_ = state.get('feature_names')
            instance.best_params_ = state.get('best_params')
            
            # Restore parameters from saved state
            init_params = state.get('init_params', {})
            for key, value in init_params.items():
                if hasattr(instance, key):
                    setattr(instance, key, value)
            
            # Restore model_params dictionary with non-primary parameters
            primary_params = {
                'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf',
                'max_features', 'bootstrap', 'random_state', 'n_jobs'
            }
            instance.model_params = {k: v for k, v in init_params.items() if k not in primary_params}
            
            # Restore randomized search config
            instance.randomized_search_config = state.get('randomized_search_config', {})
            
            logger.info(f"Model loaded from {path}")
            return instance
            
        except Exception as e:
            logger.error(f"Error loading model from {path}: {e}", exc_info=True)
            raise
===== FILE: ensembleflex/models/neural_network.py =====
"""
Neural Network model implementation for the EnsembleFlex ML pipeline.

This module provides a PyTorch-based neural network model
for protein flexibility prediction, with support for
hyperparameter optimization.
"""

import os
import logging
import json
from typing import Dict, Any, Optional, Union, List, Tuple

import numpy as np
import pandas as pd
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score, mean_absolute_error # Added mean_absolute_error
from scipy.stats import pearsonr # Added pearsonr
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import pickle

from ensembleflex.models import register_model
from ensembleflex.models.base import BaseModel
from ensembleflex.utils.helpers import ProgressCallback, progress_bar

logger = logging.getLogger(__name__)

class FlexibilityNN(nn.Module):
    """
    Neural network architecture for protein flexibility prediction.
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_layers: List[int] = [64, 32],
        activation: str = "relu",
        dropout: float = 0.2
    ):
        """
        Initialize the neural network architecture.
        
        Args:
            input_dim: Number of input features
            hidden_layers: List of hidden layer sizes
            activation: Activation function to use
            dropout: Dropout rate
        """
        super(FlexibilityNN, self).__init__()
        
        # Define activation function
        if activation.lower() == "relu":
            act_fn = nn.ReLU()
        elif activation.lower() == "leaky_relu":
            act_fn = nn.LeakyReLU()
        elif activation.lower() == "tanh":
            act_fn = nn.Tanh()
        elif activation.lower() == "sigmoid":
            act_fn = nn.Sigmoid()
        else:
            act_fn = nn.ReLU()
            logger.warning(f"Unknown activation function '{activation}', using ReLU")
        
        # Create layers
        layers = []
        prev_dim = input_dim
        
        # Hidden layers
        for i, dim in enumerate(hidden_layers):
            layers.append(nn.Linear(prev_dim, dim))
            layers.append(act_fn)
            layers.append(nn.Dropout(dropout))
            prev_dim = dim
        
        # Output layer (single value for RMSF prediction)
        layers.append(nn.Linear(prev_dim, 1))
        
        # Create sequential model
        self.model = nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the network.
        
        Args:
            x: Input tensor
            
        Returns:
            Output tensor
        """
        return self.model(x).squeeze()

@register_model("neural_network")
class NeuralNetworkModel(BaseModel):
    """
    Neural Network model for protein flexibility prediction.
    
    This model uses a feed-forward neural network to learn complex
    relationships between protein features and flexibility.
    """
    
    def __init__(
        self,
        architecture: Dict[str, Any] = None,
        training: Dict[str, Any] = None,
        random_state: int = 42,
        **kwargs
    ):
        """
        Initialize the Neural Network model.
        
        Args:
            architecture: Dictionary of architecture parameters
            training: Dictionary of training parameters
            random_state: Random seed for reproducibility
            **kwargs: Additional parameters
        """
        # Set default architecture if not provided
        if architecture is None:
            architecture = {
                "hidden_layers": [64, 32],
                "activation": "relu",
                "dropout": 0.2
            }
        
        # Set default training parameters if not provided
        if training is None:
            training = {
                "optimizer": "adam",
                "learning_rate": 0.001,
                "batch_size": 32,
                "epochs": 100,
                "early_stopping": True,
                "patience": 10
            }
        
        self.architecture = architecture
        self.training = training
        self.random_state = random_state
        self.model = None
        self.feature_names_ = None
        self.scaler = None
        self.history = None
        
        # Set random seeds for reproducibility
        torch.manual_seed(random_state)
        np.random.seed(random_state)
        
        # Device configuration
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
    
    def _create_model(self, input_dim: int) -> FlexibilityNN:
        """
        Create the neural network model.
        
        Args:
            input_dim: Number of input features
            
        Returns:
            Initialized FlexibilityNN model
        """
        model = FlexibilityNN(
            input_dim=input_dim,
            hidden_layers=self.architecture.get("hidden_layers", [64, 32]),
            activation=self.architecture.get("activation", "relu"),
            dropout=self.architecture.get("dropout", 0.2)
        )
        return model.to(self.device)
    
    def _get_optimizer(self, model: FlexibilityNN) -> optim.Optimizer:
        """
        Get the appropriate optimizer.
        
        Args:
            model: The neural network model
            
        Returns:
            Configured optimizer
        """
        optimizer_name = self.training.get("optimizer", "adam").lower()
        lr = self.training.get("learning_rate", 0.001)
        
        if optimizer_name == "adam":
            return optim.Adam(model.parameters(), lr=lr)
        elif optimizer_name == "sgd":
            return optim.SGD(model.parameters(), lr=lr)
        elif optimizer_name == "rmsprop":
            return optim.RMSprop(model.parameters(), lr=lr)
        else:
            logger.warning(f"Unknown optimizer '{optimizer_name}', using Adam")
            return optim.Adam(model.parameters(), lr=lr)
        
        
    
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series], feature_names: Optional[List[str]] = None) -> 'NeuralNetworkModel':
        """
        Train the Neural Network model, displaying key validation metrics in progress bar.

        Args:
            X: Feature matrix
            y: Target RMSF values
            feature_names: Optional list of feature names

        Returns:
            Self, for method chaining
        """
        # Store feature names
        if isinstance(X, pd.DataFrame): self.feature_names_ = X.columns.tolist()
        elif feature_names is not None: self.feature_names_ = feature_names

        # Convert to numpy arrays
        X_array = X.values if isinstance(X, pd.DataFrame) else X
        y_array = y.values if isinstance(y, pd.Series) else y
        y_array = y_array.reshape(-1) # Ensure y is 1D

        # Scale features
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X_array)

        # Create PyTorch tensors
        X_tensor = torch.FloatTensor(X_scaled)
        y_tensor = torch.FloatTensor(y_array)

        # Create validation split
        val_split_prop = 0.2
        if len(X_scaled) < 5:
            logger.warning("Dataset too small for validation split, training on all data.")
            train_indices = np.arange(len(X_scaled))
            val_indices = np.array([], dtype=int)
            X_val_tensor = torch.FloatTensor([]).to(self.device)
            y_val_tensor = torch.FloatTensor([]).to(self.device)
        else:
            train_indices, val_indices = train_test_split(
                np.arange(len(X_scaled)), test_size=val_split_prop, random_state=self.random_state
            )
            X_val_tensor = X_tensor[val_indices].to(self.device)
            y_val_tensor = y_tensor[val_indices].to(self.device)

        # Create Dataloader
        train_dataset = TensorDataset(X_tensor[train_indices], y_tensor[train_indices])
        batch_size = self.training.get("batch_size", 32)
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)

        # Initialize model, loss, optimizer
        input_dim = X_array.shape[1]
        self.model = self._create_model(input_dim)
        criterion = nn.MSELoss()
        optimizer = self._get_optimizer(self.model)

        # Training parameters
        epochs = self.training.get("epochs", 100)
        early_stopping = self.training.get("early_stopping", True) and len(val_indices) > 0
        patience = self.training.get("patience", 10)

        # Initialize history
        self.history = {
            'train_loss': [], 'val_loss': [], 'train_r2': [], 'val_r2': [],
            'train_pcc': [], 'val_pcc': [], 'train_mae': [], 'val_mae': [],
            'learning_rate': []
        }

        # Training loop setup
        best_val_metric = float('inf')
        patience_counter = 0
        best_model_state = None

        logger.info(f"Starting training for {epochs} epochs...")
        # --- Use tqdm directly for the epoch loop ---
        epoch_pbar = tqdm(range(epochs), desc="Training NN", unit="epoch", disable=False, leave=True) # Ensure disable=False, leave=True

        for epoch in epoch_pbar:
            # --- Training phase ---
            self.model.train()
            batch_losses = []
            epoch_train_preds_list = [] # Use list for append efficiency
            epoch_train_targets_list = []

            # --- Use tqdm directly for the batch loop ---
            batch_pbar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{epochs}", leave=False, disable=False) # leave=False for inner loop
            for batch_X, batch_y in batch_pbar:
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                if outputs.shape != batch_y.shape: outputs = outputs.squeeze() # Adjust shape if needed
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()

                batch_losses.append(loss.item())
                epoch_train_preds_list.append(outputs.detach().cpu().numpy())
                epoch_train_targets_list.append(batch_y.detach().cpu().numpy())
                # Update inner progress bar postfix frequently
                batch_pbar.set_postfix(batch_loss=f"{loss.item():.4f}", refresh=False) # No need to refresh constantly

            avg_train_loss = np.mean(batch_losses) if batch_losses else 0.0
            epoch_train_preds = np.concatenate(epoch_train_preds_list) if epoch_train_preds_list else np.array([])
            epoch_train_targets = np.concatenate(epoch_train_targets_list) if epoch_train_targets_list else np.array([])

            # Calculate training metrics
            train_r2, train_mae, train_pcc = 0.0, 0.0, 0.0
            if epoch_train_targets.size > 1 and epoch_train_preds.size == epoch_train_targets.size:
                train_r2 = r2_score(epoch_train_targets, epoch_train_preds)
                train_mae = mean_absolute_error(epoch_train_targets, epoch_train_preds)
                try:
                    pearson_val, _ = pearsonr(epoch_train_targets, epoch_train_preds)
                    train_pcc = 0.0 if np.isnan(pearson_val) else pearson_val
                except ValueError: train_pcc = 0.0

            # --- Validation phase ---
            avg_val_loss, val_r2, val_mae, val_pcc = np.nan, np.nan, np.nan, np.nan
            if len(val_indices) > 0:
                self.model.eval()
                with torch.no_grad():
                     val_outputs = self.model(X_val_tensor)
                     if val_outputs.shape != y_val_tensor.shape: val_outputs = val_outputs.squeeze()
                     val_batch_loss = criterion(val_outputs, y_val_tensor)
                     avg_val_loss = val_batch_loss.item()
                     val_preds = val_outputs.cpu().numpy()
                     val_targets = y_val_tensor.cpu().numpy()

                if val_targets.size > 1:
                    val_r2 = r2_score(val_targets, val_preds)
                    val_mae = mean_absolute_error(val_targets, val_preds)
                    try:
                        pearson_val, _ = pearsonr(val_targets, val_preds)
                        val_pcc = 0.0 if np.isnan(pearson_val) else pearson_val
                    except ValueError: val_pcc = 0.0
                elif val_targets.size == 1: # Handle single point case
                    val_mae = mean_absolute_error(val_targets, val_preds)

            # Store metrics in history
            self.history['train_loss'].append(avg_train_loss)
            self.history['val_loss'].append(avg_val_loss)
            self.history['train_r2'].append(train_r2)
            self.history['val_r2'].append(val_r2)
            self.history['train_pcc'].append(train_pcc)
            self.history['val_pcc'].append(val_pcc)
            self.history['train_mae'].append(train_mae)
            self.history['val_mae'].append(val_mae)
            self.history['learning_rate'].append(optimizer.param_groups[0]['lr'])

            # --- Prepare postfix dictionary ---
            postfix_dict = {
                "trn_loss": f"{avg_train_loss:.4f}",
                "val_loss": f"{avg_val_loss:.4f}" if not np.isnan(avg_val_loss) else "N/A",
                "val_R2": f"{val_r2:.3f}" if not np.isnan(val_r2) else "N/A",
                "val_PCC": f"{val_pcc:.3f}" if not np.isnan(val_pcc) else "N/A",
                "val_MAE": f"{val_mae:.3f}" if not np.isnan(val_mae) else "N/A"
            }
            # --- Explicitly Log the metrics ---
            log_message = f"Epoch {epoch+1} Metrics: " + ", ".join([f"{k}={v}" for k, v in postfix_dict.items()])
            logger.debug(log_message) # Use DEBUG level so it doesn't clutter INFO logs

            # --- Update epoch progress bar ---
            epoch_pbar.set_postfix(postfix_dict, refresh=True) # Force refresh

            # --- Early stopping check ---
            if early_stopping:
                current_val_metric = avg_val_loss
                if not np.isnan(current_val_metric) and current_val_metric < best_val_metric - 1e-5:
                    best_val_metric = current_val_metric
                    patience_counter = 0
                    best_model_state = self.model.state_dict()
                    logger.debug(f"Epoch {epoch+1}: New best validation loss: {best_val_metric:.4f}")
                elif not np.isnan(current_val_metric):
                    patience_counter += 1
                    logger.debug(f"Epoch {epoch+1}: Val loss no improve. Patience: {patience_counter}/{patience}")
                    if patience_counter >= patience:
                        logger.info(f"Early stopping triggered at epoch {epoch+1}.")
                        if best_model_state:
                             logger.info("Restoring best model state.")
                             self.model.load_state_dict(best_model_state)
                        break
        # --- End Epoch Loop ---

        # Close the epoch progress bar cleanly
        epoch_pbar.close()

        # Ensure model is in eval mode after training
        self.model.eval()
        logger.info("Training finished.")
        return self
    # def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series], feature_names: Optional[List[str]] = None) -> 'NeuralNetworkModel':
    #     """
    #     Train the Neural Network model.
        
    #     Args:
    #         X: Feature matrix
    #         y: Target RMSF values
    #         feature_names: Optional list of feature names
            
    #     Returns:
    #         Self, for method chaining
    #     """
    #     # Store feature names if available
    #     if isinstance(X, pd.DataFrame):
    #         self.feature_names_ = X.columns.tolist()
    #     elif feature_names is not None:
    #         self.feature_names_ = feature_names
        
    #     # Convert to numpy array if DataFrame
    #     if isinstance(X, pd.DataFrame):
    #         X_array = X.values
    #     else:
    #         X_array = X
        
    #     # Convert target to numpy array if needed
    #     if isinstance(y, pd.Series):
    #         y_array = y.values
    #     else:
    #         y_array = y
        
    #     # Scale features using StandardScaler
    #     from sklearn.preprocessing import StandardScaler
    #     self.scaler = StandardScaler()
    #     X_scaled = self.scaler.fit_transform(X_array)
        
    #     # Create PyTorch tensors
    #     X_tensor = torch.FloatTensor(X_scaled)
    #     y_tensor = torch.FloatTensor(y_array)
        
    #     # Create validation split (20% of training data)
    #     from sklearn.model_selection import train_test_split
    #     train_indices, val_indices = train_test_split(
    #         np.arange(len(X_scaled)), 
    #         test_size=0.2, 
    #         random_state=self.random_state
    #     )
        
    #     # Create dataset and dataloader for training
    #     train_dataset = TensorDataset(X_tensor[train_indices], y_tensor[train_indices])
    #     batch_size = self.training.get("batch_size", 32)
    #     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
    #     # Initialize model
    #     input_dim = X_array.shape[1]
    #     self.model = self._create_model(input_dim)
        
    #     # Loss function and optimizer
    #     criterion = nn.MSELoss()
    #     optimizer = self._get_optimizer(self.model)
        
    #     # Training parameters
    #     epochs = self.training.get("epochs", 100)
    #     early_stopping = self.training.get("early_stopping", True)
    #     patience = self.training.get("patience", 10)
        
    #     # Initialize training history
    #     self.history = {
    #         'train_loss': [],
    #         'val_loss': [],
    #         'train_r2': [],
    #         'val_r2': [],
    #         'learning_rate': []
    #     }
        
    #     # Training loop
    #     best_loss = float('inf')
    #     patience_counter = 0
        
    #     # Use progress bar for epochs
    #     epoch_pbar = progress_bar(range(epochs), desc="Training NN")
        
    #     for epoch in epoch_pbar:
    #         # Training phase
    #         self.model.train()
    #         train_running_loss = 0.0
    #         train_preds = []
    #         train_targets = []
            
    #         # Track batch progress
    #         batch_pbar = progress_bar(train_dataloader, desc=f"Epoch {epoch+1}/{epochs}", leave=False)
            
    #         for batch_X, batch_y in batch_pbar:
    #             # Move tensors to the configured device
    #             batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                
    #             # Forward pass
    #             outputs = self.model(batch_X)
    #             loss = criterion(outputs, batch_y)
                
    #             # Backward pass and optimize
    #             optimizer.zero_grad()
    #             loss.backward()
    #             optimizer.step()
                
    #             train_running_loss += loss.item() * batch_X.size(0)
                
    #             # Collect predictions and targets for R¬≤ calculation
    #             train_preds.append(outputs.detach().cpu().numpy())
    #             train_targets.append(batch_y.detach().cpu().numpy())
                
    #             # Update batch progress bar
    #             batch_pbar.set_postfix(loss=f"{loss.item():.4f}")
            
    #         # Compute average epoch loss for training
    #         train_epoch_loss = train_running_loss / len(train_dataset)
            
    #         # Concatenate predictions and targets for full training set R¬≤
    #         train_preds = np.concatenate(train_preds)
    #         train_targets = np.concatenate(train_targets)
            
    #         # Calculate R¬≤ for training set
    #         from sklearn.metrics import r2_score
    #         train_r2 = r2_score(train_targets, train_preds)
            
    #         # Validation phase
    #         self.model.eval()
    #         val_loss = 0.0
            
    #         with torch.no_grad():
    #             val_X = X_tensor[val_indices].to(self.device)
    #             val_y = y_tensor[val_indices].to(self.device)
    #             val_outputs = self.model(val_X)
    #             val_batch_loss = criterion(val_outputs, val_y)
    #             val_loss = val_batch_loss.item()
                
    #             # Calculate R¬≤ for validation set
    #             val_preds = val_outputs.cpu().numpy()
    #             val_targets = val_y.cpu().numpy()
    #             val_r2 = r2_score(val_targets, val_preds)
            
    #         # Store metrics in history
    #         self.history['train_loss'].append(train_epoch_loss)
    #         self.history['val_loss'].append(val_loss)
    #         self.history['train_r2'].append(train_r2)
    #         self.history['val_r2'].append(val_r2)
    #         self.history['learning_rate'].append(self.training.get("learning_rate", 0.001))
            
    #         # Update epoch progress bar
    #         epoch_pbar.set_postfix(
    #             train_loss=f"{train_epoch_loss:.4f}",
    #             val_loss=f"{val_loss:.4f}",
    #             val_r2=f"{val_r2:.4f}"
    #         )
            
    #         # Early stopping check
    #         if early_stopping:
    #             if val_loss < best_loss:
    #                 best_loss = val_loss
    #                 patience_counter = 0
    #                 # Save best model state
    #                 best_state = {
    #                     'model_state': self.model.state_dict(),
    #                     'input_dim': input_dim
    #                 }
    #             else:
    #                 patience_counter += 1
    #                 if patience_counter >= patience:
    #                     logger.info(f"Early stopping at epoch {epoch+1}")
    #                     # Restore best model state
    #                     self.model.load_state_dict(best_state['model_state'])
    #                     break
        
    #     return self
    
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Generate RMSF predictions.
        
        Args:
            X: Feature matrix
            
        Returns:
            Array of predicted RMSF values
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Convert to numpy array if DataFrame
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        
        # Scale features
        X_scaled = self.scaler.transform(X_array)
        
        # Convert to tensor
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        
        # Set model to evaluation mode
        self.model.eval()
        
        # Make predictions
        with torch.no_grad():
            predictions = self.model(X_tensor).cpu().numpy()
        
        return predictions
    
    def predict_with_std(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate predictions with uncertainty estimates using MC Dropout.
        
        Args:
            X: Feature matrix
            
        Returns:
            Tuple of (predictions, std_deviation)
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Convert to numpy array if DataFrame
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        
        # Scale features
        X_scaled = self.scaler.transform(X_array)
        
        # Convert to tensor
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        
        # Set model to training mode to enable dropout for MC Dropout
        self.model.train()
        
        # Perform multiple forward passes with dropout enabled
        n_samples = 30  # Number of MC samples
        samples = []
        
        with torch.no_grad():  # No gradients needed
            for _ in range(n_samples):
                predictions = self.model(X_tensor).cpu().numpy()
                samples.append(predictions)
        
        # Calculate mean and standard deviation across samples
        samples = np.stack(samples, axis=0)
        mean_prediction = np.mean(samples, axis=0)
        std_prediction = np.std(samples, axis=0)
        
        return mean_prediction, std_prediction
    
    def hyperparameter_optimize(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        method: str = "bayesian",
        n_trials: int = 20,
        cv: int = 3
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid or distributions
            method: Optimization method ("grid", "random", or "bayesian")
            n_trials: Number of trials for random or bayesian methods
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        if method.lower() == "bayesian":
            # Try to use optuna for Bayesian optimization
            try:
                import optuna
                logger.info("Using Optuna for Bayesian hyperparameter optimization")
                return self._bayesian_optimization(X, y, param_grid, n_trials, cv)
            except ImportError:
                logger.warning("Optuna not available, falling back to random search")
                method = "random"
        
        if method.lower() == "random":
            return self._random_optimization(X, y, param_grid, n_trials, cv)
        
        elif method.lower() == "grid":
            return self._grid_optimization(X, y, param_grid, cv)
        
        else:
            logger.warning(f"Unknown optimization method '{method}', using random search")
            return self._random_optimization(X, y, param_grid, n_trials, cv)
    
    def _random_optimization(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        n_trials: int,
        cv: int
    ) -> Dict[str, Any]:
        """
        Perform random search hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid
            n_trials: Number of trials
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        # Convert to numpy arrays
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
            
        if isinstance(y, pd.Series):
            y_array = y.values
        else:
            y_array = y
        
        # Create KFold cross-validator
        kf = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        
        # Create parameter combinations
        from itertools import product
        import random
        
        # Get all possible combinations
        param_combinations = []
        
        # If hidden_layers is in the grid
        if 'hidden_layers' in param_grid:
            hidden_layer_options = param_grid['hidden_layers']
            rest_params = {k: v for k, v in param_grid.items() if k != 'hidden_layers'}
        else:
            hidden_layer_options = [self.architecture.get('hidden_layers', [64, 32])]
            rest_params = param_grid
            
        # Get combinations of the rest
        keys = list(rest_params.keys())
        values = list(rest_params.values())
        
        for hidden_layer in hidden_layer_options:
            for combination in product(*values):
                param_combo = dict(zip(keys, combination))
                param_combo['hidden_layers'] = hidden_layer
                param_combinations.append(param_combo)
                
        # Limit to n_trials
        if len(param_combinations) > n_trials:
            random.seed(self.random_state)
            param_combinations = random.sample(param_combinations, n_trials)
            
        logger.info(f"Performing random search with {len(param_combinations)} parameter combinations")
        
        # Train and evaluate each combination
        results = []
        
        for i, params in enumerate(progress_bar(param_combinations, desc="Parameter combinations")):
            # Extract architecture and training params
            arch_params = {
                'hidden_layers': params.get('hidden_layers', self.architecture.get('hidden_layers', [64, 32])),
                'activation': params.get('activation', self.architecture.get('activation', 'relu')),
                'dropout': params.get('dropout', self.architecture.get('dropout', 0.2))
            }
            
            train_params = {
                'optimizer': params.get('optimizer', self.training.get('optimizer', 'adam')),
                'learning_rate': params.get('learning_rate', self.training.get('learning_rate', 0.001)),
                'batch_size': params.get('batch_size', self.training.get('batch_size', 32)),
                'epochs': params.get('epochs', self.training.get('epochs', 100)),
                'early_stopping': params.get('early_stopping', self.training.get('early_stopping', True)),
                'patience': params.get('patience', self.training.get('patience', 10))
            }
            
            # Perform cross-validation
            cv_scores = []
            
            for train_idx, val_idx in kf.split(X_array):
                X_train, X_val = X_array[train_idx], X_array[val_idx]
                y_train, y_val = y_array[train_idx], y_array[val_idx]
                
                # Create and train model
                model = NeuralNetworkModel(
                    architecture=arch_params,
                    training=train_params,
                    random_state=self.random_state
                )
                
                # Limit epochs for CV
                model.training['epochs'] = min(model.training['epochs'], 30)
                
                # Train model
                model.fit(X_train, y_train)
                
                # Evaluate on validation set
                preds = model.predict(X_val)
                from sklearn.metrics import mean_squared_error
                score = -mean_squared_error(y_val, preds)  # Negative MSE (higher is better)
                cv_scores.append(score)
            
            # Store results
            mean_score = np.mean(cv_scores)
            results.append((mean_score, arch_params, train_params))
            logger.debug(f"Parameters: {params}, Score: {mean_score:.4f}")
        
        # Find best parameters
        results.sort(reverse=True)  # Higher score is better
        best_score, best_arch, best_train = results[0]
        
        logger.info(f"Best score: {best_score:.4f}")
        logger.info(f"Best architecture parameters: {best_arch}")
        logger.info(f"Best training parameters: {best_train}")
        
        # Update model parameters
        self.architecture = best_arch
        self.training = best_train
        
        # Train final model on all data
        self.fit(X, y)
        
        # Return combined parameters
        return {**best_arch, **best_train}
    
    def _grid_optimization(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        cv: int
    ) -> Dict[str, Any]:
        """
        Perform grid search hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        # Grid search is same as random search but with all combinations
        from itertools import product
        
        # Get all possible combinations
        param_combinations = []
        
        # If hidden_layers is in the grid
        if 'hidden_layers' in param_grid:
            hidden_layer_options = param_grid['hidden_layers']
            rest_params = {k: v for k, v in param_grid.items() if k != 'hidden_layers'}
        else:
            hidden_layer_options = [self.architecture.get('hidden_layers', [64, 32])]
            rest_params = param_grid
            
        # Get combinations of the rest
        keys = list(rest_params.keys())
        values = list(rest_params.values())
        
        for hidden_layer in hidden_layer_options:
            for combination in product(*values):
                param_combo = dict(zip(keys, combination))
                param_combo['hidden_layers'] = hidden_layer
                param_combinations.append(param_combo)
                
        logger.info(f"Performing grid search with {len(param_combinations)} parameter combinations")
        
        # Use random optimization with all combinations
        n_trials = len(param_combinations)
        return self._random_optimization(X, y, param_grid, n_trials, cv)
    
    def _bayesian_optimization(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        n_trials: int,
        cv: int
    ) -> Dict[str, Any]:
        """
        Perform Bayesian hyperparameter optimization using Optuna.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid
            n_trials: Number of trials
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        try:
            import optuna
        except ImportError:
            logger.warning("Optuna not available, falling back to random search")
            return self._random_optimization(X, y, param_grid, n_trials, cv)
        
        # Convert to numpy arrays
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
            
        if isinstance(y, pd.Series):
            y_array = y.values
        else:
            y_array = y
        
        # Create KFold cross-validator
        kf = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        
        # Define objective function for Optuna
        def objective(trial):
            # Suggest hyperparameters
            hidden_layers_options = param_grid.get('hidden_layers', [[64, 32], [128, 64], [32, 16]])
            architecture = {
                'hidden_layers': trial.suggest_categorical('hidden_layers', hidden_layers_options),
                'activation': trial.suggest_categorical('activation', param_grid.get('activation', ['relu', 'leaky_relu'])),
                'dropout': trial.suggest_float('dropout', 
                                            low=min(param_grid.get('dropout', [0.1, 0.5])),
                                            high=max(param_grid.get('dropout', [0.1, 0.5])))
            }
            
            learning_rates = param_grid.get('learning_rate', [0.0001, 0.01])
            batch_sizes = param_grid.get('batch_size', [16, 32, 64])
            patience_range = param_grid.get('patience', [5, 15])
            epochs_range = param_grid.get('epochs', [50, 100])
            optimizer_options = param_grid.get('optimizer', ['adam', 'rmsprop'])
            
            training = {
                'optimizer': trial.suggest_categorical('optimizer', optimizer_options),
                'learning_rate': trial.suggest_float('learning_rate', 
                                                low=min(learning_rates),
                                                high=max(learning_rates),
                                                log=True),
                'batch_size': trial.suggest_categorical('batch_size', batch_sizes),
                'early_stopping': True,
                'patience': trial.suggest_int('patience', 
                                            low=min(patience_range),
                                            high=max(patience_range)),
                'epochs': trial.suggest_int('epochs', 
                                        low=min(epochs_range),
                                        high=max(epochs_range))
            }
            
            # Perform cross-validation
            cv_scores = []
            
            for train_idx, val_idx in kf.split(X_array):
                X_train, X_val = X_array[train_idx], X_array[val_idx]
                y_train, y_val = y_array[train_idx], y_array[val_idx]
                
                # Create and train model
                model = NeuralNetworkModel(
                    architecture=architecture,
                    training=training,
                    random_state=self.random_state
                )
                
                # Limit epochs for CV
                model.training['epochs'] = min(model.training['epochs'], 30)
                
                # Train model
                model.fit(X_train, y_train)
                
                # Evaluate on validation set
                preds = model.predict(X_val)
                from sklearn.metrics import mean_squared_error
                score = -mean_squared_error(y_val, preds)  # Negative MSE (higher is better)
                cv_scores.append(score)
            
            return np.mean(cv_scores)
        
        # Create and run Optuna study
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)
        
        # Get best parameters
        best_params = study.best_params
        logger.info(f"Best score: {study.best_value:.4f}")
        logger.info(f"Best parameters: {best_params}")
        
        # Construct best parameters dictionary
        best_hidden_layers = param_grid.get('hidden_layers', [[64, 32], [128, 64], [32, 16]])
        hidden_layers_idx = 0
        if 'hidden_layers' in best_params:
            for i, layers in enumerate(param_grid.get('hidden_layers', [])):
                if str(layers) == str(best_params['hidden_layers']):
                    hidden_layers_idx = i
                    break
        
        best_arch = {
            'hidden_layers': param_grid.get('hidden_layers', [[64, 32]])[hidden_layers_idx] if param_grid.get('hidden_layers') else [64, 32],
            'activation': best_params.get('activation', self.architecture.get('activation', 'relu')),
            'dropout': best_params.get('dropout', self.architecture.get('dropout', 0.2))
        }
        
        best_train = {
            'optimizer': best_params.get('optimizer', self.training.get('optimizer', 'adam')),
            'learning_rate': best_params.get('learning_rate', self.training.get('learning_rate', 0.001)),
            'batch_size': best_params.get('batch_size', self.training.get('batch_size', 32)),
            'early_stopping': True,
            'patience': best_params.get('patience', self.training.get('patience', 10)),
            'epochs': best_params.get('epochs', self.training.get('epochs', 100))
        }
        
        # Update model parameters
        self.architecture = best_arch
        self.training = best_train
        
        # Train final model on all data
        self.fit(X, y)
        
        # Return combined parameters
        return {**best_arch, **best_train}
    
    def save(self, path: str) -> None:
        """
        Save model to disk.
        
        Args:
            path: Path to save location
        """
        if self.model is None:
            raise RuntimeError("Cannot save untrained model")
        
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
        
        # Save model state
        state = {
            'model_state': self.model.state_dict(),
            'architecture': self.architecture,
            'training': self.training,
            'random_state': self.random_state,
            'feature_names': self.feature_names_,
            'scaler': self.scaler,
            'history': self.history,
            'input_dim': self.model.model[0].in_features  # Get input dimension from first layer
        }
        
        # Save state
        torch.save(state, path)
        logger.info(f"Model saved to {path}")
        
        # Save training history separately as CSV
        if self.history:
            history_path = os.path.splitext(path)[0] + "_history.csv"
            history_df = pd.DataFrame(self.history)
            history_df.to_csv(history_path, index=False)
            logger.info(f"Training history saved to {history_path}")
    
    @classmethod
    def load(cls, path: str) -> 'NeuralNetworkModel':
        """
        Load model state (including scaler) from disk.

        Handles potential PyTorch security changes regarding pickled objects.

        Args:
            path: Path to the saved model file (.pt).

        Returns:
            Loaded NeuralNetworkModel instance.
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found: {path}")

        logger.info(f"Attempting to load model state from: {path}")
        try:
            # Determine device first
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

            # --- MODIFIED: Explicitly set weights_only=False ---
            # This is necessary because we saved the entire state dictionary,
            # including the non-tensor StandardScaler object.
            # Only do this if you trust the source of the checkpoint file.
            state = torch.load(path, map_location=device, weights_only=False)
            # --- END MODIFICATION ---

            logger.debug("Model state loaded successfully.")

            # Validate loaded state dictionary (basic checks)
            required_keys = ['model_state', 'architecture', 'training', 'random_state', 'input_dim']
            if not all(key in state for key in required_keys):
                missing_keys = [key for key in required_keys if key not in state]
                raise ValueError(f"Loaded state dictionary is missing required keys: {missing_keys}")
            if state.get('scaler') is None:
                 logger.warning("Loaded model state does not contain a scaler object. Prediction might fail if data requires scaling.")


            # Create new instance with saved parameters
            # Use .get with defaults for robustness against older save files
            instance = cls(
                architecture=state.get('architecture', {}),
                training=state.get('training', {}),
                random_state=state.get('random_state', 42) # Provide a default random state
            )

            # Restore feature names, scaler, and history
            instance.feature_names_ = state.get('feature_names')
            instance.scaler = state.get('scaler') # Will be None if not saved or key missing
            instance.history = state.get('history')

            # Create and restore model state
            input_dim = state['input_dim']
            if input_dim <= 0:
                # Attempt to infer from feature names if dim wasn't saved correctly
                if instance.feature_names_:
                     input_dim = len(instance.feature_names_)
                     logger.warning(f"Input dimension not found in state, inferred as {input_dim} from feature names.")
                else:
                     raise ValueError("Cannot create model: input_dim missing or invalid in saved state and cannot infer from feature names.")

            instance.model = instance._create_model(input_dim)
            instance.model.load_state_dict(state['model_state'])
            instance.model.to(instance.device) # Ensure model is on the correct device
            instance.model.eval() # Set to evaluation mode

            logger.info(f"Neural Network model '{cls.__name__}' loaded successfully from {path}")
            return instance

        except FileNotFoundError: # Should be caught earlier, but keep for safety
            logger.error(f"Model file not found during load: {path}")
            raise
        except (pickle.UnpicklingError, TypeError, ValueError, KeyError) as load_error:
             # Catch specific load-related errors
             logger.error(f"Error loading or interpreting model state from {path}: {load_error}", exc_info=True)
             # Provide more specific guidance if possible based on error type
             if "weights_only" in str(load_error):
                  logger.error("This might be related to PyTorch's weights_only=True default. Ensure weights_only=False was used if non-tensor objects were saved.")
             raise ValueError(f"Failed to load model state from {path}. File might be corrupt or incompatible.") from load_error
        except Exception as e:
            logger.error(f"An unexpected error occurred loading model from {path}: {e}", exc_info=True)
            raise # Re-raise the original exception
    
    def get_feature_importance(self, X_val=None, y_val=None) -> Optional[Dict[str, float]]:
        """
        Get feature importance values using permutation importance.
        
        Args:
            X_val: Optional validation features for permutation importance
            y_val: Optional validation targets for permutation importance
            
        Returns:
            Dictionary mapping feature names to importance values or None
        """
        if self.model is None:
            return None
        
        # If validation data is provided, use permutation importance
        if X_val is not None and y_val is not None and len(X_val) > 0:
            try:
                from sklearn.inspection import permutation_importance
                
                # Set model to evaluation mode
                self.model.eval()
                
                # Define a prediction function for permutation importance
                def predict_fn(X_test):
                    X_tensor = torch.FloatTensor(self.scaler.transform(X_test)).to(self.device)
                    with torch.no_grad():
                        return self.model(X_tensor).cpu().numpy()
                
                # Calculate permutation importance
                r = permutation_importance(
                    predict_fn, X_val, y_val, 
                    n_repeats=10, 
                    random_state=self.random_state
                )
                
                # Use mean importance as the feature importance
                importance_values = r.importances_mean
                
                # Map to feature names if available
                if self.feature_names_ is not None and len(self.feature_names_) == len(importance_values):
                    return dict(zip(self.feature_names_, importance_values))
                else:
                    return {f"feature_{i}": imp for i, imp in enumerate(importance_values)}
                    
            except Exception as e:
                logger.warning(f"Could not compute permutation importance: {e}")
                # Fall back to weight-based importance
        
        # Fall back to weight-based importance
        try:
            # Get weights from the first layer
            first_layer = self.model.model[0]
            weights = first_layer.weight.data.cpu().numpy()
            
            # Use absolute values of weights as importance
            importance = np.mean(np.abs(weights), axis=0)
            
            # Map to feature names if available
            if self.feature_names_ is not None and len(self.feature_names_) == len(importance):
                return dict(zip(self.feature_names_, importance))
            else:
                return {f"feature_{i}": imp for i, imp in enumerate(importance)}
                
        except Exception as e:
            logger.warning(f"Could not compute feature importance: {e}")
            return None
    
    def get_training_history(self) -> Optional[Dict[str, List[float]]]:
        """
        Get training history if available.
        
        Returns:
            Dictionary with training metrics by epoch, or None if not available
        """
        return self.history
### Data Handling Files ###
---------------------------------------------------------
===== FILE: ensembleflex/data/loader.py =====
# /home/s_felix/ensembleflex/ensembleflex/data/loader.py

"""
Data loading utilities for the EnsembleFlex ML pipeline.

This module provides functions for loading protein data from various formats.
Temperature-specific loading logic has been removed for the unified model approach.
"""

import os
import logging
# import re # No longer needed
from typing import List, Dict, Any, Optional # Union, Tuple no longer needed here
from functools import lru_cache

import pandas as pd
import numpy as np
# import glob # No longer needed

logger = logging.getLogger(__name__)

# Removed: list_data_files
# Removed: get_temperature_files

def detect_file_format(file_path: str) -> str:
    """
    Detect file format based on extension and content.

    Args:
        file_path: Path to data file

    Returns:
        Format string ('csv', 'tsv', 'pickle', etc.)
    """
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()

    if ext == '.csv':
        return 'csv'
    elif ext == '.tsv':
        return 'tsv'
    elif ext in ['.pkl', '.pickle']:
        return 'pickle'
    elif ext == '.json':
        return 'json'
    elif ext == '.parquet':
        return 'parquet'
    elif ext == '.h5':
        return 'hdf5'
    else:
        # Try to detect CSV/TSV by reading first line
        try:
            with open(file_path, 'r') as f:
                first_line = f.readline()
                if '\t' in first_line:
                    return 'tsv'
                elif ',' in first_line:
                    return 'csv'
        except:
            pass

        # Default to CSV if can't determine
        logger.warning(f"Could not determine format for {file_path}, defaulting to CSV")
        return 'csv'

@lru_cache(maxsize=16)
def load_file(file_path: str, **kwargs) -> pd.DataFrame:
    """
    Load data from a file with format auto-detection. Uses LRU cache.

    Args:
        file_path: Path to data file
        **kwargs: Additional arguments to pass to pandas

    Returns:
        Loaded DataFrame

    Raises:
        FileNotFoundError: If file doesn't exist
        ValueError: If file format is not supported or loading fails
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Data file not found: {file_path}")

    # Detect format
    file_format = detect_file_format(file_path)
    logger.debug(f"Detected format '{file_format}' for file: {file_path}")

    try:
        # Load based on format
        if file_format == 'csv':
            df = pd.read_csv(file_path, **kwargs)
        elif file_format == 'tsv':
            df = pd.read_csv(file_path, sep='\t', **kwargs)
        elif file_format == 'pickle':
            df = pd.read_pickle(file_path, **kwargs)
        elif file_format == 'json':
            df = pd.read_json(file_path, **kwargs)
        elif file_format == 'parquet':
            df = pd.read_parquet(file_path, **kwargs)
        elif file_format == 'hdf5':
            df = pd.read_hdf(file_path, **kwargs)
        else:
            # Should not happen if detect_file_format is robust
            raise ValueError(f"Unsupported file format: {file_format}")

        logger.info(f"Successfully loaded file: {file_path} (shape: {df.shape})")
        return df

    except Exception as e:
        logger.error(f"Error loading file {file_path}: {e}", exc_info=True)
        raise ValueError(f"Failed to load file {file_path}: {e}")

def merge_data_files(file_paths: List[str], **kwargs) -> pd.DataFrame:
    """
    Merge multiple data files into a single DataFrame.
    Note: Primarily used by the aggregation script, less relevant for main pipeline now.

    Args:
        file_paths: List of paths to data files
        **kwargs: Additional arguments to pass to pandas

    Returns:
        Merged DataFrame
    """
    if not file_paths:
        raise ValueError("No files provided for merging")

    # Load and concatenate files
    dfs = []
    logger.info(f"Attempting to merge {len(file_paths)} files.")
    for file_path in file_paths:
        try:
            # Use the cached loader
            df = load_file(file_path, **kwargs)
            dfs.append(df)
        except Exception as e:
            logger.warning(f"Skipping file {file_path} during merge due to error: {e}")

    if not dfs:
        raise ValueError("No data files could be loaded for merging")

    logger.info(f"Successfully loaded {len(dfs)} files for merging. Concatenating...")
    merged_df = pd.concat(dfs, ignore_index=True)
    logger.info(f"Merging complete. Final shape: {merged_df.shape}")
    return merged_df

def validate_data_columns(df: pd.DataFrame, required_columns: List[str]) -> bool:
    """
    Validate that DataFrame contains all required columns.

    Args:
        df: DataFrame to validate
        required_columns: List of required column names

    Returns:
        True if all required columns are present, False otherwise
    """
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        logger.warning(f"Missing required columns in DataFrame: {missing_columns}")
        return False
    else:
        logger.debug("All required columns found in DataFrame.")
        return True

# Removed: load_temperature_data
# Removed: load_all_temperature_data

def summarize_data(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Generate summary statistics for a dataset.

    Args:
        df: Input DataFrame

    Returns:
        Dictionary of summary statistics
    """
    if df is None or df.empty:
        logger.warning("Cannot summarize empty or None DataFrame.")
        return {}

    summary = {
        "num_rows": len(df),
        "num_columns": len(df.columns),
        "columns": list(df.columns),
        "memory_usage": None,
        "domains": None,
        "residues_per_domain_rows": None, # Renamed for clarity
        "temperatures": None, # Added for aggregated data
        "column_types": {},
        "missing_values": {},
    }

    # Memory usage
    try:
        memory_bytes = df.memory_usage(deep=True).sum()

        if memory_bytes < 1024:
            summary["memory_usage"] = f"{memory_bytes} bytes"
        elif memory_bytes < 1024**2:
            summary["memory_usage"] = f"{memory_bytes / 1024:.2f} KB"
        elif memory_bytes < 1024**3:
            summary["memory_usage"] = f"{memory_bytes / (1024**2):.2f} MB"
        else:
            summary["memory_usage"] = f"{memory_bytes / (1024**3):.2f} GB"
    except Exception as e:
        logger.warning(f"Could not estimate memory usage: {e}")


    # Domain statistics if domain_id is present
    if "domain_id" in df.columns:
        try:
            domains = df["domain_id"].unique()
            summary["domains"] = {
                "count": len(domains),
                "examples": list(domains[:5])
            }
            # Note: In aggregated data, this counts rows per domain (residues * temperatures)
            # A more accurate 'residues per domain' requires grouping by ('domain_id', 'temperature') first
            residue_counts = df.groupby("domain_id").size()
            summary["residues_per_domain_rows"] = { # Renamed key
                "min": residue_counts.min(),
                "max": residue_counts.max(),
                "mean": residue_counts.mean(),
                "median": residue_counts.median()
            }
        except Exception as e:
             logger.warning(f"Could not calculate domain statistics: {e}")


    # Temperature statistics
    if "temperature" in df.columns:
         try:
             temps = df["temperature"].unique()
             summary["temperatures"] = {
                 "count": len(temps),
                 "values": sorted([t for t in temps if pd.notna(t)]),
                 "has_nan": df["temperature"].isnull().any()
             }
         except Exception as e:
             logger.warning(f"Could not calculate temperature statistics: {e}")

    # Column types and missing values
    for col in df.columns:
        try:
            summary["column_types"][col] = str(df[col].dtype)
            missing = df[col].isna().sum()
            if missing > 0:
                summary["missing_values"][col] = {
                    "count": missing,
                    "percentage": (missing / len(df)) * 100
                }
        except Exception as e:
            logger.warning(f"Could not process column '{col}' for summary: {e}")


    # Check for specific columns (no longer temperature-specific RMSF)
    if "rmsf" in df.columns:
        summary["target_column"] = "rmsf"

    omniflex_columns = ["esm_rmsf", "voxel_rmsf"]
    found_omniflex_columns = [col for col in omniflex_columns if col in df.columns]
    if found_omniflex_columns:
        summary["omniflex_columns"] = found_omniflex_columns

    return summary

def log_data_summary(summary: Dict[str, Any]) -> None:
    """
    Log a summary of dataset statistics.

    Args:
        summary: Dictionary of summary statistics generated by summarize_data
    """
    if not summary:
        logger.info("No data summary to log.")
        return

    logger.info("=== Dataset Summary ===")
    logger.info(f"Rows: {summary.get('num_rows', 'N/A')}, Columns: {summary.get('num_columns', 'N/A')}")

    if summary.get("memory_usage"):
        logger.info(f"Memory usage: {summary['memory_usage']}")

    if summary.get("domains"):
        logger.info(f"Domains: {summary['domains']['count']} unique domains")
        # Ensure examples are strings for joining
        logger.info(f"Examples: {', '.join(map(str, summary['domains']['examples']))}")

    if summary.get("residues_per_domain_rows"): # Using the renamed key
        stats = summary["residues_per_domain_rows"]
        logger.info(f"Rows per domain (Residues * Temps): min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}, median={stats['median']:.1f}")

    if summary.get("temperatures"):
        stats = summary["temperatures"]
        logger.info(f"Temperatures: {stats['count']} unique values detected.")
        logger.info(f"Numeric Temps Found: {stats['values']}")
        if stats['has_nan']:
            logger.warning("NaN temperature values detected (potentially from 'average' files).")

    missing_stats = summary.get("missing_values", {})
    if missing_stats:
        logger.info("Columns with missing values:")
        for col, stats in missing_stats.items():
            logger.info(f"  {col}: {stats['count']} missing ({stats['percentage']:.1f}%)")
    else:
        logger.info("No missing values detected.")

    if summary.get("target_column"):
        logger.info(f"Target column: {summary['target_column']}")

    if summary.get("omniflex_columns"):
        logger.info(f"OmniFlex columns found: {', '.join(summary['omniflex_columns'])}")

    logger.info("========================")
===== FILE: ensembleflex/data/processor.py =====
# /home/s_felix/ensembleflex/ensembleflex/data/processor.py

"""
Data processing for the EnsembleFlex ML pipeline.

This module provides functions for preprocessing aggregated protein data,
including cleaning, feature engineering, window-based feature generation,
data splitting, and preparation for machine learning models.
Temperature-specific logic has been removed for the unified model approach.
"""

import os
import logging
# from functools import lru_cache # Not used directly here
from typing import Dict, List, Tuple, Optional, Any, Union

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split # For split_data

# Updated imports for ensembleflex structure
from ensembleflex.data.loader import load_file, validate_data_columns # Removed load_temperature_data, lru_cache
from ensembleflex.utils.helpers import progress_bar # Assuming progress_bar helper exists

logger = logging.getLogger(__name__)

# --- Amino Acid and Secondary Structure Mappings ---
# (Keep these mappings as they are based on fundamental properties)
AA_MAP = {
    'ALA': 1, 'ARG': 2, 'ASN': 3, 'ASP': 4, 'CYS': 5, 'GLN': 6,
    'GLU': 7, 'GLY': 8, 'HIS': 12, 'HSD': 9, 'HSE': 10, 'HSP': 11, # Group Histidines
    'ILE': 13, 'LEU': 14, 'LYS': 15, 'MET': 16, 'PHE': 17, 'PRO': 23,
    'SER': 18, 'THR': 19, 'TRP': 20, 'TYR': 21, 'VAL': 22,
    'UNK': 0, None: 0 # Handle unknown and potential None values
}

SS_MAP = {
    'H': 0, 'G': 0, 'I': 0, # Helices
    'E': 1, 'B': 1,         # Strands
    'T': 2, 'S': 2, 'C': 2, # Coil/Turn/Bend
    '-': 2, None: 2         # Unknown/None default to Coil
}

# --- Data Cleaning and Basic Feature Engineering ---

def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Cleans input DataFrame by handling missing values and ensuring types.
    (No fundamental changes needed for aggregated data structure).

    Args:
        df: Input DataFrame with raw protein data.

    Returns:
        Cleaned DataFrame.
    """
    logger.debug(f"Starting data cleaning on DataFrame with shape {df.shape}.")
    cleaned_df = df.copy()

    # Define fill values for different column types/meanings
    fill_values = {
        'dssp': 'C',                         # Default SS to Coil
        'relative_accessibility': 0.5,       # Moderate accessibility
        'core_exterior': 'core',             # Assume core if unknown
        'phi': 0.0,                          # Neutral angle
        'psi': 0.0,                          # Neutral angle
        'esm_rmsf': lambda d: d.mean(),      # Fill with mean of existing values
        'voxel_rmsf': lambda d: d.mean(),    # Fill with mean of existing values
        'temperature': lambda d: d.median()  # Fill missing temp with median? Or 0? Or raise error? Median seems safer.
        # Add other columns needing specific fill logic here
    }

    for col, fill_val in fill_values.items():
        if col in cleaned_df.columns:
            nan_count = cleaned_df[col].isna().sum()
            if nan_count == 0:
                continue # Skip if no NaNs

            if callable(fill_val): # Handle functions like mean(), median()
                try:
                    calculated_val = fill_val(cleaned_df[col].dropna())
                    if pd.isna(calculated_val): # Handle case where all values were NaN
                         calculated_val = 0.0 if col in ['esm_rmsf', 'voxel_rmsf', 'temperature', 'relative_accessibility', 'phi', 'psi'] else 'UNK' # Fallback
                         logger.warning(f"All values in '{col}' were NaN. Filling with fallback: {calculated_val}")

                    if col in ['esm_rmsf', 'voxel_rmsf', 'temperature', 'relative_accessibility', 'phi', 'psi']: # Ensure numeric type before filling
                        cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')

                    cleaned_df[col].fillna(calculated_val, inplace=True)
                    logger.debug(f"Filled {nan_count} NaNs in '{col}' with calculated value: {calculated_val:.4f}")

                except Exception as e:
                    logger.error(f"Failed to calculate fill value for '{col}': {e}. Skipping fill.")

            else: # Handle fixed fill values
                if col in ['esm_rmsf', 'voxel_rmsf', 'temperature', 'relative_accessibility', 'phi', 'psi']: # Ensure numeric type
                    cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')
                cleaned_df[col].fillna(fill_val, inplace=True)
                logger.debug(f"Filled {nan_count} NaNs in '{col}' with default value: {fill_val}")

    # Ensure temperature is float after potential filling
    if 'temperature' in cleaned_df.columns:
        cleaned_df['temperature'] = cleaned_df['temperature'].astype(float)

    logger.debug(f"Data cleaning finished. DataFrame shape: {cleaned_df.shape}")
    return cleaned_df

def _add_derived_features(df: pd.DataFrame) -> pd.DataFrame:
    """Adds derived features like normalized position and encoded categories.
       (No changes needed here, operates row-wise or on existing columns).
    """
    logger.debug("Adding derived features...")
    derived_df = df.copy()

    # 1. Normalized Residue Position
    if 'normalized_resid' not in derived_df.columns and 'resid' in derived_df.columns and 'domain_id' in derived_df.columns:
        # Calculate max - min per group, handle single-residue domains (max-min=0)
        grp = derived_df.groupby('domain_id')['resid']
        min_res = grp.transform('min')
        max_minus_min = (grp.transform('max') - min_res).replace(0, 1) # Avoid division by zero
        derived_df['normalized_resid'] = (derived_df['resid'] - min_res) / max_minus_min
        logger.debug("Added 'normalized_resid'.")

    # 2. Encoded Residue Name
    if 'resname' in derived_df.columns and 'resname_encoded' not in derived_df.columns:
        derived_df['resname_encoded'] = derived_df['resname'].map(AA_MAP).fillna(0).astype(int)
        logger.debug("Added 'resname_encoded'.")

    # 3. Encoded Core/Exterior
    if 'core_exterior' in derived_df.columns and 'core_exterior_encoded' not in derived_df.columns:
        # Ensure 'core' maps to 0, others (e.g., 'surface', 'exterior') to 1
        derived_df['core_exterior_encoded'] = derived_df['core_exterior'].apply(
            lambda x: 0 if isinstance(x, str) and x.lower() == 'core' else 1
        ).astype(int)
        logger.debug("Added 'core_exterior_encoded'.")

    # 4. Encoded Secondary Structure
    if 'dssp' in derived_df.columns and 'secondary_structure_encoded' not in derived_df.columns:
        # Apply mapping, fill potential NaNs introduced by unknown DSSP codes
        derived_df['secondary_structure_encoded'] = derived_df['dssp'].map(SS_MAP).fillna(2).astype(int)
        logger.debug("Added 'secondary_structure_encoded'.")

    # 5. Normalized Phi/Psi Angles
    if 'phi' in derived_df.columns and 'phi_norm' not in derived_df.columns:
        # Normalize angle to [-1, 1] using sine for cyclical nature
        derived_df['phi_norm'] = np.sin(np.radians(derived_df['phi'].fillna(0)))
        logger.debug("Added 'phi_norm'.")
    if 'psi' in derived_df.columns and 'psi_norm' not in derived_df.columns:
        derived_df['psi_norm'] = np.sin(np.radians(derived_df['psi'].fillna(0)))
        logger.debug("Added 'psi_norm'.")

    return derived_df


# --- Domain Filtering ---

def filter_domains(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    Filters DataFrame based on domain inclusion/exclusion and size rules.
    (No changes needed, operates on 'domain_id' and calculates size dynamically).

    Args:
        df: Input DataFrame with protein data.
        config: Configuration dictionary with domain filtering rules.

    Returns:
        Filtered DataFrame.
    """
    domain_config = config.get("dataset", {}).get("domains", {})
    if not domain_config: # No filtering rules specified
        logger.info("No domain filtering rules specified in config.")
        return df.copy()

    filtered_df = df.copy()
    initial_rows = len(filtered_df)
    logger.debug(f"Starting domain filtering from {initial_rows} rows.")

    # Apply inclusion/exclusion lists
    include_domains = set(domain_config.get("include", []))
    exclude_domains = set(domain_config.get("exclude", []))

    if include_domains:
        filtered_df = filtered_df[filtered_df['domain_id'].isin(include_domains)]
        logger.debug(f"Applied inclusion filter: {len(filtered_df)} rows remain.")
    if exclude_domains:
        filtered_df = filtered_df[~filtered_df['domain_id'].isin(exclude_domains)]
        logger.debug(f"Applied exclusion filter: {len(filtered_df)} rows remain.")

    # Apply size filters (calculate sizes only if needed)
    # NOTE: 'protein_size' in aggregated data refers to the size of the original protein,
    # not the number of rows per domain in the aggregated file. We need to calculate
    # the actual number of unique residues per domain if size filtering is based on that.
    min_size = domain_config.get("min_protein_size", 0)
    max_size = domain_config.get("max_protein_size") # None means no upper limit

    if min_size > 0 or max_size is not None:
        if filtered_df.empty:
             logger.warning("DataFrame is empty before size filtering.")
        else:
            # Calculate unique residues per domain for size filtering
            domain_residue_counts = filtered_df.groupby('domain_id')['resid'].transform('nunique')
            if min_size > 0:
                filtered_df = filtered_df[domain_residue_counts >= min_size]
                logger.debug(f"Applied min residue count filter ({min_size}): {len(filtered_df)} rows remain.")
            if max_size is not None:
                filtered_df = filtered_df[domain_residue_counts <= max_size]
                logger.debug(f"Applied max residue count filter ({max_size}): {len(filtered_df)} rows remain.")

    final_rows = len(filtered_df)
    if final_rows < initial_rows:
         logger.info(f"Domain filtering removed {initial_rows - final_rows} rows. {final_rows} rows remaining.")
    else:
         logger.debug("No rows removed by domain filtering.")

    return filtered_df

# --- Window Feature Generation (Optimized) ---

def create_window_features_optimized(
    df: pd.DataFrame,
    window_size: int,
    feature_cols: List[str]
) -> pd.DataFrame:
    """
    Creates window-based features using vectorized operations and pd.concat.
    (No changes needed here, groups by domain_id and sorts by resid,
     ignoring other columns like temperature).

    This version is optimized to reduce DataFrame fragmentation warnings and
    improve performance compared to iterative column addition.

    Args:
        df: Input DataFrame with protein data, indexed uniquely.
        window_size: Number of residues on each side (k). Window is size 2k+1.
        feature_cols: List of *existing* feature column names in df to use.
                      **IMPORTANT:** Should *not* include 'temperature'.

    Returns:
        DataFrame with added window features.
    """
    if 'domain_id' not in df.columns or 'resid' not in df.columns:
        raise ValueError("DataFrame must contain 'domain_id' and 'resid' columns for windowing.")
    if window_size <= 0:
        logger.warning("Window size must be positive. Skipping window feature creation.")
        return df.copy()

    original_cols = set(df.columns)
    # Only use features that actually exist in the input df
    valid_feature_cols = [col for col in feature_cols if col in original_cols]

    if not valid_feature_cols:
        logger.warning("No valid feature columns found for windowing.")
        return df.copy()

    all_domain_windows_dfs = [] # Collect DataFrames of window features per domain
    logger.info(f"Creating window features (k={window_size}) for {len(valid_feature_cols)} features using optimized method...")

    # Ensure the DataFrame index is unique if it's not already the default RangeIndex
    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_unique:
        logger.debug("Resetting index for reliable window feature alignment.")
        df_indexed = df.reset_index(drop=True) # Work with a re-indexed copy
    else:
        df_indexed = df # Use original if index is okay

    # Process each domain
    # Grouping by domain_id handles the structure correctly even with multiple temperatures per residue
    grouped_domains = df_indexed.groupby('domain_id', observed=False, sort=False) # sort=False might be faster
    for domain_id, domain_indices in progress_bar(grouped_domains.groups.items(), desc="Processing Domains"):
        # Sorting by resid is crucial for correct offsets within the sequence dimension
        domain_df = df_indexed.loc[domain_indices].sort_values('resid')
        domain_window_series_dict = {} # Collect {col_name: pd.Series} for this domain

        for feature in valid_feature_cols:
            feature_values = domain_df[feature].values # Efficient NumPy array access
            dtype = feature_values.dtype
            # Use a sensible default fill value based on dtype (0 for int, NaN for float)
            default_fill = 0 if np.issubdtype(dtype, np.integer) else np.nan

            for offset in range(-window_size, window_size + 1):
                if offset == 0: continue # Skip self

                col_name = f"{feature}_offset_{offset}"
                # Pre-allocate array with default fill value
                offset_values = np.full_like(feature_values, fill_value=default_fill, dtype=dtype)

                # Apply shifts using slicing (vectorized)
                if offset < 0: # Look backwards (use values from start)
                    offset_values[-offset:] = feature_values[:offset]
                else: # Look forwards (use values towards end)
                    offset_values[:-offset] = feature_values[offset:]

                # Store as Series with the correct index from domain_df
                domain_window_series_dict[col_name] = pd.Series(offset_values, index=domain_df.index, name=col_name)

        # Combine all window series for this domain into one DataFrame
        if domain_window_series_dict:
            all_domain_windows_dfs.append(pd.concat(domain_window_series_dict.values(), axis=1))

    # Concatenate window features from all domains (aligns by original index)
    if not all_domain_windows_dfs:
         logger.warning("No window features were generated.")
         return df.copy() # Return a copy of the original df

    logger.info("Concatenating window features for all domains...")
    window_features_combined = pd.concat(all_domain_windows_dfs)

    # Merge window features back to the *original* df using the index
    result_df = pd.concat([df, window_features_combined], axis=1)

    # --- NaN Filling (After all columns are combined) ---
    all_new_col_names = list(window_features_combined.columns)
    logger.info(f"Added {len(all_new_col_names)} window feature columns. Filling NaNs...")

    # Pre-calculate fill values for base features to avoid repeated computation
    base_feature_fill_values = {}
    for col in all_new_col_names:
        base_feature = col.split('_offset_')[0]
        if base_feature not in base_feature_fill_values and base_feature in result_df.columns:
            # Use median for floats, mode for integers/objects
            if np.issubdtype(result_df[base_feature].dtype, np.floating):
                fill_val = result_df[base_feature].median()
                fill_type = "median"
                if pd.isna(fill_val): fill_val = 0.0; fill_type = "fallback (0.0)"
            elif np.issubdtype(result_df[base_feature].dtype, np.integer):
                try: fill_val = result_df[base_feature].mode()[0]; fill_type = "mode"
                except IndexError: fill_val = 0; fill_type = "fallback (0)"
            else: # Object or other types
                 try: fill_val = result_df[base_feature].mode()[0]; fill_type = "mode"
                 except IndexError: fill_val = ''; fill_type = "fallback ('')" # Use empty string for objects
            logger.debug(f"Using {fill_type} ({fill_val}) for base feature {base_feature}")
            base_feature_fill_values[base_feature] = fill_val

    # Fill NaNs using pre-calculated values
    for col in all_new_col_names:
        if col not in result_df.columns: continue # Should not happen

        base_feature = col.split('_offset_')[0]
        fill_val = base_feature_fill_values.get(base_feature, 0) # Default to 0 if base feature somehow missed

        nan_count = result_df[col].isna().sum()
        if nan_count > 0:
             result_df[col].fillna(fill_val, inplace=True)
             # logger.debug(f"Filled {nan_count} NaNs in '{col}' with {fill_val}.")


    logger.info("Window feature creation and NaN filling complete (Optimized).")
    return result_df

# --- Main Processing Function ---

def process_features(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    Processes features: cleaning, deriving features, adding window features.
    Operates on the aggregated DataFrame.

    Args:
        df: Input **aggregated** DataFrame with raw protein data and temperature column.
        config: Configuration dictionary.

    Returns:
        DataFrame with processed features ready for filtering/splitting.
    """
    try:
        logger.info(f"Starting feature processing for DataFrame with shape {df.shape}.")
        # 1. Basic Cleaning (includes handling temperature column NaNs)
        cleaned_df = clean_data(df)

        # 2. Add Derived Features (Encodings, Normalizations)
        derived_df = _add_derived_features(cleaned_df)

        # 3. Add Protein Size (if necessary and possible)
        if "protein_size" not in derived_df.columns and 'domain_id' in derived_df.columns and 'resid' in derived_df.columns:
            logger.debug("Calculating 'protein_size'...")
            # Calculate size based on unique residues per domain
            derived_df["protein_size"] = derived_df.groupby("domain_id")["resid"].transform("nunique")
        elif "protein_size" not in derived_df.columns:
             logger.warning("Cannot calculate 'protein_size': missing 'domain_id' or 'resid'.")

        # 4. Ensure Target Column is Numeric and filled
        target_col = config["dataset"]["target"] # Should be "rmsf"
        if target_col in derived_df.columns:
            initial_nan_count = derived_df[target_col].isna().sum()
            derived_df[target_col] = pd.to_numeric(derived_df[target_col], errors='coerce')
            # Fill NaNs in target with 0.0 - Check if this is appropriate for the task!
            # Alternatively, drop rows: derived_df.dropna(subset=[target_col], inplace=True)
            fill_target_val = 0.0
            final_nan_count = derived_df[target_col].isna().sum()
            if final_nan_count > 0:
                derived_df[target_col].fillna(fill_target_val, inplace=True)
                logger.warning(f"Filled {final_nan_count} NaN values in target column '{target_col}' with {fill_target_val}.")
            elif initial_nan_count > 0 and final_nan_count == 0: # Only log if NaNs were actually present and filled
                 logger.debug(f"NaNs in target column '{target_col}' handled (filled/coerced).")

        else:
            logger.error(f"Target column '{target_col}' not found in data after cleaning/deriving features!")
            raise ValueError(f"Target column '{target_col}' missing.")

        # 5. Identify Active Features for Windowing (based on config *after* derivation)
        feature_config = config["dataset"]["features"]
        use_features_config = feature_config.get("use_features", {})
        active_features_for_windowing = []
        for feature, enabled in use_features_config.items():
            # Check if feature exists *now*, is enabled, AND is NOT the temperature column
            if enabled and feature in derived_df.columns and feature != 'temperature':
                active_features_for_windowing.append(feature)
            elif enabled and feature == 'temperature':
                logger.debug(f"Feature '{feature}' is enabled but explicitly excluded from windowing.")
            elif enabled and feature not in derived_df.columns:
                logger.warning(f"Feature '{feature}' is enabled but not found after deriving features.")
        logger.info(f"Identified {len(active_features_for_windowing)} base features for windowing.")

        # 6. Add Window Features (using optimized method)
        window_config = feature_config.get("window", {})
        processed_df = derived_df # Start with the derived features df
        if window_config.get("enabled", False):
            window_size = window_config.get("size", 3)
            if window_size > 0 and active_features_for_windowing:
                 processed_df = create_window_features_optimized(
                     processed_df, window_size, active_features_for_windowing
                 )
            else:
                 logger.warning("Windowing enabled but size <= 0 or no active base features found. Skipping.")
                 processed_df = processed_df.copy() # Ensure consistency by returning a copy
        else:
             logger.info("Window feature creation skipped (disabled in config).")
             processed_df = processed_df.copy() # Return a copy even if windowing disabled

        logger.info(f"Feature processing complete. Final shape: {processed_df.shape}")
        return processed_df

    except Exception as e:
        logger.exception(f"Feature processing failed catastrophically: {e}")
        # Return a copy of the original DataFrame on error to prevent side effects
        return df.copy()


# --- Data Preparation for Models ---

def prepare_data_for_model(
    df: pd.DataFrame,
    config: Dict[str, Any],
    include_target: bool = True
) -> Tuple[np.ndarray, Optional[np.ndarray], List[str]]:
    """
    Prepares final data matrices (X, y) and feature names for model input.

    Selects features based on config, including generated window features and
    the 'temperature' feature if enabled.

    Args:
        df: Processed DataFrame containing all potential features.
        config: Configuration dictionary.
        include_target: Whether to include the target variable (y) in the output.

    Returns:
        Tuple containing:
        - X (np.ndarray): Feature matrix.
        - y (Optional[np.ndarray]): Target vector (or None if include_target=False).
        - feature_names (List[str]): List of names for columns in X.

    Raises:
        ValueError: If the target column is missing when include_target is True,
                    or if no valid features are selected.
    """
    logger.debug("Preparing data for model...")
    feature_config = config["dataset"]["features"]
    use_features_config = feature_config.get("use_features", {})
    window_config = feature_config.get("window", {})
    window_enabled = window_config.get("enabled", False)
    window_size = window_config.get("size", 0) if window_enabled else 0

    # 1. Determine base features enabled in config (excluding temperature initially)
    base_features_enabled = {
        feature for feature, enabled in use_features_config.items()
        if enabled and feature in df.columns and feature != 'temperature'
    }
    logger.debug(f"Base features (excluding temp) enabled & present: {sorted(list(base_features_enabled))}")

    # 2. Determine window features to include (if enabled) based on base features
    window_feature_names = []
    if window_enabled and window_size > 0:
        for base_feature in base_features_enabled:
            for offset in range(-window_size, window_size + 1):
                if offset == 0: continue
                col_name = f"{base_feature}_offset_{offset}"
                if col_name in df.columns:
                    window_feature_names.append(col_name)
        logger.debug(f"Found {len(window_feature_names)} potential window features.")

    # 3. Combine base and window features
    final_feature_names = sorted(list(base_features_enabled)) + sorted(window_feature_names)

    # 4. Explicitly check and add 'temperature' feature if enabled
    if use_features_config.get('temperature', False):
        if 'temperature' in df.columns:
            final_feature_names.append('temperature')
            logger.debug("Including 'temperature' as an input feature.")
        else:
            logger.warning("'temperature' feature enabled in config but column not found in data!")
    else:
        logger.debug("'temperature' feature is not enabled in config.")


    # 5. Sanity check and prepare X matrix
    missing_final_features = [f for f in final_feature_names if f not in df.columns]
    if missing_final_features:
        logger.warning(f"Some selected features are missing from the final DataFrame: {missing_final_features}. They will be excluded.")
        final_feature_names = [f for f in final_feature_names if f in df.columns]

    if not final_feature_names:
        raise ValueError("No valid features selected or available for the model.")

    logger.info(f"Preparing feature matrix X with {len(final_feature_names)} columns: {final_feature_names}")
    # Select columns and convert to NumPy, ensuring numeric types where possible
    X_df = df[final_feature_names].copy() # Use copy to avoid SettingWithCopyWarning
    # Attempt conversion, coercing errors - check dtypes afterwards
    for col in X_df.columns:
        if X_df[col].dtype == 'object':
             try:
                 X_df[col] = pd.to_numeric(X_df[col], errors='coerce').fillna(0) # Fill coerced NaNs with 0
             except ValueError: logger.warning(f"Could not convert object column '{col}' to numeric. It might cause issues.")
        elif pd.api.types.is_bool_dtype(X_df[col].dtype):
             X_df[col] = X_df[col].astype(int) # Convert bools to int
        # Ensure temperature is float
        elif col == 'temperature':
             X_df[col] = X_df[col].astype(float)

    # Check final dtypes
    non_numeric_cols = X_df.select_dtypes(exclude=np.number).columns
    if len(non_numeric_cols) > 0:
        logger.warning(f"Non-numeric columns remain in feature matrix: {list(non_numeric_cols)}. Models might fail.")

    X = X_df.values

    # 6. Prepare target vector y (if requested)
    y = None
    if include_target:
        target_col = config["dataset"]["target"] # Assumes target name is "rmsf"
        if target_col in df.columns:
            # Ensure target is numeric and handle potential NaNs (e.g., fill with 0 or mean)
            y_series = pd.to_numeric(df[target_col], errors='coerce')
            nan_count = y_series.isna().sum()
            if nan_count > 0:
                # Consider if 0.0 is appropriate, or maybe mean/median of the target column
                fill_val = y_series.median() if not y_series.dropna().empty else 0.0
                y_series.fillna(fill_val, inplace=True)
                logger.warning(f"Filled {nan_count} NaNs in target column '{target_col}' with median value ({fill_val:.4f}) before creating y vector.")
            y = y_series.values
        else:
            raise ValueError(f"Target column '{target_col}' not found in DataFrame for y preparation.")

    logger.debug(f"Data preparation complete. X shape: {X.shape}, y shape: {y.shape if y is not None else 'None'}")
    return X, y, final_feature_names


# --- Main Loading and Processing Function ---


def load_and_process_data(
    data_input: Union[str, pd.DataFrame, None] = None, # Accept path OR DataFrame
    config: Dict[str, Any] = None,
) -> pd.DataFrame:
    """
    Loads data (from path or DataFrame), validates required columns,
    processes features, and filters domains based on the provided config.

    This is the main entry point for getting processed data ready for splitting or prediction.

    Args:
        data_input: Path to the aggregated data file (str) OR a pandas DataFrame.
                    If None, it attempts to load from the path specified in the config.
        config: Configuration dictionary (required).

    Returns:
        Processed and filtered DataFrame.

    Raises:
        FileNotFoundError: If a specified data file path does not exist.
        ValueError: If config is missing, required columns are absent in loaded data,
                    or data_input type is invalid.
        TypeError: If data_input is not a str, DataFrame, or None.
    """
    if not config:
         raise ValueError("Configuration dictionary is required for load_and_process_data.")

    logger.info("Loading and processing data...")
    df = None # Initialize df

    # --- Load initial data ---
    if isinstance(data_input, str): # Input is a file path
        data_path = data_input
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"Specified data file not found: {data_path}")
        logger.info(f"Loading data from specified path: {data_path}")
        try:
            # Use load_file directly (caching handled within load_file)
            df = load_file(data_path)
        except Exception as e:
            raise ValueError(f"Error loading data from {data_path}: {e}") from e
    elif isinstance(data_input, pd.DataFrame): # Input is already a DataFrame
         logger.info("Processing provided DataFrame.")
         df = data_input.copy() # Work on a copy
    elif data_input is None: # Load from config path
        data_dir = config["paths"]["data_dir"]
        file_pattern = config["dataset"]["file_pattern"] # e.g., "aggregated_rmsf_data.csv"
        config_file_path = os.path.join(data_dir, file_pattern)

        if not os.path.exists(config_file_path):
             raise FileNotFoundError(f"Aggregated data file specified in config not found: {config_file_path}")

        logger.info(f"Loading data from config path: {config_file_path}")
        try:
            df = load_file(config_file_path)
        except Exception as e:
            raise ValueError(f"Error loading data from {config_file_path}: {e}") from e
    else: # Invalid input type
        raise TypeError(f"Invalid input type for data_input: {type(data_input)}. Expected str, pd.DataFrame, or None.")

    # --- Validation & Processing ---
    if df is None or df.empty:
         raise ValueError("Loaded or provided DataFrame is empty.")

    # Validate Required Columns (Static list from config)
    required_cols = config["dataset"]["features"].get("required", [])
    if required_cols:
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Loaded data is missing required columns: {missing_cols}")
        logger.debug("All required columns found in loaded data.")
    else:
         logger.warning("No required columns specified in config.")


    # Process Features (Cleaning, Deriving, Windowing)
    processed_df = process_features(df, config) # Pass the loaded/copied df

    # Filter Domains
    filtered_df = filter_domains(processed_df, config)

    logger.info(f"Data loading and processing complete. Final shape: {filtered_df.shape}")
    return filtered_df

# --- Data Splitting ---

def split_data(
    df: pd.DataFrame,
    config: Dict[str, Any]
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Splits data into train, validation, and test sets.

    Supports stratified splitting by domain ID to prevent data leakage.
    Operates on the aggregated data, ensuring all temperature points for a
    domain stay together in the same split.

    Args:
        df: Processed **aggregated** DataFrame with features and target.
        config: Configuration dictionary containing split parameters.

    Returns:
        Tuple of (train_df, val_df, test_df).
    """
    split_config = config["dataset"]["split"]
    test_size = split_config.get("test_size", 0.2)
    val_size = split_config.get("validation_size", 0.15) # Proportion of *original* data for validation
    random_state = config["system"].get("random_state", 42) # Use global random state
    stratify_by_domain = split_config.get("stratify_by_domain", True)

    logger.info(f"Splitting aggregated data: Test={test_size*100:.1f}%, Val={val_size*100:.1f}%, Stratify by domain={stratify_by_domain}")

    if stratify_by_domain:
        if 'domain_id' not in df.columns:
            raise ValueError("Cannot stratify by domain: 'domain_id' column missing.")

        unique_domains = df['domain_id'].unique()
        n_domains = len(unique_domains)
        logger.debug(f"Found {n_domains} unique domains for stratified splitting.")

        if n_domains < 3: # Need at least one domain in each split
             logger.warning("Too few domains for reliable stratified splitting. Falling back to random split.")
             stratify_by_domain = False # Override stratification

    if stratify_by_domain:
        # Calculate actual validation proportion relative to the training set size
        if (1 - test_size) <= 0: raise ValueError("test_size must be less than 1")
        val_ratio_of_train = val_size / (1 - test_size)
        if not (0 < val_ratio_of_train < 1):
             raise ValueError(f"Invalid split sizes: val_size ({val_size}) must be smaller than remaining train size ({1-test_size})")

        # Split unique domains into train+val / test
        domains_train_val, domains_test = train_test_split(
            unique_domains,
            test_size=test_size,
            random_state=random_state
        )
        # Split train+val domains into train / val
        domains_train, domains_val = train_test_split(
            domains_train_val,
            test_size=val_ratio_of_train,
            random_state=random_state # Use same seed for reproducibility at this stage too
        )

        logger.debug(f"Domain split: Train={len(domains_train)}, Val={len(domains_val)}, Test={len(domains_test)}")

        # Create DataFrames based on domain splits. All rows (all temps) for a domain go together.
        train_df = df[df['domain_id'].isin(domains_train)].copy()
        val_df = df[df['domain_id'].isin(domains_val)].copy()
        test_df = df[df['domain_id'].isin(domains_test)].copy()

    else: # Regular sample-based splitting (if stratification disabled or not possible)
        # NOTE: This will randomly split rows, potentially separating different temperature
        # points for the same residue into different sets. Stratification is highly recommended.
        logger.warning("Performing random sample split (not stratified by domain). This may split temperature points for the same residue.")
         # Calculate actual validation proportion relative to the training set size
        if (1 - test_size) <= 0: raise ValueError("test_size must be less than 1")
        val_ratio_of_train = val_size / (1 - test_size)
        if not (0 < val_ratio_of_train < 1):
             raise ValueError(f"Invalid split sizes: val_size ({val_size}) must be smaller than remaining train size ({1-test_size})")

        # First split into (train + val) and test
        train_val_df, test_df = train_test_split(
            df,
            test_size=test_size,
            random_state=random_state
        )
        # Then split (train + val) into train and val
        train_df, val_df = train_test_split(
            train_val_df,
            test_size=val_ratio_of_train,
            random_state=random_state # Use same seed
        )

    logger.info(f"Split complete: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)} rows.")

    # Final check for empty splits
    if train_df.empty or val_df.empty or test_df.empty:
        logger.error("One or more data splits are empty! Check split sizes and data.")

    return train_df, val_df, test_df
### Temperature Analysis Files (Simplified) ###
---------------------------------------------------------
===== FILE: ensembleflex/temperature/comparison.py =====
# /home/s_felix/ensembleflex/ensembleflex/temperature/comparison.py
"""
Temperature Analysis Utilities for EnsembleFlex.

"""

import os
import logging
from typing import Dict, List, Any, Optional, Union

import numpy as np
import pandas as pd
from scipy import stats

logger = logging.getLogger(__name__)

# --- Potentially Reusable (but less central) Functions ---

def calculate_prediction_temperature_correlations(
    combined_pred_df: pd.DataFrame,
    temperatures: List[Union[int, float]],
    model_name: str
) -> Optional[pd.DataFrame]:
    """
    Calculates correlations between a single model's *predictions* made at
    different input temperatures.

    Requires input DataFrame generated by the `compare-temperatures` CLI command.

    Args:
        combined_pred_df: DataFrame with columns like 'domain_id', 'resid',
                          'temperature', 'predicted_rmsf'.
        temperatures: List of numeric temperatures present in the DataFrame.
        model_name: Name of the model (used for logging/context).

    Returns:
        DataFrame correlation matrix or None if calculation fails.
    """
    logger.info(f"Calculating correlations between predictions at different input temperatures for model '{model_name}'.")
    if combined_pred_df.empty or 'predicted_rmsf' not in combined_pred_df.columns or 'temperature' not in combined_pred_df.columns:
        logger.warning("Input DataFrame is empty or missing required columns ('predicted_rmsf', 'temperature').")
        return None

    # Pivot the table to get predictions per temp as columns
    try:
        pivot_df = combined_pred_df.pivot_table(
            index=['domain_id', 'resid'],
            columns='temperature',
            values='predicted_rmsf'
        )
    except Exception as e:
        logger.error(f"Failed to pivot DataFrame for correlation calculation: {e}")
        return None

    # Ensure columns match the requested temperatures and are numeric
    valid_temp_cols = sorted([t for t in temperatures if t in pivot_df.columns])

    if len(valid_temp_cols) < 2:
        logger.warning("Need at least two valid temperature columns in pivoted data to calculate correlations.")
        return None

    logger.debug(f"Calculating correlations for temperature columns: {valid_temp_cols}")

    try:
        # Calculate correlation matrix, handling potential NaNs
        # Use pairwise correlation to handle missing values gracefully
        corr_matrix = pivot_df[valid_temp_cols].corr(method='pearson', min_periods=10) # require some overlap
    except Exception as e:
        logger.error(f"Failed to calculate correlation matrix: {e}")
        return None

    # Format as DataFrame with string indices/columns for consistency
    corr_df = pd.DataFrame(corr_matrix, index=[str(t) for t in valid_temp_cols], columns=[str(t) for t in valid_temp_cols])
    # Reindex to match the full list of input temperatures if needed (will introduce NaNs)
    # corr_df = corr_df.reindex(index=[str(t) for t in temperatures], columns=[str(t) for t in temperatures])

    return corr_df


logger.info("ensembleflex temperature.comparison module loaded (simplified version).")
### Utility Files ###
---------------------------------------------------------
===== FILE: ensembleflex/utils/helpers.py =====
"""
Helper functions for the FlexSeq ML pipeline.

This module provides utility functions used throughout the pipeline.
"""

import os
import logging
import re
from typing import Dict, List, Any, Tuple, Optional, Union, Callable, Iterable, TypeVar
from functools import wraps
from time import time

import numpy as np
import pandas as pd
from tqdm import tqdm

logger = logging.getLogger(__name__)

T = TypeVar('T')

def timer(func):
    """
    Decorator for measuring function execution time.
    
    Args:
        func: Function to wrap
        
    Returns:
        Wrapped function
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time()
        result = func(*args, **kwargs)
        end_time = time()
        logger.info(f"Function '{func.__name__}' executed in {end_time - start_time:.2f} seconds")
        return result
    return wrapper

def ensure_dir(directory: str) -> str:
    """
    Ensure a directory exists, creating it if necessary.
    
    Args:
        directory: Directory path
        
    Returns:
        Directory path
    """
    os.makedirs(directory, exist_ok=True)
    return directory

def estimate_memory_usage(df: pd.DataFrame) -> Tuple[float, str]:
    """
    Estimate memory usage of a DataFrame.
    
    Args:
        df: Input DataFrame
        
    Returns:
        Tuple of (size, unit) where size is a number and unit is a string
    """
    memory_bytes = df.memory_usage(deep=True).sum()
    
    if memory_bytes < 1024:
        return memory_bytes, "bytes"
    elif memory_bytes < 1024**2:
        return memory_bytes / 1024, "KB"
    elif memory_bytes < 1024**3:
        return memory_bytes / (1024**2), "MB"
    else:
        return memory_bytes / (1024**3), "GB"

def chunk_dataframe(df: pd.DataFrame, chunk_size: int) -> List[pd.DataFrame]:
    """
    Split a DataFrame into chunks of specified size.
    
    Args:
        df: Input DataFrame
        chunk_size: Number of rows per chunk
        
    Returns:
        List of DataFrame chunks
    """
    return [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]

def safe_open(file_path: str, mode: str = 'r'):
    """
    Safely open a file with proper directory creation.
    
    Args:
        file_path: Path to file
        mode: File opening mode
        
    Returns:
        File object
    """
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
    return open(file_path, mode)

def truncate_filename(filename: str, max_length: int = 255) -> str:
    """
    Truncate a filename to ensure it doesn't exceed maximum path length.
    
    Args:
        filename: Original filename
        max_length: Maximum allowed length
        
    Returns:
        Truncated filename
    """
    if len(filename) <= max_length:
        return filename
    
    name, ext = os.path.splitext(filename)
    return name[:max_length - len(ext)] + ext

def safe_parse_float(value: Any) -> Optional[float]:
    """
    Safely parse a value to float, returning None if not possible.
    
    Args:
        value: Value to convert
        
    Returns:
        Float value or None if conversion fails
    """
    if pd.isna(value):
        return None
    
    try:
        return float(value)
    except (ValueError, TypeError):
        return None

def get_amino_acid_properties() -> Dict[str, Dict[str, Any]]:
    """
    Get dictionary of amino acid properties.
    
    Returns:
        Dictionary mapping amino acid codes to property dictionaries
    """
    properties = {
        # Hydrophobic residues
        'ALA': {'hydropathy': 1.8, 'volume': 88.6, 'charge': 0, 'group': 'hydrophobic'},
        'VAL': {'hydropathy': 4.2, 'volume': 140.0, 'charge': 0, 'group': 'hydrophobic'},
        'LEU': {'hydropathy': 3.8, 'volume': 166.7, 'charge': 0, 'group': 'hydrophobic'},
        'ILE': {'hydropathy': 4.5, 'volume': 166.7, 'charge': 0, 'group': 'hydrophobic'},
        'MET': {'hydropathy': 1.9, 'volume': 162.9, 'charge': 0, 'group': 'hydrophobic'},
        'PHE': {'hydropathy': 2.8, 'volume': 189.9, 'charge': 0, 'group': 'hydrophobic'},
        'TRP': {'hydropathy': -0.9, 'volume': 227.8, 'charge': 0, 'group': 'hydrophobic'},
        'PRO': {'hydropathy': -1.6, 'volume': 112.7, 'charge': 0, 'group': 'special'},
        'GLY': {'hydropathy': -0.4, 'volume': 60.1, 'charge': 0, 'group': 'special'},
        
        # Polar residues
        'SER': {'hydropathy': -0.8, 'volume': 89.0, 'charge': 0, 'group': 'polar'},
        'THR': {'hydropathy': -0.7, 'volume': 116.1, 'charge': 0, 'group': 'polar'},
        'CYS': {'hydropathy': 2.5, 'volume': 108.5, 'charge': 0, 'group': 'polar'},
        'TYR': {'hydropathy': -1.3, 'volume': 193.6, 'charge': 0, 'group': 'polar'},
        'ASN': {'hydropathy': -3.5, 'volume': 111.1, 'charge': 0, 'group': 'polar'},
        'GLN': {'hydropathy': -3.5, 'volume': 143.8, 'charge': 0, 'group': 'polar'},
        
        # Charged residues
        'LYS': {'hydropathy': -3.9, 'volume': 168.6, 'charge': 1, 'group': 'positive'},
        'ARG': {'hydropathy': -4.5, 'volume': 173.4, 'charge': 1, 'group': 'positive'},
        'HIS': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 0.5, 'group': 'positive'},
        'ASP': {'hydropathy': -3.5, 'volume': 111.1, 'charge': -1, 'group': 'negative'},
        'GLU': {'hydropathy': -3.5, 'volume': 138.4, 'charge': -1, 'group': 'negative'},
        
        # Non-standard
        'HSE': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 0.5, 'group': 'positive'},
        'HSD': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 0.5, 'group': 'positive'},
        'HSP': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 1, 'group': 'positive'},
        'UNK': {'hydropathy': 0.0, 'volume': 0.0, 'charge': 0, 'group': 'unknown'}
    }
    
    return properties

def is_glycine_or_proline(residue: str) -> bool:
    """
    Check if residue is Glycine or Proline, which significantly affect flexibility.
    
    Args:
        residue: Residue name
        
    Returns:
        True if residue is GLY or PRO
    """
    return residue in ["GLY", "PRO"]

def analyze_hydrogen_bonds(
    secondary_structure: str, 
    residue_name: str
) -> float:
    """
    Analyze potential hydrogen bonding based on secondary structure and residue type.
    Returns a relative measure of hydrogen bond stabilization (higher means more stable).
    
    Args:
        secondary_structure: DSSP code
        residue_name: Residue name
        
    Returns:
        Relative stability score (0-1)
    """
    # Secondary structure types have different H-bond patterns
    ss_stability = {
        'H': 1.0,  # Alpha helix - most stable H-bond pattern
        'G': 0.8,  # 3-10 helix
        'I': 0.9,  # Pi helix
        'E': 0.9,  # Beta sheet - very stable
        'B': 0.7,  # Beta bridge
        'T': 0.4,  # Turn
        'S': 0.3,  # Bend
        'C': 0.1   # Coil - least stable
    }
    
    # Some residues have different H-bond propensities
    residue_factors = {
        'SER': 1.2,  # Can form side-chain H-bonds
        'THR': 1.2,  # Can form side-chain H-bonds
        'ASN': 1.2,  # Can form side-chain H-bonds
        'GLN': 1.2,  # Can form side-chain H-bonds
        'TYR': 1.1,  # Can form side-chain H-bonds
        'PRO': 0.7,  # Disrupts H-bond patterns
        'GLY': 0.9   # More flexible backbone
    }
    
    # Get base stability from secondary structure
    stability = ss_stability.get(secondary_structure, 0.1)
    
    # Apply residue-specific factor
    factor = residue_factors.get(residue_name, 1.0)
    
    return min(1.0, stability * factor)

def calculate_sequence_complexity(sequence: List[str], window_size: int = 5) -> List[float]:
    """
    Calculate sequence complexity using Shannon entropy in a sliding window.
    Higher values indicate more diverse/complex sequence regions.
    
    Args:
        sequence: List of amino acid types
        window_size: Size of sliding window
        
    Returns:
        List of complexity values for each position
    """
    # Convert to single-letter amino acid codes if needed
    if all(len(aa) == 3 for aa in sequence if aa):
        three_to_one = {
            'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',
            'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',
            'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',
            'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V',
            'HSD': 'H', 'HSE': 'H', 'HSP': 'H', 'UNK': 'X'
        }
        sequence = [three_to_one.get(aa, 'X') for aa in sequence]
    
    # Compute complexity for each position
    complexity = []
    half_window = window_size // 2
    padded_seq = ['X'] * half_window + list(sequence) + ['X'] * half_window
    
    for i in range(half_window, len(padded_seq) - half_window):
        window = padded_seq[i - half_window:i + half_window + 1]
        
        # Calculate Shannon entropy
        aa_counts = {}
        for aa in window:
            if aa in aa_counts:
                aa_counts[aa] += 1
            else:
                aa_counts[aa] = 1
        
        entropy = 0
        for count in aa_counts.values():
            p = count / window_size
            entropy -= p * np.log2(p)
        
        # Normalize by maximum possible entropy (all different amino acids)
        max_entropy = np.log2(min(20, window_size))
        if max_entropy > 0:
            normalized_entropy = entropy / max_entropy
        else:
            normalized_entropy = 0
        
        complexity.append(normalized_entropy)
    
    return complexity

def progress_bar(
    iterable: Iterable[T],
    desc: str = None,
    total: Optional[int] = None,
    disable: bool = False,
    leave: bool = True,
    **kwargs
) -> Iterable[T]:
    """
    Create a progress bar for an iterable.
    
    Args:
        iterable: Iterable to track progress of
        desc: Description for the progress bar
        total: Total number of items (inferred if not provided)
        disable: Whether to disable the progress bar
        leave: Whether to leave the progress bar after completion
        **kwargs: Additional arguments to pass to tqdm
        
    Returns:
        Wrapped iterable with progress tracking
    """
    # Disable progress bar if verbose logging is not enabled
    log_level = logging.getLogger().getEffectiveLevel()
    if log_level > logging.INFO:
        disable = True
        
    return tqdm(
        iterable,
        desc=desc,
        total=total,
        disable=disable,
        leave=leave,
        ncols=100,  # Fixed width
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
        **kwargs
    )

class ProgressCallback:
    """
    Callback class to track progress of operations that don't use iterables.
    """
    
    def __init__(
        self, 
        total: int, 
        desc: str = None,
        disable: bool = False,
        leave: bool = True,
        **kwargs
    ):
        """
        Initialize progress callback.
        
        Args:
            total: Total number of steps
            desc: Description for the progress bar
            disable: Whether to disable the progress bar
            leave: Whether to leave the progress bar after completion
            **kwargs: Additional arguments to pass to tqdm
        """
        # Disable progress bar if verbose logging is not enabled
        log_level = logging.getLogger().getEffectiveLevel()
        if log_level > logging.INFO:
            disable = True
            
        self.pbar = tqdm(
            total=total,
            desc=desc,
            disable=disable,
            leave=leave,
            ncols=100,  # Fixed width
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
            **kwargs
        )
    
    def update(self, n: int = 1):
        """Update the progress bar by n steps."""
        self.pbar.update(n)
    
    def set_description(self, desc: str):
        """Set the description of the progress bar."""
        self.pbar.set_description(desc)
    
    def set_postfix(self, **kwargs):
        """Set the postfix of the progress bar."""
        self.pbar.set_postfix(**kwargs)
    
    def close(self):
        """Close the progress bar."""
        self.pbar.close()
    
    def __enter__(self):
        """Context manager entry."""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.close()

def format_time(seconds: float) -> str:
    """
    Format time in seconds to a human-readable string.
    
    Args:
        seconds: Time in seconds
        
    Returns:
        Formatted time string
    """
    if seconds < 60:
        return f"{seconds:.1f} seconds"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f} minutes"
    else:
        hours = seconds / 3600
        return f"{hours:.1f} hours"

def get_temperature_color(temperature: Union[int, str]) -> str:
    """
    Get a color code for a temperature value.
    Colors range from blue (cold) to red (hot).
    
    Args:
        temperature: Temperature value
        
    Returns:
        Hex color code
    """
    # Handle special case for "average"
    if temperature == "average" or not isinstance(temperature, (int, float)):
        return "#7F7F7F"  # Gray
    
    # Temperature ranges for FlexSeq
    min_temp = 320
    max_temp = 450
    
    # Clamp temperature to range
    temp = max(min_temp, min(temperature, max_temp))
    
    # Normalize to 0-1 range
    normalized = (temp - min_temp) / (max_temp - min_temp)
    
    # Generate color (blue to red)
    r = int(255 * normalized)
    b = int(255 * (1 - normalized))
    g = int(100 * (1 - abs(2 * normalized - 1)))
    
    return f"#{r:02x}{g:02x}{b:02x}"

def make_model_color_map(model_names: List[str]) -> Dict[str, str]:
    """
    Create a consistent color mapping for models.
    
    Args:
        model_names: List of model names
        
    Returns:
        Dictionary mapping model names to color codes
    """
    # Standard colors for models
    standard_colors = [
        "#1f77b4",  # Blue
        "#ff7f0e",  # Orange
        "#2ca02c",  # Green
        "#d62728",  # Red
        "#9467bd",  # Purple
        "#8c564b",  # Brown
        "#e377c2",  # Pink
        "#7f7f7f",  # Gray
        "#bcbd22",  # Olive
        "#17becf"   # Cyan
    ]
    
    # Create mapping
    color_map = {}
    for i, model in enumerate(model_names):
        color_map[model] = standard_colors[i % len(standard_colors)]
    
    return color_map
===== FILE: ensembleflex/utils/metrics.py =====
# ensembleflex/utils/metrics.py

"""
Evaluation metrics for the ensembleflex ML pipeline.

This module provides functions for evaluating model performance on aggregated data
and potentially performing cross-validation. Temperature-specific logic related
to comparing separate runs has been removed.
"""

import logging
from typing import Dict, List, Any, Union, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    explained_variance_score,
    max_error,
    median_absolute_error
)
from sklearn.model_selection import KFold, cross_val_score # Keep cross_val_score for Q2 placeholder
from scipy.stats import pearsonr, spearmanr

# Assuming these are available from elsewhere in the project if needed by CV
# from ensembleflex.data.processor import prepare_data_for_model
# from ensembleflex.utils.helpers import progress_bar

logger = logging.getLogger(__name__)

# Default set of metrics if not specified in config
DEFAULT_METRICS = {
    "rmse": True,
    "mae": True,
    "r2": True,
    "pearson_correlation": True,
    "spearman_correlation": False,
    "explained_variance": False,
    "max_error": False,
    "median_absolute_error": False,
    "adjusted_r2": False,
    "root_mean_square_absolute_error": False, # Note: Interpreted as RMSE
    "q2": False # Placeholder for cross-validated R2
}

def evaluate_predictions(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    config: Dict[str, Any],
    X: Optional[np.ndarray] = None,
    n_features: Optional[int] = None
) -> Dict[str, float]:
    """
    Evaluate model predictions using multiple metrics specified in the configuration.

    Handles potential calculation errors gracefully by logging warnings and
    returning NaN for failed metrics.

    Args:
        y_true: Array of true target values. Must be 1D.
        y_pred: Array of predicted values. Must be 1D and same length as y_true.
        config: Configuration dictionary, expected to have config['evaluation']['metrics'].
        X: Optional feature matrix (NumPy array) used for metrics like Q2 (currently placeholder).
        n_features: Optional number of features used by the model, required for adjusted R2.

    Returns:
        Dictionary mapping metric names (str) to their calculated values (float).
        Metrics that fail calculation will have a value of np.nan.
    """
    results = {}
    # Safely get metrics config, use defaults if missing
    metrics_config = config.get("evaluation", {}).get("metrics", {})
    if not metrics_config:
        logger.warning("No metrics specified in config['evaluation']['metrics']. Using defaults.")
        metrics_config = DEFAULT_METRICS

    # --- Input Validation ---
    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):
        logger.error("Inputs y_true and y_pred must be NumPy arrays.")
        return {metric: np.nan for metric, enabled in metrics_config.items() if enabled}
    if y_true.ndim != 1 or y_pred.ndim != 1:
        logger.error(f"Inputs y_true (shape {y_true.shape}) and y_pred (shape {y_pred.shape}) must be 1D arrays.")
        return {metric: np.nan for metric, enabled in metrics_config.items() if enabled}
    if len(y_true) != len(y_pred):
        logger.error(f"Inputs y_true ({len(y_true)}) and y_pred ({len(y_pred)}) must have the same length.")
        return {metric: np.nan for metric, enabled in metrics_config.items() if enabled}
    if len(y_true) == 0:
        logger.warning("Input arrays are empty. Returning NaN for all metrics.")
        return {metric: np.nan for metric, enabled in metrics_config.items() if enabled}

    # --- Calculate Metrics ---

    # Helper function to calculate metric safely
    def _calculate_metric(metric_name: str, func, *args, **kwargs):
        if metrics_config.get(metric_name, False):
            try:
                value = func(*args, **kwargs)
                # Check for tuple return (like correlations)
                if isinstance(value, tuple):
                    results[metric_name] = value[0] if not np.isnan(value[0]) else 0.0 # Store corr, handle NaN
                    # Optionally store p-value if needed: results[f"{metric_name}_p_value"] = value[1]
                else:
                    results[metric_name] = float(value) if not np.isnan(value) else np.nan
            except ValueError as ve: # Catches issues like constant input for correlation/r2
                 logger.warning(f"{metric_name.upper()} calculation failed: {ve}. Assigning NaN.")
                 results[metric_name] = np.nan
            except Exception as e:
                 logger.warning(f"{metric_name.upper()} calculation failed unexpectedly: {e}. Assigning NaN.")
                 results[metric_name] = np.nan

    _calculate_metric("rmse", lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)), y_true, y_pred)
    _calculate_metric("mae", mean_absolute_error, y_true, y_pred)
    _calculate_metric("r2", r2_score, y_true, y_pred)
    _calculate_metric("pearson_correlation", pearsonr, y_true, y_pred)
    _calculate_metric("spearman_correlation", spearmanr, y_true, y_pred)
    _calculate_metric("explained_variance", explained_variance_score, y_true, y_pred)
    _calculate_metric("max_error", max_error, y_true, y_pred)
    _calculate_metric("median_absolute_error", median_absolute_error, y_true, y_pred)

    # Adjusted R2 (requires n_features)
    if metrics_config.get("adjusted_r2", False):
        if n_features is not None and n_features > 0:
            n = len(y_true)
            r2 = results.get("r2") # Use already calculated R2 if available
            if r2 is None: # Calculate if needed
                 try: r2 = r2_score(y_true, y_pred); results["r2"] = r2 # Store it too
                 except ValueError: r2 = np.nan

            if not np.isnan(r2) and (n - n_features - 1) > 0: # Check denominator
                adj_r2 = 1.0 - (1.0 - r2) * (n - 1) / (n - n_features - 1)
                results["adjusted_r2"] = adj_r2
            else:
                results["adjusted_r2"] = np.nan
                if (n - n_features - 1) <= 0: logger.warning("Adjusted R2 calc failed: n_samples <= n_features + 1.")
                elif np.isnan(r2): logger.warning("Adjusted R2 calc failed: R2 was NaN.")
        else:
            logger.warning("Adjusted R2 requested but n_features not provided or invalid.")
            results["adjusted_r2"] = np.nan

    # Root Mean Square Absolute Error (Interpreted as RMSE)
    if metrics_config.get("root_mean_square_absolute_error", False):
        # This is mathematically equivalent to RMSE
        results["root_mean_square_absolute_error"] = results.get("rmse", np.nan)
        if np.isnan(results["root_mean_square_absolute_error"]): # Recalculate if RMSE failed/disabled
             _calculate_metric("root_mean_square_absolute_error", lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)), y_true, y_pred)


    # Q2 Placeholder
    if metrics_config.get("q2", False):
        logger.warning("Q2 metric calculation is not implemented (requires CV setup). Assigning NaN.")
        results["q2"] = np.nan
        # if X is not None:
        #     # Placeholder for actual Q2 calculation using cross-validation
        #     # cv_r2 = cross_val_score(model_placeholder, X, y_true, cv=5, scoring='r2').mean()
        #     # results["q2"] = cv_r2
        #     pass
        # else:
        #     results["q2"] = np.nan


    # Ensure all requested metrics exist in the results dictionary
    for metric, enabled in metrics_config.items():
         if enabled and metric not in results:
              logger.debug(f"Metric '{metric}' was enabled but not calculated (or failed). Setting to NaN.")
              results[metric] = np.nan

    return results


# Note: cross_validate_model needs careful review if used with the new pipeline structure.
# It might be simpler to perform CV within the pipeline's train method if needed.
# The version below is adapted but might require further testing in the ensembleflex context.
def cross_validate_model(
    model_class: Any,
    model_params: Dict[str, Any],
    data: pd.DataFrame, # Expects aggregated data now
    config: Dict[str, Any],
    n_folds: int = 5,
    return_predictions: bool = False
) -> Union[Dict[str, float], Tuple[Dict[str, float], pd.DataFrame]]:
    """
    Perform cross-validation for a model using aggregated data.

    Handles stratified splitting by domain if configured and possible.

    Args:
        model_class: Model class to instantiate (should inherit from BaseModel).
        model_params: Parameters for model initialization (specific to the model).
        data: DataFrame with aggregated features and target.
        config: Configuration dictionary.
        n_folds: Number of cross-validation folds.
        return_predictions: Whether to return Out-Of-Fold (OOF) predictions.

    Returns:
        - Dictionary with cross-validation results (mean/std of metrics).
        - If return_predictions is True, also returns a DataFrame containing
          OOF predictions aligned with the original data index.
    """
    # Import here to avoid circular dependency if metrics is imported by processor
    from ensembleflex.data.processor import prepare_data_for_model
    from ensembleflex.utils.helpers import progress_bar

    logger.info(f"Starting {n_folds}-fold cross-validation...")

    # Use a copy to avoid modifying original data
    cv_data = data.copy()

    # Get metrics to calculate from config
    metrics_config = config.get("evaluation", {}).get("metrics", {})
    metrics_to_calculate = {m: [] for m, enabled in metrics_config.items() if enabled}
    if not metrics_to_calculate: # Add defaults if none specified
         metrics_to_calculate = {'rmse': [], 'mae': [], 'r2': [], 'pearson_correlation': []}
         logger.warning(f"No metrics enabled in config for CV, using defaults: {list(metrics_to_calculate.keys())}")


    # Add storage for OOF predictions if requested
    oof_preds_list = []
    oof_indices_list = []

    # Create cross-validation folds
    stratify_by_domain = config["dataset"]["split"].get("stratify_by_domain", True)
    random_state = config["system"]["random_state"]
    folds = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)
    fold_iterator_desc = f"CV ({n_folds} folds)"
    fold_indices_or_domains: Union[List[Tuple[np.ndarray, np.ndarray]], List[Tuple[List[str], List[str]]]] = []

    # Prepare splits (either domain IDs or row indices)
    if stratify_by_domain and 'domain_id' in cv_data.columns:
        unique_domains = cv_data["domain_id"].unique()
        if len(unique_domains) < n_folds:
            logger.warning(f"Number of unique domains ({len(unique_domains)}) < n_folds ({n_folds}). Stratification might be imperfect.")
        fold_indices_or_domains = [(unique_domains[train_idx], unique_domains[test_idx])
                                   for train_idx, test_idx in folds.split(unique_domains)]
        fold_iterator_desc += " stratified by domain"
    else:
        if stratify_by_domain: # Log if stratification was requested but not possible
            logger.warning("Stratify by domain requested but 'domain_id' column missing. Using regular KFold on rows.")
        fold_indices_or_domains = list(folds.split(cv_data)) # List of (train_indices, test_indices)

    # Perform CV
    fold_iterator = progress_bar(range(n_folds), desc=fold_iterator_desc)
    for i in fold_iterator:
        train_idx_or_domains, test_idx_or_domains = fold_indices_or_domains[i]

        # Select train/test data based on split type
        if stratify_by_domain and 'domain_id' in cv_data.columns:
            train_data = cv_data[cv_data["domain_id"].isin(train_idx_or_domains)]
            test_data = cv_data[cv_data["domain_id"].isin(test_idx_or_domains)]
        else: # Index-based split
            train_data = cv_data.iloc[train_idx_or_domains]
            test_data = cv_data.iloc[test_idx_or_domains]

        fold_test_indices = test_data.index # Store original indices for OOF

        if train_data.empty or test_data.empty:
             logger.warning(f"Skipping fold {i+1} due to empty train or test split.")
             continue

        try:
            # Prepare data for model (important to use fold-specific data)
            X_train, y_train, feature_names = prepare_data_for_model(train_data, config)
            X_test, y_test, _ = prepare_data_for_model(test_data, config)

            if X_train.size == 0 or X_test.size == 0:
                 logger.warning(f"Skipping fold {i+1} due to empty feature matrix after preparation.")
                 continue

            # Create and train model instance for this fold
            # Pass only model-specific params, not the whole config section
            fold_model_params = get_model_config(config, model_class.__name__.lower()) # Assumes class name matches config key
            model_instance = model_class(**fold_model_params)

            # Fit model
            if 'feature_names' in inspect.signature(model_instance.fit).parameters:
                 model_instance.fit(X_train, y_train, feature_names=feature_names)
            else:
                 model_instance.fit(X_train, y_train)

            # Generate predictions for the test fold
            y_pred = model_instance.predict(X_test)

            # Evaluate predictions for this fold
            n_features_fold = X_train.shape[1]
            # Pass the main config for evaluation metric settings
            fold_metrics = evaluate_predictions(y_test, y_pred, config, X=X_test, n_features=n_features_fold)

            # Store results
            for metric, value in fold_metrics.items():
                if metric in metrics_to_calculate:
                     metrics_to_calculate[metric].append(value)

            # Store OOF predictions if requested
            if return_predictions:
                oof_preds_list.append(pd.Series(y_pred, index=fold_test_indices))
                # oof_true_values.append(pd.Series(y_test, index=fold_test_indices)) # Less common to return true

        except Exception as e:
             logger.error(f"Error occurred in CV fold {i+1}: {e}", exc_info=True)
             # Add NaN for this fold's metrics to maintain alignment?
             for metric in metrics_to_calculate:
                  metrics_to_calculate[metric].append(np.nan)


    # Calculate final statistics (mean and stddev over folds)
    results_summary = {}
    for metric, values in metrics_to_calculate.items():
        valid_values = [v for v in values if pd.notna(v)] # Filter out NaNs
        if valid_values:
            results_summary[f"mean_{metric}"] = np.mean(valid_values)
            results_summary[f"std_{metric}"] = np.std(valid_values)
        else: # Handle case where all folds failed for a metric
             results_summary[f"mean_{metric}"] = np.nan
             results_summary[f"std_{metric}"] = np.nan
        # Add raw fold values if desired
        # results_summary[f"{metric}_folds"] = values


    if return_predictions:
        if oof_preds_list:
            # Concatenate predictions, aligning by original index
            oof_predictions_df = pd.concat(oof_preds_list).sort_index()
            # Add original target values for comparison if needed
            oof_predictions_df = pd.DataFrame({'oof_prediction': oof_predictions_df})
            oof_predictions_df = oof_predictions_df.join(data[[config['dataset']['target']]]) # Join original target
        else:
             oof_predictions_df = pd.DataFrame() # Return empty df if no preds generated
        return results_summary, oof_predictions_df
    else:
        return results_summary


# --- Other Utility Functions (Review for ensembleflex context) ---

def calculate_residue_metrics(
    df: pd.DataFrame,
    target_col: str,
    prediction_cols: List[str],
    include_uncertainty: bool = False,
    uncertainty_cols: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    Calculate residue-level metrics from a DataFrame containing predictions.

    Assumes input df has 'domain_id', 'resid', 'resname', target_col, and prediction_cols.

    Args:
        df: DataFrame with true values and predictions.
        target_col: Column with true target values (e.g., 'rmsf').
        prediction_cols: List of column names with model predictions (e.g., ['nn_predicted']).
        include_uncertainty: Whether to include uncertainty metrics.
        uncertainty_cols: List of column names with prediction uncertainties (e.g., ['nn_uncertainty']).

    Returns:
        DataFrame with residue-level metrics (one row per residue).
    """
    if not all(c in df.columns for c in ['domain_id', 'resid', target_col]) or not prediction_cols:
        logger.error("Input DataFrame missing essential columns for calculate_residue_metrics.")
        return pd.DataFrame()

    results = []
    required_cols = ['domain_id', 'resid', target_col] + prediction_cols
    if 'resname' in df.columns: required_cols.append('resname')
    if include_uncertainty and uncertainty_cols: required_cols.extend(uncertainty_cols)
    # Add other structural features if needed and available
    for feature in ["secondary_structure_encoded", "core_exterior_encoded", "temperature"]:
        if feature in df.columns: required_cols.append(feature)

    # Group by unique residue identifier
    for (domain_id, resid), residue_group in df.groupby(["domain_id", "resid"], observed=False):
        if residue_group.empty: continue
        # Take the first row's data for residue-specific info (assuming it's constant)
        residue_data = residue_group.iloc[0]
        residue_metrics = {
            "domain_id": domain_id,
            "resid": resid,
        }
        # Copy static features
        for col in required_cols:
             if col not in [target_col] + prediction_cols + (uncertainty_cols or []):
                  residue_metrics[col] = residue_data.get(col, np.nan)

        # Use the mean of target/prediction/uncertainty if multiple temps exist per residue
        true_value = residue_group[target_col].mean() # Average actual value across temps
        residue_metrics["actual_mean"] = true_value

        for pred_col in prediction_cols:
            if pred_col not in residue_group.columns: continue
            pred_value = residue_group[pred_col].mean() # Average predicted value
            residue_metrics[pred_col+"_mean"] = pred_value

            # Calculate error based on average values
            error = pred_value - true_value
            abs_error = abs(error)
            model_name = pred_col.split("_predicted")[0]
            residue_metrics[f"{model_name}_error_mean"] = error
            residue_metrics[f"{model_name}_abs_error_mean"] = abs_error

            # Handle uncertainty
            if include_uncertainty and uncertainty_cols:
                unc_col = next((col for col in uncertainty_cols if model_name in col), None)
                if unc_col and unc_col in residue_group.columns:
                    unc_value = residue_group[unc_col].mean() # Average uncertainty
                    residue_metrics[f"{model_name}_uncertainty_mean"] = unc_value
                    # Calculate normalized error (using average error and average uncertainty)
                    if unc_value > 1e-9: # Avoid division by zero/small numbers
                        normalized_error = abs_error / unc_value
                        residue_metrics[f"{model_name}_normalized_error_mean"] = normalized_error
                    else:
                         residue_metrics[f"{model_name}_normalized_error_mean"] = np.nan

        results.append(residue_metrics)

    return pd.DataFrame(results)

# Note: This function is likely obsolete for ensembleflex as it assumes rmsf_{temp} columns
def calculate_temperature_scaling_factors(
    df: pd.DataFrame,
    temperatures: List[Union[int, float]] # Should be numeric temps
) -> Dict[str, float]:
    """
    [OBSOLETE - Requires Refactor] Calculate scaling factors between RMSF values at different temperatures.
    This requires columns named like 'rmsf_320', 'rmsf_450' which are not present
    in the standard ensembleflex aggregated data. Could be adapted to work on the
    output of the `compare-temperatures` CLI command's CSV.

    Args:
        df: DataFrame with RMSF values (e.g., 'rmsf_320', 'rmsf_450').
        temperatures: List of numeric temperatures corresponding to columns.

    Returns:
        Dictionary mapping temperature pairs (str) to scaling factors (float).
    """
    logger.warning("calculate_temperature_scaling_factors is likely obsolete or needs refactoring for ensembleflex.")
    temp_list = sorted([t for t in temperatures if isinstance(t, (int, float))])
    if len(temp_list) < 2: return {}

    scaling_factors = {}
    for i, temp1 in enumerate(temp_list):
        col1 = f"rmsf_{temp1}" # Assumes old column naming convention
        if col1 not in df.columns: continue
        for temp2 in temp_list[i+1:]:
            col2 = f"rmsf_{temp2}"
            if col2 not in df.columns: continue
            try:
                # Calculate average ratio, avoiding division by zero/NaNs
                valid_mask = (df[col1] > 1e-6) & df[col2].notna()
                if valid_mask.sum() > 0:
                    ratios = df.loc[valid_mask, col2] / df.loc[valid_mask, col1]
                    scaling_factors[f"{temp1}_to_{temp2}"] = ratios.mean()
                else:
                     scaling_factors[f"{temp1}_to_{temp2}"] = np.nan
            except Exception as e:
                 logger.error(f"Error calculating scaling factor {temp1}->{temp2}: {e}")
                 scaling_factors[f"{temp1}_to_{temp2}"] = np.nan

    return scaling_factors


def calculate_uncertainty_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_std: np.ndarray
) -> Dict[str, float]:
    """
    Calculate metrics related to uncertainty quantification.

    Requires true values, mean predictions, and predicted standard deviations.

    Args:
        y_true: Array of true target values.
        y_pred: Array of mean predicted values.
        y_std: Array of predicted standard deviations (uncertainty).

    Returns:
        Dictionary of uncertainty metrics.
    """
    if not all(isinstance(arr, np.ndarray) and arr.ndim == 1 and len(arr) == len(y_true) for arr in [y_pred, y_std]):
         logger.error("Invalid input shapes or types for calculate_uncertainty_metrics.")
         return {}
    if len(y_true) == 0: return {}

    results = {}
    errors = np.abs(y_true - y_pred)
    # Ensure std dev is not zero or negative for calculations
    y_std_safe = np.maximum(y_std, 1e-9) # Add small epsilon to avoid division by zero

    # Calibration metrics: Percentage within N std devs
    results["within_1std"] = np.mean(errors <= y_std_safe)
    results["within_2std"] = np.mean(errors <= 2 * y_std_safe)
    results["within_3std"] = np.mean(errors <= 3 * y_std_safe)

    # Calibration Error (deviation from ideal Gaussian coverage)
    results["calibration_error_1std"] = np.abs(results["within_1std"] - 0.6827)
    results["calibration_error_2std"] = np.abs(results["within_2std"] - 0.9545)
    results["calibration_error_3std"] = np.abs(results["within_3std"] - 0.9973)
    results["avg_calibration_error"] = np.mean([
        results["calibration_error_1std"],
        results["calibration_error_2std"],
        results["calibration_error_3std"]
    ])

    # Negative Log Predictive Density (NLPD) - Lower is better
    # Assumes Gaussian predictive distribution N(y_pred, y_std^2)
    try:
        variance = y_std_safe**2
        nlpd = 0.5 * np.mean(np.log(2 * np.pi * variance) + ((y_true - y_pred)**2) / variance)
        results["nlpd"] = nlpd
    except Exception as e:
        logger.warning(f"NLPD calculation failed: {e}")
        results["nlpd"] = np.nan


    # Uncertainty-Error Correlation (Pearson) - Higher positive correlation is generally better
    try:
        # Need variation in both std and error
        if np.var(y_std_safe) > 1e-9 and np.var(errors) > 1e-9:
            unc_err_corr, _ = pearsonr(y_std_safe, errors)
            results["uncertainty_error_correlation"] = unc_err_corr if not np.isnan(unc_err_corr) else 0.0
        else:
             results["uncertainty_error_correlation"] = np.nan
    except Exception as e:
        logger.warning(f"Uncertainty-error correlation failed: {e}")
        results["uncertainty_error_correlation"] = np.nan

    return results

def calculate_domain_performance(
    df: pd.DataFrame,
    target_col: str,
    prediction_cols: List[str]
) -> pd.DataFrame:
    """
    Calculate performance metrics grouped by domain ID.

    Assumes input df contains 'domain_id', target_col, prediction_cols,
    and potentially other static features like 'resname', 'secondary_structure_encoded'.

    Args:
        df: DataFrame with predictions and actual values (e.g., from all_results.csv).
        target_col: Target column name ('rmsf').
        prediction_cols: List of columns with model predictions (e.g., ['nn_predicted']).

    Returns:
        DataFrame with domain performance metrics.
    """
    if 'domain_id' not in df.columns or target_col not in df.columns or not prediction_cols:
         logger.error("Missing required columns ('domain_id', target, predictions) for calculate_domain_performance.")
         return pd.DataFrame()

    results = []
    logger.info("Calculating domain performance metrics...")

    for domain_id, domain_df in progress_bar(df.groupby("domain_id", observed=False), desc="Domain Perf."):
        if domain_df.empty: continue
        row = {"domain_id": domain_id}

        # Aggregate static features (take first value)
        static_cols = ['resname', 'secondary_structure_encoded', 'core_exterior_encoded', 'temperature'] # Add temp?
        for col in static_cols:
            if col in domain_df.columns:
                 # If feature is constant per domain (like resname, SS at a residue), take first.
                 # If feature varies (like temperature), calculate mean/median.
                 if domain_df[col].nunique() == 1:
                      row[f"{col}_unique"] = domain_df[col].iloc[0]
                 elif pd.api.types.is_numeric_dtype(domain_df[col].dtype):
                      row[f"{col}_mean"] = domain_df[col].mean()
                 else: # Categorical but not unique
                      row[f"{col}_mode"] = domain_df[col].mode()[0] if not domain_df[col].mode().empty else None


        # Add residue/row counts
        row["num_unique_residues"] = domain_df['resid'].nunique() if 'resid' in domain_df.columns else np.nan
        row["num_rows"] = len(domain_df)

        # Calculate performance metrics for each model
        for pred_col in prediction_cols:
            model_name = pred_col.split("_predicted")[0]
            if pred_col not in domain_df.columns: continue

            # Drop NaNs for metric calculation for this model/domain
            valid_rows = domain_df[[target_col, pred_col]].dropna()
            if valid_rows.empty or len(valid_rows) < 2: # Need at least 2 points for R2/Corr
                 row[f"{model_name}_rmse"] = np.nan
                 row[f"{model_name}_mae"] = np.nan
                 row[f"{model_name}_r2"] = np.nan
                 row[f"{model_name}_pearson"] = np.nan
                 continue

            y_true = valid_rows[target_col].values
            y_pred = valid_rows[pred_col].values

            try: row[f"{model_name}_rmse"] = np.sqrt(mean_squared_error(y_true, y_pred))
            except: row[f"{model_name}_rmse"] = np.nan

            try: row[f"{model_name}_mae"] = mean_absolute_error(y_true, y_pred)
            except: row[f"{model_name}_mae"] = np.nan

            try: row[f"{model_name}_r2"] = r2_score(y_true, y_pred) if np.var(y_true) > 1e-9 else np.nan
            except: row[f"{model_name}_r2"] = np.nan

            try:
                 pearson_corr, _ = pearsonr(y_true, y_pred) if np.var(y_true) > 1e-9 and np.var(y_pred) > 1e-9 else (np.nan, np.nan)
                 row[f"{model_name}_pearson"] = pearson_corr if not np.isnan(pearson_corr) else 0.0
            except: row[f"{model_name}_pearson"] = np.nan

        results.append(row)

    logger.info("Finished calculating domain performance metrics.")
    return pd.DataFrame(results)
===== FILE: ensembleflex/utils/visualization.py =====
# /home/s_felix/ensembleflex/ensembleflex/utils/visualization.py

"""
Visualization functions for the ensembleflex ML pipeline.

Generates plots and saves visualization data CSVs based on the single model's
results and the aggregated dataset structure. Focuses on analyzing performance
relative to features like temperature.
"""

import os
import logging
from typing import Dict, List, Tuple, Any, Optional

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
# from scipy import stats # Only needed if doing KDE manually, seaborn handles it

# Updated imports
from ensembleflex.utils.helpers import (
    # get_temperature_color, # Less relevant
    make_model_color_map, # Still useful for comparing models in specific plots if needed
    ensure_dir
)
# Import metrics calculation if needed within plotting (e.g., for binned analysis)
from ensembleflex.utils.metrics import evaluate_predictions

logger = logging.getLogger(__name__)

def save_plot(plt_or_fig, output_path: str, dpi: int = 300) -> None:
    """
    Save a matplotlib plot or figure to disk, ensuring directory exists and closing plot.

    Args:
        plt_or_fig: Matplotlib pyplot instance or Figure object.
        output_path: Path to save the plot.
        dpi: Resolution in dots per inch.
    """
    if output_path is None:
        logger.warning("No output path provided for saving plot. Skipping save.")
        plt.close('all')
        return

    ensure_dir(os.path.dirname(output_path))
    try:
        fig = None
        # Check if it's a Figure object or pyplot module
        if isinstance(plt_or_fig, plt.Figure):
            fig = plt_or_fig
        elif hasattr(plt_or_fig, 'gcf'): # Check if it's likely the pyplot module
             fig = plt_or_fig.gcf()

        if fig is not None and fig.get_axes(): # Check if figure has content
            # Try tight_layout, but handle potential errors gracefully
            try:
                fig.tight_layout()
            except ValueError as tle:
                logger.warning(f"tight_layout failed for {os.path.basename(output_path)}: {tle}. Proceeding without it.")
            fig.savefig(output_path, dpi=dpi, bbox_inches='tight')
            logger.info(f"Plot saved successfully to {output_path}")
        elif fig is not None:
             logger.warning(f"Attempted to save an empty figure to {output_path}. Skipping save.")
        else:
             logger.error("Could not get figure object to save plot.")

    except Exception as e:
        logger.error(f"Failed to save plot to {output_path}: {e}", exc_info=True)
    finally:
        # Always close all figures to prevent memory leaks
        plt.close('all')


# --- Removed Obsolete Functions ---
# plot_temperature_comparison (Metrics vs Temp handled by plot_error_vs_temperature)
# plot_model_metrics_table (Metrics saved directly in evaluate step)
# plot_r2_comparison (Metrics saved directly in evaluate step)
# plot_r2_comparison_scatter (Could be adapted, but less central)

# --- Adapted Visualization Functions ---

# Place this corrected function in ensembleflex/utils/visualization.py

def plot_feature_importance(
    importance_dict: Dict[str, float], # Expects a dictionary
    plot_path: str,
    csv_path: Optional[str] = None,
    top_n: int = 20
) -> None:
    """
    Generate feature importance bar chart and save data from a dictionary.
    Highlights the 'temperature' feature if present.

    Args:
        importance_dict: Dictionary mapping feature names (str) to importance values (float).
        plot_path: Path to save the bar chart plot (.png).
        csv_path: Optional path to save the importance data (.csv).
        top_n: Number of top features to display in the plot.
    """
    if not importance_dict or not isinstance(importance_dict, dict):
        logger.warning("Invalid or empty importance_dict provided. Skipping plot/save.")
        return

    try:
        # Create DataFrame directly from the dictionary
        importance_df = pd.DataFrame(importance_dict.items(), columns=['feature', 'importance'])
        # Handle potential NaN/Inf values before sorting
        importance_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        importance_df.dropna(subset=['importance'], inplace=True)
        if importance_df.empty:
             logger.warning("No valid importance values found after cleaning.")
             return
        importance_df = importance_df.sort_values('importance', ascending=False).reset_index(drop=True)

        # Save to CSV if path provided
        if csv_path:
            ensure_dir(os.path.dirname(csv_path))
            try:
                importance_df.to_csv(csv_path, index=False)
                logger.info(f"Feature importance data saved to {csv_path}")
            except Exception as e:
                logger.error(f"Failed to save feature importance CSV: {e}")

        # Create bar plot
        fig, ax = plt.subplots(figsize=(10, max(6, min(top_n, len(importance_df)) * 0.4)))
        top_features = importance_df.head(top_n)

        # Highlight temperature feature using the correct feature names
        colors = ['#d62728' if feat == 'temperature' else '#1f77b4' for feat in top_features['feature']]

        sns.barplot(x='importance', y='feature', data=top_features, palette=colors, ax=ax)
        ax.set_title(f'Top {min(top_n, len(top_features))} Feature Importances') # Adjust title if fewer than top_n
        ax.set_xlabel('Importance Score')
        ax.set_ylabel('Feature Name') # Y-axis now uses the actual names

        # Add annotation for temperature rank
        if 'temperature' in importance_df['feature'].values:
             try:
                 temp_rank = importance_df[importance_df['feature'] == 'temperature'].index[0] + 1
                 # Add annotation only if not already displayed in top_n
                 if temp_rank > top_n:
                      ax.text(0.05, 0.01, f"'temperature' rank: {temp_rank}", transform=ax.transAxes,
                              fontsize=9, color='red', ha='left', va='bottom')
             except (IndexError, TypeError):
                 logger.warning("Could not determine rank for 'temperature' feature.")


        save_plot(fig, plot_path) # Pass the figure object

    except Exception as e:
        logger.error(f"Failed to create feature importance plot: {e}", exc_info=True)
        plt.close('all')


def plot_scatter_with_density_contours(
    results_df: pd.DataFrame,
    model_name: str,
    target_col: str,
    plot_path: str,
    csv_path: Optional[str] = None,
    sample_size: int = 5000
) -> None:
    """
    Generate scatter plot of Actual vs. Predicted with density contours for the single model.

    Args:
        results_df: DataFrame containing 'all_results.csv' data.
        model_name: Name of the model (used to find prediction column).
        target_col: Name of the actual target column (e.g., 'rmsf').
        plot_path: Path to save the plot (.png).
        csv_path: Optional path to save the sampled data (.csv).
        sample_size: Number of points to sample for plotting.
    """
    pred_col = f"{model_name}_predicted"
    if target_col not in results_df.columns or pred_col not in results_df.columns:
        logger.error(f"Missing required columns ('{target_col}', '{pred_col}') for scatter plot. Skipping.")
        return

    # Select data and drop NaNs in target or prediction
    plot_data = results_df[[target_col, pred_col]].dropna().copy()
    plot_data.rename(columns={target_col: 'actual', pred_col: 'predicted'}, inplace=True)

    if plot_data.empty:
         logger.warning("No valid data points found for scatter plot after dropping NaNs.")
         return

    # Sample data for plotting
    n_samples = min(sample_size, len(plot_data))
    sampled_df = plot_data.sample(n_samples, random_state=42) # Use fixed seed

    # Save sampled data if path provided
    if csv_path:
        ensure_dir(os.path.dirname(csv_path))
        try:
            sampled_df.to_csv(csv_path, index=False)
            logger.info(f"Sampled scatter plot data saved to {csv_path}")
        except Exception as e:
            logger.error(f"Failed to save scatter plot data CSV: {e}")

    # Create the plot
    try:
        fig, ax = plt.subplots(figsize=(8, 8)) # Get figure and axes objects
        # Use seaborn for easier density plotting
        sns.kdeplot(
            x='actual',
            y='predicted',
            data=sampled_df,
            fill=True,
            cmap='Blues',
            alpha=0.6,
            levels=8, # Adjust number of contours
            ax=ax
        )
        # Overlay scatter points
        ax.scatter(
            sampled_df['actual'],
            sampled_df['predicted'],
            alpha=0.15, # Make points less prominent
            s=15,
            color='darkblue',
            edgecolors='none' # Remove edges for dense plots
        )

        # Add diagonal line
        # Calculate limits based on the data range
        min_val = min(sampled_df['actual'].min(), sampled_df['predicted'].min())
        max_val = max(sampled_df['actual'].max(), sampled_df['predicted'].max())
        lim_min = min_val - 0.1 * (max_val - min_val) # Add some buffer
        lim_max = max_val + 0.1 * (max_val - min_val)
        ax.plot([lim_min, lim_max], [lim_min, lim_max], 'r--', linewidth=1.5, label="Ideal (y=x)")

        ax.set_xlabel(f'Actual {target_col.upper()}')
        ax.set_ylabel(f'{model_name} Predicted {target_col.upper()}')
        ax.set_title(f'Actual vs. Predicted {target_col.upper()} ({model_name})')
        ax.set_xlim(lim_min, lim_max)
        ax.set_ylim(lim_min, lim_max)
        ax.legend()
        ax.grid(True, linestyle=':', alpha=0.5)

        save_plot(fig, plot_path) # Pass the figure object

    except Exception as e:
        logger.error(f"Failed to create density scatter plot: {e}", exc_info=True)
        plt.close('all')


def plot_amino_acid_error_analysis(
    results_df: pd.DataFrame,
    model_name: str,
    target_col: str, # Keep for context, though error should be precalculated
    csv_path: str,
    plot_path: str
) -> None:
    """
    Generate amino acid error analysis data and plot for the single model.

    Args:
        results_df: DataFrame containing 'all_results.csv' data.
        model_name: Name of the model.
        target_col: Name of the actual target column (e.g., 'rmsf').
        csv_path: Path to save the error summary CSV.
        plot_path: Path to save the bar plot (.png).
    """
    error_col = f"{model_name}_abs_error"
    if 'resname' not in results_df.columns:
        logger.warning("Missing 'resname' column for amino acid error analysis. Skipping.")
        return
    if error_col not in results_df.columns:
         # Try to calculate it if prediction and target exist
         pred_col = f"{model_name}_predicted"
         if target_col in results_df.columns and pred_col in results_df.columns:
             logger.info(f"Calculating '{error_col}' on the fly for amino acid analysis.")
             results_df[error_col] = (results_df[pred_col] - results_df[target_col]).abs()
         else:
              logger.error(f"Cannot perform amino acid analysis: Missing '{error_col}' or source columns ('{pred_col}', '{target_col}').")
              return

    # Group and aggregate, handling potential NaNs in error column
    aa_errors = results_df.groupby('resname')[error_col].agg(['mean', 'median', 'std', 'count']).reset_index()
    aa_errors.rename(columns={'mean': 'mean_abs_error', 'median': 'median_abs_error', 'std': 'std_abs_error'}, inplace=True)

    if aa_errors.empty:
         logger.warning("No data found after grouping by resname for error analysis.")
         return

    # Save to CSV
    ensure_dir(os.path.dirname(csv_path))
    try:
        aa_errors.to_csv(csv_path, index=False)
        logger.info(f"Amino acid error summary saved to {csv_path}")
    except Exception as e:
        logger.error(f"Failed to save amino acid error CSV: {e}")

    # Create bar plot
    try:
        fig, ax = plt.subplots(figsize=(12, 6))
        sorted_df = aa_errors.sort_values('mean_abs_error')
        sns.barplot(x='resname', y='mean_abs_error', data=sorted_df, ax=ax)
        ax.set_title(f'Mean Absolute Error by Amino Acid ({model_name})')
        ax.set_xlabel('Amino Acid')
        ax.set_ylabel('Mean Absolute Error')
        ax.tick_params(axis='x', rotation=45)

        save_plot(fig, plot_path)

    except Exception as e:
        logger.error(f"Failed to create amino acid error plot: {e}", exc_info=True)
        plt.close('all')


def plot_error_analysis_by_property(
    results_df: pd.DataFrame,
    model_name: str,
    target_col: str, # Keep for context
    output_base_path: str # e.g., ../output/ensembleflex/residue_analysis/model_error_by
) -> None:
    """
    Generate error analysis data and plots grouped by various properties for the single model.

    Args:
        results_df: DataFrame containing 'all_results.csv' data.
        model_name: Name of the model.
        target_col: Name of the actual target column (e.g., 'rmsf').
        output_base_path: Base path for saving files (property name and extension added).
    """
    error_col = f"{model_name}_abs_error"
    if error_col not in results_df.columns:
         pred_col = f"{model_name}_predicted"
         if target_col in results_df.columns and pred_col in results_df.columns:
             logger.info(f"Calculating '{error_col}' on the fly for property analysis.")
             # Ensure calculation happens on a temporary copy if needed downstream
             # Use .loc to potentially avoid SettingWithCopyWarning if results_df is a view
             df_copy_for_calc = results_df.copy()
             df_copy_for_calc[error_col] = (df_copy_for_calc[pred_col] - df_copy_for_calc[target_col]).abs()
             # We'll use this df_copy_for_calc below if error had to be calculated
         else:
              logger.error(f"Cannot perform property analysis: Missing '{error_col}' or source columns ('{pred_col}', '{target_col}').")
              return
    else:
        # If error column already exists, still use a copy for modifications like binning
        df_copy_for_calc = results_df.copy()


    properties_to_analyze = {}
    # Define properties based on columns available in the df_copy_for_calc
    if 'secondary_structure_encoded' in df_copy_for_calc.columns:
        properties_to_analyze['secondary_structure'] = {
            'column': 'secondary_structure_encoded',
            'labels': {0: 'Helix', 1: 'Sheet', 2: 'Loop/Other'}
        }
    if 'core_exterior_encoded' in df_copy_for_calc.columns:
        properties_to_analyze['surface_exposure'] = {
            'column': 'core_exterior_encoded',
            'labels': {0: 'Core', 1: 'Surface'}
        }
    if 'normalized_resid' in df_copy_for_calc.columns:
        properties_to_analyze['sequence_position'] = {
            'column': 'normalized_resid',
            'labels': None, # Will be binned
            'bins': 5,
            'bin_labels': ['N-term', 'N-quarter', 'Middle', 'C-quarter', 'C-term']
        }
    if 'relative_accessibility' in df_copy_for_calc.columns:
         properties_to_analyze['accessibility_bin'] = {
             'column': 'relative_accessibility',
             'labels': None,
             'bins': 5
             # Example: Use default numeric labels for bins
         }

    if not properties_to_analyze:
         logger.warning("No suitable columns found for property error analysis.")
         return

    for prop_name, prop_config in properties_to_analyze.items():
        col = prop_config['column']
        if col not in df_copy_for_calc.columns:
            logger.warning(f"Column '{col}' for property '{prop_name}' not found. Skipping.")
            continue

        group_col = col
        df_group = df_copy_for_calc # Start with the dataframe possibly containing calculated error

        # Handle binning for continuous properties
        if 'bins' in prop_config:
            bin_col_name = f"{prop_name}_bin"
            group_col = bin_col_name
            bin_labels = prop_config.get('bin_labels', False)
            try:
                 # Ensure labels match number of bins if provided
                 if isinstance(bin_labels, list) and len(bin_labels) != prop_config['bins']:
                      logger.warning(f"Num labels ({len(bin_labels)}) != num bins ({prop_config['bins']}) for {prop_name}. Using default.")
                      bin_labels = False

                 # Important: Use pd.cut on the DataFrame copy
                 df_group[bin_col_name] = pd.cut(
                     df_group[col],
                     bins=prop_config['bins'],
                     labels=bin_labels,
                     include_lowest=True,
                     duplicates='drop'
                 )
                 # Remove rows where binning failed (resulted in NaN bin)
                 initial_rows = len(df_group)
                 df_group = df_group.dropna(subset=[bin_col_name])
                 if len(df_group) < initial_rows:
                      logger.debug(f"Dropped {initial_rows - len(df_group)} rows with NaN bins for '{col}'.")

                 logger.debug(f"Binned '{col}' into '{bin_col_name}'.")
            except Exception as e:
                 logger.error(f"Failed to bin column '{col}': {e}. Skipping analysis for {prop_name}.")
                 continue

        if df_group.empty:
             logger.warning(f"DataFrame empty after potential binning for property {prop_name}. Skipping.")
             continue

        # Group and aggregate
        try:
            # Use observed=False if using categorical bins to include empty bins
            observed_flag = False if pd.api.types.is_categorical_dtype(df_group.get(group_col)) else True
            # Group by the potentially new bin column
            prop_errors = df_group.groupby(group_col, observed=observed_flag)[error_col].agg(
                ['mean', 'median', 'std', 'count']
            ).reset_index()
            prop_errors.rename(columns={
                group_col: 'value', 'mean': 'mean_abs_error',
                'median': 'median_abs_error', 'std': 'std_abs_error'
                }, inplace=True)
            prop_errors.insert(0, 'property', prop_name)

            # Apply labels if available
            if prop_config.get('labels'):
                # Convert 'value' column to the type of the keys in labels dict before mapping
                label_keys = list(prop_config['labels'].keys())
                if label_keys:
                    key_type = type(label_keys[0])
                    try:
                         prop_errors['value'] = prop_errors['value'].astype(key_type)
                    except (ValueError, TypeError):
                         logger.warning(f"Could not convert 'value' column to type {key_type} for mapping property {prop_name}.")
                prop_errors['value'] = prop_errors['value'].map(prop_config['labels']).fillna(prop_errors['value'].astype(str))


            if prop_errors.empty:
                 logger.warning(f"No results after grouping for property {prop_name}.")
                 continue

            # Save CSV
            csv_path = f"{output_base_path}_{prop_name}.csv"
            ensure_dir(os.path.dirname(csv_path))
            prop_errors.to_csv(csv_path, index=False)
            logger.info(f"Error analysis by {prop_name} saved to {csv_path}")

            # Create Plot
            plot_path = f"{output_base_path}_{prop_name}.png"
            fig, ax = plt.subplots(figsize=(max(8, len(prop_errors)*0.8), 5))
            plot_order = None
            if prop_name == 'sequence_position': plot_order = prop_config.get('bin_labels')
            elif prop_name == 'surface_exposure': plot_order = ['Core', 'Surface']
            elif prop_name == 'secondary_structure': plot_order = ['Helix', 'Sheet', 'Loop/Other']

            sns.barplot(x='value', y='mean_abs_error', data=prop_errors, order=plot_order, ax=ax, palette="viridis")
            ax.set_title(f'Mean Absolute Error by {prop_name.replace("_", " ").title()} ({model_name})')
            ax.set_xlabel(prop_name.replace('_', ' ').title())
            ax.set_ylabel('Mean Absolute Error')

            # --- CORRECTED LINE ---
            ax.tick_params(axis='x', rotation=45 if prop_name not in ['sequence_position', 'surface_exposure'] else 0)
            # --- END CORRECTED LINE ---

            # Optionally improve label readability further if rotated
            if prop_name not in ['sequence_position', 'surface_exposure']:
                 fig.autofmt_xdate(rotation=45) # Alternative way to handle rotated labels

            save_plot(fig, plot_path)

        except Exception as e:
            logger.error(f"Failed processing or plotting for property {prop_name}: {e}", exc_info=True)
            plt.close('all')


# --- New Visualization Functions ---

def plot_prediction_vs_temperature(
    results_df: pd.DataFrame,
    model_name: str,
    plot_path: str,
    sample_size: int = 5000,
    color_by: Optional[str] = 'actual' # 'actual', 'error', 'resname', None
) -> None:
    """
    Generate scatter plot of Predicted RMSF vs. Input Temperature feature.

    Args:
        results_df: DataFrame containing 'all_results.csv' data.
        model_name: Name of the model.
        plot_path: Path to save the plot (.png).
        sample_size: Number of points to sample for plotting.
        color_by: Column to use for coloring points ('actual', 'error', 'resname', or None).
    """
    pred_col = f"{model_name}_predicted"
    temp_col = 'temperature'
    target_col = 'rmsf' # Assumes target is 'rmsf'
    error_col = f"{model_name}_abs_error"

    required_cols = [pred_col, temp_col]
    color_col = None
    if color_by == 'actual': color_col = target_col
    elif color_by == 'error': color_col = error_col
    elif color_by == 'resname': color_col = 'resname'

    if color_col: required_cols.append(color_col)

    if not all(col in results_df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in results_df.columns]
        logger.error(f"Missing required columns ({missing}) for prediction vs temperature plot. Skipping.")
        return

    plot_data = results_df[required_cols].dropna().copy()

    if plot_data.empty:
        logger.warning("No valid data points found for prediction vs temperature plot.")
        return

    # Sample data
    n_samples = min(sample_size, len(plot_data))
    sampled_df = plot_data.sample(n_samples, random_state=42)

    # Create plot
    try:
        fig, ax = plt.subplots(figsize=(10, 6))
        hue_column_data = None
        palette = None
        cbar_label = None
        legend_title = None
        is_categorical_hue = False

        if color_col and color_col != 'resname': # Numeric coloring
            hue_column_data = sampled_df[color_col]
            palette = 'viridis' if color_col == target_col else 'coolwarm'
            cbar_label = f'Actual {target_col.upper()}' if color_col == target_col else 'Absolute Error'
        elif color_col == 'resname': # Categorical coloring
             hue_column_data = sampled_df['resname']
             is_categorical_hue = True
             unique_res = sampled_df['resname'].unique()
             n_colors = len(unique_res)
             if n_colors <= 20:
                  palette = sns.color_palette("tab20", n_colors=n_colors)
                  legend_title = "Residue"
             else: # Too many categories, disable coloring
                  logger.warning("Too many residue types for distinct coloring. Disabling coloring.")
                  hue_column_data = None
                  is_categorical_hue = False

        # Use seaborn for easier hue mapping
        sns.scatterplot(
            x=temp_col,
            y=pred_col,
            data=sampled_df,
            hue=hue_column_data,
            palette=palette,
            s=25,
            alpha=0.7,
            edgecolor='none',
            legend='brief' if is_categorical_hue and n_colors <= 20 else False, # Show legend only for categorical & manageable N
            ax=ax
        )

        # Add colorbar for numeric hue
        # Get the collection used by scatterplot to create a colorbar
        if not is_categorical_hue and hue_column_data is not None:
            norm = plt.Normalize(hue_column_data.min(), hue_column_data.max())
            sm = plt.cm.ScalarMappable(cmap=palette, norm=norm)
            sm.set_array([])
            fig.colorbar(sm, ax=ax, label=cbar_label)


        if is_categorical_hue and legend_title and ax.get_legend() is not None:
             ax.legend(title=legend_title, bbox_to_anchor=(1.05, 1), loc='upper left')
        elif ax.get_legend() is not None:
             ax.get_legend().remove() # Remove default legend if not needed

        ax.set_xlabel("Input Temperature Feature (K)")
        ax.set_ylabel(f"{model_name} Predicted {target_col.upper()}")
        ax.set_title(f"Predicted {target_col.upper()} vs. Input Temperature ({model_name})")
        ax.grid(True, linestyle=':', alpha=0.5)

        save_plot(fig, plot_path)

    except Exception as e:
        logger.error(f"Failed to create prediction vs temperature plot: {e}", exc_info=True)
        plt.close('all')


def plot_error_vs_temperature(
    results_df: pd.DataFrame,
    model_name: str,
    config: Dict[str, Any], # Needed for metrics config
    plot_path: str,
    csv_path: Optional[str] = None,
    n_bins: int = 10
) -> None:
    """
    Generate plots of evaluation metrics (calculated in bins) vs. Input Temperature.

    Args:
        results_df: DataFrame containing 'all_results.csv' data.
        model_name: Name of the model.
        config: Global configuration dictionary (for metric definitions).
        plot_path: Path to save the plot (.png).
        csv_path: Optional path to save the binned metric data (.csv).
        n_bins: Number of temperature bins to create.
    """
    temp_col = 'temperature'
    target_col = config['dataset']['target'] # 'rmsf'
    pred_col = f"{model_name}_predicted"

    if not all(c in results_df.columns for c in [temp_col, target_col, pred_col]):
        missing = [c for c in [temp_col, target_col, pred_col] if c not in results_df.columns]
        logger.error(f"Missing required columns ({missing}) for error vs temperature analysis. Skipping.")
        return

    df_valid = results_df[[temp_col, target_col, pred_col]].dropna().copy()
    if df_valid.empty:
        logger.warning("No valid data points found for error vs temperature analysis.")
        return

    # Create temperature bins
    min_temp, max_temp = df_valid[temp_col].min(), df_valid[temp_col].max()
    if max_temp <= min_temp: # Handle single temperature value or invalid range
         logger.warning(f"Cannot create bins for temperature range: min={min_temp}, max={max_temp}. Skipping plot.")
         return

    try:
        # Create bins with slightly extended range to include endpoints
        bins = np.linspace(min_temp - 0.01 * abs(min_temp) if min_temp != 0 else -0.01,
                           max_temp + 0.01 * abs(max_temp) if max_temp != 0 else 0.01,
                           n_bins + 1)
        # Use bin midpoints as labels for plotting later
        bin_centers = [(bins[i] + bins[i+1]) / 2 for i in range(n_bins)]
        # Use pd.cut without labels first to get bin indices, then map to centers
        df_valid['temp_bin_idx'] = pd.cut(df_valid[temp_col], bins=bins, labels=False, right=True, include_lowest=True)

    except Exception as e:
        logger.error(f"Failed to create temperature bins: {e}. Skipping plot.")
        return


    # Calculate metrics per bin
    binned_metrics = []
    metrics_to_calc_config = config['evaluation']['metrics'] # Get metrics config
    metrics_to_calculate = [m for m, enabled in metrics_to_calc_config.items() if enabled]

    for bin_idx, group in df_valid.groupby('temp_bin_idx', observed=False):
        if group.empty: continue
        if pd.isna(bin_idx): continue # Skip rows that didn't fall into a bin

        y_true = group[target_col].values
        y_pred = group[pred_col].values

        # Calculate metrics using the evaluate_predictions function
        # Need to create a temporary config for evaluate_predictions with only the required metrics
        temp_eval_config = {'evaluation': {'metrics': {m: True for m in metrics_to_calculate}}}
        bin_result = evaluate_predictions(y_true, y_pred, temp_eval_config) # Use temp config

        bin_center = bin_centers[int(bin_idx)]
        bin_result['temp_center'] = bin_center
        bin_result['count'] = len(group)

        binned_metrics.append(bin_result)

    if not binned_metrics:
        logger.warning("No metrics calculated across temperature bins.")
        return

    metrics_df = pd.DataFrame(binned_metrics).sort_values('temp_center')

     # Save binned metrics data if path provided
    if csv_path:
        ensure_dir(os.path.dirname(csv_path))
        try:
            metrics_df.to_csv(csv_path, index=False)
            logger.info(f"Binned metrics vs temperature data saved to {csv_path}")
        except Exception as e:
            logger.error(f"Failed to save binned metrics CSV: {e}")

    # Create plots (e.g., RMSE and R2 vs Temp)
    try:
        # Identify which metrics were successfully calculated
        plot_cols = [m for m in metrics_to_calculate if m in metrics_df.columns]

        if not plot_cols:
             logger.warning("No suitable metrics available for plotting vs temperature.")
             return

        n_plots = len(plot_cols)
        fig, axes = plt.subplots(n_plots, 1, figsize=(8, n_plots * 3.5), sharex=True, squeeze=False) # Ensure axes is 2D array
        axes = axes.flatten() # Flatten for easy iteration

        for i, metric in enumerate(plot_cols):
            ax = axes[i]
            ax.plot(metrics_df['temp_center'], metrics_df[metric], marker='o', linestyle='-')
            ax.set_ylabel(metric.upper())
            ax.set_title(f'{metric.upper()} vs. Input Temperature ({model_name})')
            ax.grid(True, linestyle=':', alpha=0.5)
            # Add count as secondary y-axis? Or text labels?
            # ax2 = ax.twinx()
            # ax2.bar(metrics_df['temp_center'], metrics_df['count'], alpha=0.2, width=(bins[1]-bins[0])*0.8, color='grey')
            # ax2.set_ylabel('Count', color='grey')

        axes[-1].set_xlabel("Temperature Bin Center (K)")
        fig.suptitle(f"Performance Metrics vs. Temperature ({model_name})", y=1.02) # Add overall title
        fig.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent title overlap

        save_plot(fig, plot_path) # Pass the figure object

    except Exception as e:
        logger.error(f"Failed to create error vs temperature plot: {e}", exc_info=True)
        plt.close('all')

def plot_training_validation_curves(
    train_metrics: Dict[str, List[float]],
    val_metrics: Dict[str, List[float]],
    model_name: str,
    plot_path: str # Changed from config to direct path
    # config: Dict[str, Any] # Config might not be needed if path is direct
) -> None:
    """
    Generate training and validation curves plot. CSV saving handled by Pipeline.

    Args:
        train_metrics: Dictionary of training metrics by epoch (e.g., {'train_loss': [], 'train_r2': []}).
        val_metrics: Dictionary of validation metrics by epoch (e.g., {'val_loss': [], 'val_r2': []}).
        model_name: Name of the model.
        plot_path: Full path to save the plot (.png).
    """
    if not train_metrics and not val_metrics:
        logger.warning(f"No training or validation metrics provided for {model_name}. Skipping curve plot.")
        return

    # Determine available epochs and metrics
    epochs = 0
    if train_metrics: epochs = max(epochs, len(next(iter(train_metrics.values()), [])))
    if val_metrics: epochs = max(epochs, len(next(iter(val_metrics.values()), [])))

    if epochs == 0:
        logger.warning(f"No epochs found in metrics data for {model_name}. Skipping curve plot.")
        return

    # Identify metrics present in both train and val (or just one)
    possible_metrics = set(k.replace('train_', '').replace('val_', '') for k in train_metrics) | \
                       set(k.replace('train_', '').replace('val_', '') for k in val_metrics)
    metrics_to_plot = []
    if 'loss' in possible_metrics: metrics_to_plot.append(('loss', 'Loss'))
    if 'r2' in possible_metrics: metrics_to_plot.append(('r2', 'R¬≤'))
    if 'mae' in possible_metrics: metrics_to_plot.append(('mae', 'MAE'))
    # Add others if needed

    if not metrics_to_plot:
         logger.warning(f"No plottable metrics (loss, r2, mae) found for {model_name}.")
         return

    n_plots = len(metrics_to_plot)
    try:
        fig, axes = plt.subplots(n_plots, 1, figsize=(10, n_plots * 4), sharex=True, squeeze=False)
        axes = axes.flatten()

        x_values = range(epochs)

        for i, (metric_key, metric_title) in enumerate(metrics_to_plot):
            ax = axes[i]
            train_key = f"train_{metric_key}"
            val_key = f"val_{metric_key}"

            if train_key in train_metrics and len(train_metrics[train_key]) == epochs:
                ax.plot(x_values, train_metrics[train_key], marker='.', linestyle='-', label=f'Training {metric_title}')
            if val_key in val_metrics and len(val_metrics[val_key]) == epochs:
                ax.plot(x_values, val_metrics[val_key], marker='.', linestyle='-', label=f'Validation {metric_title}')

            ax.set_ylabel(metric_title)
            ax.set_title(f'{metric_title} Curve')
            ax.legend()
            ax.grid(True, linestyle=':', alpha=0.5)

        axes[-1].set_xlabel('Epoch')
        fig.suptitle(f'Training & Validation Curves for {model_name}', y=1.02)
        fig.tight_layout(rect=[0, 0.03, 1, 0.98])

        save_plot(fig, plot_path)

    except Exception as e:
         logger.error(f"Failed to generate training curves plot: {e}", exc_info=True)
         plt.close('all')
         
### Script Files ###
---------------------------------------------------------
===== FILE: scripts/aggregate_data.py =====
# /home/s_felix/ensembleflex/scripts/aggregate_data.py

import os
import glob
import re
import argparse
import logging
import pandas as pd

# --- Configuration ---
LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)
logger = logging.getLogger(__name__)

# Regex to extract temperature (numeric or 'average') from filename
# Example: temperature_320_train.csv -> 320
# Example: temperature_average_train.csv -> average
FILENAME_PATTERN = re.compile(r"temperature_(\d+|average)_.*\.csv", re.IGNORECASE)

def parse_arguments():
    """Parses command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Aggregate temperature-specific RMSF data files into a single CSV."
    )
    parser.add_argument(
        "--input-dir",
        type=str,
        required=True,
        help="Directory containing the temperature-specific CSV files (e.g., ../data/)."
    )
    parser.add_argument(
        "--output-file",
        type=str,
        required=True,
        help="Path to save the aggregated CSV file (e.g., ../data/aggregated_rmsf_data.csv)."
    )
    parser.add_argument(
        "--file-glob",
        type=str,
        default="temperature_*.csv",
        help="Glob pattern to find input temperature files (default: 'temperature_*.csv')."
    )
    parser.add_argument(
        "--exclude-average",
        action='store_true', # Default is False (include average if found)
        help="Exclude the 'temperature_average_train.csv' file from aggregation."
    )
    parser.add_argument(
        "--target-column-prefix",
        type=str,
        default="rmsf_",
        help="Prefix for the temperature-specific target columns (e.g., 'rmsf_')."
    )
    parser.add_argument(
        "--new-target-column",
        type=str,
        default="rmsf",
        help="Name for the unified target column in the output file (default: 'rmsf')."
    )
    parser.add_argument(
        "--new-temp-column",
        type=str,
        default="temperature",
        help="Name for the new temperature feature column (default: 'temperature')."
    )
    return parser.parse_args()

def process_file(file_path: str, args: argparse.Namespace) -> pd.DataFrame | None:
    """Loads a single temperature file, processes it, and returns a DataFrame."""
    filename = os.path.basename(file_path)
    match = FILENAME_PATTERN.match(filename)

    if not match:
        logger.warning(f"Skipping file (does not match pattern): {filename}")
        return None

    temp_str = match.group(1)
    logger.info(f"Processing file: {filename} for temperature '{temp_str}'")

    if temp_str.lower() == "average":
        if args.exclude_average:
            logger.info(f"Excluding 'average' temperature file as requested: {filename}")
            return None
        # For the ensembleflex model expecting numeric temperature, 'average' doesn't fit well.
        # We will assign NaN. The data processing step later might need to handle/drop these.
        temperature_value = float('nan')
        logger.warning(f"Assigning NaN to temperature column for 'average' file: {filename}. "
                       f"Ensure downstream processing handles this.")
    else:
        try:
            temperature_value = float(temp_str)
        except ValueError:
            logger.error(f"Could not convert temperature '{temp_str}' to float in file: {filename}. Skipping.")
            return None

    # Construct the expected original target column name
    original_target_col = f"{args.target_column_prefix}{temp_str}"

    try:
        df = pd.read_csv(file_path)
        logger.debug(f"Loaded {filename} with shape {df.shape}")

        if original_target_col not in df.columns:
            logger.error(f"Expected target column '{original_target_col}' not found in {filename}. Skipping.")
            return None

        # Add the new temperature column
        df = df.assign(**{args.new_temp_column: temperature_value})

        # Rename the target column
        df = df.rename(columns={original_target_col: args.new_target_column})

        # Drop other potential rmsf_ columns to avoid confusion? Optional.
        # cols_to_drop = [col for col in df.columns if col.startswith(args.target_column_prefix) and col != args.new_target_column]
        # if cols_to_drop:
        #     df = df.drop(columns=cols_to_drop)
        #     logger.debug(f"Dropped other RMSF columns: {cols_to_drop}")

        logger.info(f"Successfully processed: {filename}")
        return df

    except FileNotFoundError:
        logger.error(f"File not found during processing: {file_path}. Skipping.")
        return None
    except pd.errors.EmptyDataError:
        logger.warning(f"Skipping empty file: {filename}")
        return None
    except Exception as e:
        logger.error(f"Error processing file {filename}: {e}", exc_info=True)
        return None


def main():
    """Main execution function."""
    args = parse_arguments()
    logger.info("Starting data aggregation process...")
    logger.info(f"Input directory: {args.input_dir}")
    logger.info(f"Output file: {args.output_file}")
    logger.info(f"File glob pattern: {args.file_glob}")
    logger.info(f"Exclude 'average' file: {args.exclude_average}")

    input_path_pattern = os.path.join(args.input_dir, args.file_glob)
    source_files = glob.glob(input_path_pattern)

    if not source_files:
        logger.error(f"No files found matching pattern '{input_path_pattern}'. Exiting.")
        return

    logger.info(f"Found {len(source_files)} potential source files.")

    all_dfs = []
    processed_count = 0
    for file_path in sorted(source_files): # Sort for consistent order
        processed_df = process_file(file_path, args)
        if processed_df is not None:
            all_dfs.append(processed_df)
            processed_count += 1

    if not all_dfs:
        logger.error("No dataframes were successfully processed. Aggregated file will not be created.")
        return

    logger.info(f"Successfully processed {processed_count} files. Concatenating dataframes...")

    try:
        aggregated_df = pd.concat(all_dfs, ignore_index=True)
        logger.info(f"Aggregation complete. Final dataframe shape: {aggregated_df.shape}")

        # Basic validation of the output
        if args.new_temp_column not in aggregated_df.columns:
             logger.error(f"Critical Error: The new temperature column '{args.new_temp_column}' is missing in the final dataframe!")
             return
        if args.new_target_column not in aggregated_df.columns:
             logger.error(f"Critical Error: The new target column '{args.new_target_column}' is missing in the final dataframe!")
             return
        if not args.exclude_average and aggregated_df[args.new_temp_column].isnull().any():
             logger.warning(f"NaN values found in the '{args.new_temp_column}' column, likely from 'average' files.")

        # Ensure output directory exists
        output_dir = os.path.dirname(args.output_file)
        if output_dir: # Handle case where output file is in the current directory
            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"Ensured output directory exists: {output_dir}")


        # Save the aggregated dataframe
        aggregated_df.to_csv(args.output_file, index=False)
        logger.info(f"Aggregated data successfully saved to: {args.output_file}")

    except Exception as e:
        logger.error(f"Error during dataframe concatenation or saving: {e}", exc_info=True)

if __name__ == "__main__":
    main()
==========================================================
Output Result Files (First 15 lines of each file)
==========================================================

Searching for output files in: ./output/ensembleflex
Output directory './output/ensembleflex' not found.
==========================================================
End of EnsembleFlex Context Document
==========================================================
